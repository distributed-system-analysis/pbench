Included here are various items which at some point should be done for pbench.

General:
-job processor
 -Currently pbench is usually run in a terminal.  This is fine for single
  system use, but does not work well for multi-system tests.  We need a way to
  process job files, so we can (1) not maintain a terminal and (2) submit the
  same job file to many systems.  We most likely need a daemon which waits for
  new job files and processes a job file once one appears.  We could scan a
  local directory for new files (inotify) and/or periodically check a
  http/ftp/nfs location for new jobs.  Job files could simply be bash scripts,
  or we could process ourselves, exec'ing each line.  Having pbench processing
  the file might have some advantages in that some state could be saved
  (variable defs) if the job file should issue a reboot commmand (and the
  pbench daemon would resume on boot, processing the remainder of the job
  file).  Running bash scripts, on the other hand, won't survive a reboot, but
  it would be far easier to implment.  With either of these they would
  probably be run within a screen session so one could attach and watch.

Utils:
-restrict
 -If we plan to write a single job file or bash script which gets distributed
  to many systems, we need to ability to allow different systems run different
  things, even with the same job file.  This is not really difficult with an
  if statement and hostname checking, but something more convenient would be
  nice.  "restrict" would be a utility which takes a list of hostnames or IPs,
  followed by a command.  In this situation every system with the same job
  file [that has a resctrict command] runs the resctrict script, and if their
  hostname matches the list, then they get to run the command.  for example:

   #!/bin/bash
   restrict client1 uperf --mode=client --server=server1
   restrict server1 uperf --mode=server

-sync
 -When running a test with many different hosts, VMs, or containers executing,
  we often need to synchronize certain things, so we can get repeatable
  results and have high confidence that what we want to happen is actually
  happening at the right time.  This is also required when one system needs to
  set up a service (web, etc) before a client system tests that service.  A
  sync is used to wait until the server is done setting up the service.

  To do this, we need a "sync" command.  All systems using the same job file
  would call sync with (1) the sync label, "this_sync", and (2) a list of
  systems who have to participate in the sync.  The sync utility will wait
  until all members in the list are running the sync command for the
  particular sync name.  Once all members have executed the command, then and
  only then do they get to exit the sync command and resume.  An
  implementation will certainly require networking to make this happen.

  An example of job file might be

    #!/bin/bash
    restrict server1 start-web-service
    sync web-ready server1 client1
    restrict client1 benchmark-web-server
    sync web-test-complete server1 client1
    restrict server1 stop-web-service

  A sync command may also be within a benchmark script.  This may be needed
  when several systems need to execute the same test, and each iteration in
  the test must start at the same time for all systems.  A benchmark script
  (in /opt/pbench-agent/benchmark-scripts) may have something like:

    for iteration in `seq 1 10`; do
        sync $benchmark-$iteration $systems_running_benchmark
	start-tools
        benchmark-command $benchmark-options
	stop-tools
    done

  Note that the $systems_running_benchmark above may be the same list that was
  used in a restrict command in the user's jb file to call the benchmark.  The
  benchmark would also have to process an option which provided this list.

Tools:
-mpstat
 -For post-processing, group cpu graphs according to the system topology, with
  an average graph for system, then a section for each node containing an
  average graph for nodeX, then individual cpuid graphs with sibling
  hyperthread cpus paired together:

  [system average graph]

  [node0 average graph]
  [coreid0-threadid0][coredid0-threadid1]
  [coreid1-threadid0][coredid1-threadid1]
  [coreid2-threadid0][coredid2-threadid1]
  [coreid3-threadid0][coredid3-threadid1]

  [node1 average graph]
  [coreid0-threadid0][coredid0-threadid1]
  [coreid1-threadid0][coredid1-threadid1]
  [coreid2-threadid0][coredid2-threadid1]
  [coreid3-threadid0][coredid3-threadid1]

Benchmarks:
-uperf
 -before running any tests, run a very quick test (any type really) to make
  sure the client can contect the server.  If this fails, abort the whole
  thing.
