#!/bin/bash
export LANG=C.UTF-8
export LC_ALL=C.UTF-8
export PERL_HASH_SEED=0

export _tdir=$(dirname $(readlink -f $0))
cd $_tdir

if [[ ! -z "$1" ]]; then
    if [[ ! -d ./samples/$1 ]]; then
        echo "Ignoring $1, ./samples/$1 does not exist as a directory" >&2
        testname=""
    else
        testname="$1"
    fi
fi

export _testroot=/var/tmp/pbench-test-tools-postprocess
mkdir -p ${_testroot}
if [ $? -ne 0 ]; then
    echo "ERROR - unable to create ${_testroot} directory" >&2
    exit 1
fi
rm -rf ${_testroot}/*

function remove_path {
    # PATH ($2) => /bin:/opt/a dir/bin:/sbin
    WORK=:$2:
    # WORK => :/bin:/opt/a dir/bin:/sbin:
    REMOVE=$1
    WORK=${WORK/:$REMOVE:/:}
    # WORK => :/bin:/sbin:
    WORK=${WORK%:}
    WORK=${WORK#:}
    #PATH=$WORK
    # PATH => /bin:/sbin
    echo $WORK
}
PATH=$(remove_path /opt/pbench-agent/bench-scripts $PATH)
export PATH=$(remove_path /opt/pbench-agent/util-scripts $PATH)

# the timestamps in the data are (or should be) in UTC
# make sure that we test in that environment, else the
# timestamps will show a constant offset.
export TZ=UTC

export _PBENCH_UNIT_TESTS

export _TEST_SKIP_CPU_MAP=1

# Set up for a test run:  create a directory holding the sample data, copied
# from the reference directory.
function setup_test {
    rm -rf ${_testroot}/$1
    mkdir -p ${_testroot}/$1
    cp -a samples/$1/ ${_testroot}/
}

# Remove any file from the test directory that is the same as the
# corresponding file in the sample directory (or is a symlink), and
# then, bottom-up, remove any empty directories.
function remove_sample_data {
    # We change the working directory to the test output directory so that the
    # find command will generate relative paths which we can apply to our
    # current directory for running the diff command.
    local cur_dir=${PWD}
    (cd ${_testroot}/$1 && \
        find . \
            \( -type l \
            -or -type f -and -exec diff -q ${cur_dir}/samples/$1/{} {} \; \
            -or -type d -and -empty \) \
            -delete >/dev/null 2>&1)
}

# Evaluate the results of a unit test:  remove any extraneous files, then
# compare the rest to the "gold" results.
function evaluate_results {
    local diff_switch=$1
    local test_name=$2

    remove_sample_data ${test_name}

    if [[ ${test_name} == process-iteration-samples-* ]]; then
        # In this special case, a successful test is one where the
        # "reference-result" symlink is created to the sample1 directory.  Since
        # the sample1 directory is removed when the sample data is removed
        # above, unconditionally add an empty file for sample one so that the
        # diff will succeed when the symlink is properly created.
        touch ${_testroot}/${test_name}/sample1
    fi

    diff -r ${diff_switch} gold/${test_name}/ ${_testroot}/${test_name}/
    sts=$?
    if [ $sts -eq 0 ]; then
        rm -rf ${_testroot}/${test_name}
    fi

    return $sts
}

function testit {
    echo "Testing ${2}-postprocess under sample ${1} ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib \
        ./${2}-postprocess ${_testroot}/${1} \
        > ${_testroot}/${1}/stdout 2> ${_testroot}/${1}/stderr

    evaluate_results -c $1
    return $?
}

function testjstack {
    echo "Testing jstack under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib \
        ./jstack-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testprocessiterationsamples {
    echo "Testing bench-scripts/postprocess/process-iteration-samples under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/process-iteration-samples ${_testroot}/$1 `cat ${_testroot}/$1/process-iteration-samples-args` \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testgeneratebenchmarksummary {
    echo "Testing bench-scripts/postprocess/generate-benchmark-summary under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/generate-benchmark-summary $1 unused-orig-cmd ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -q $1
    return $?
}

function testuperf {
    echo "Testing bench-scripts/postprocess/uperf-postprocess under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/uperf-postprocess ${_testroot}/$1 `cat ${_testroot}/$1/uperf-postprocess-args` \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testpcp {
    echo "Testing pcp-postprocess under sample $1 ..."

    setup_test $1

    PATH=$(pwd)/mock-bin:$PATH pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ./pcp-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testfio {
    echo "Testing bench-scripts/postprocess/fio-postprocess under sample $1 ..."

    setup_test $1
    result_dir=${_testroot}/$1/result/reference-result

    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/fio-postprocess ${result_dir} fio- default \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testhaproxy_ocp {
    echo "Testing haproxy-ocp-postprocess under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib \
        ./haproxy-ocp-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testjmap {
    echo "Testing jmap under sample $1 ..."

    setup_test $1

    pbench_lib_dir=$(pwd)/../../lib \
        ./jmap-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr

    evaluate_results -c $1
    return $?
}

function testcmpr {
    echo "Testing bench-scripts/postprocess/compare-bench-results under sample $1 ..."

    setup_test $1

    # Run the test
    fullcmd=$(realpath ../../bench-scripts/postprocess/compare-bench-results)
    (cd ${_testroot}/$1; $fullcmd comparison * > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr)

    evaluate_results -c $1
    return $?
}

function testpidstat {
    echo "Testing pidstat-postprocess under sample ${1} ..."

    setup_test $1

    export pidstat_data="${_tdir}/samples/${1}/pidstat-stdout.txt"
    # Generate pidstat converted data as tool would.
    datalog_path=$(dirname $(realpath ../datalog/pidstat-datalog))
    (cd ${_testroot}/${1}; _tool_bin=${_tdir}/mock-bin/pidstat pbench_lib_dir=${datalog_path}/../../lib ${datalog_path}/pidstat-datalog ${_testroot}/${1} 42 false "p=")

    pbench_lib_dir=$(pwd)/../../lib \
        ./pidstat-postprocess ${_testroot}/${1} \
        > ${_testroot}/${1}/stdout 2> ${_testroot}/${1}/stderr

    rm -rf ${_testroot}/${1}/pids
    evaluate_results -c $1
    return $?
}

res=0

for i in $(ls -1 samples/) ; do
    if [[ ! -z "$testname" ]]; then
        if [[ "$testname" != "$i" ]]; then
            continue
        fi
    fi
    case $i in
        uperf-*)
            testuperf $i
            sts=$?
            ;;
        fio-*)
            testfio $i
            sts=$?
            ;;
        haproxy-ocp)
            testhaproxy_ocp $i
            sts=$?
            ;;
        jmap)
            testjmap $i
            sts=$?
            ;;
        jstack)
            testjstack $i
            sts=$?
            ;;
        pcp-*)
            testpcp $i
            sts=$?
            ;;
        compare-bench-results-*)
            testcmpr $i
            sts=$?
            ;;
        process-iteration-samples-*)
            testprocessiterationsamples $i
            sts=$?
            ;;
        generate-benchmark-summary-*)
            testgeneratebenchmarksummary $i
            sts=$?
            ;;
        pidstat-*)
            testpidstat $i
            sts=$?
            ;;
        *)
            testit $i ${i%-[0-9]}
            sts=$?
            ;;
    esac
    if [[ $sts -eq 0 ]]; then
        echo "PASS - $i"
    else
        echo "FAIL - $i"
    fi
    let res=$res+$sts
done

# Attempt to remove the test directory; if it fails we ignore the failure
# as it indicates failed test output left for review.
rmdir ${_testroot} > /dev/null 2>&1

exit $res
