#!/bin/bash
export LANG=C.UTF-8
export LC_ALL=C.UTF-8
export PERL_HASH_SEED=0
let MAX_PERFTEST_SECONDS=300  # 5 minutes

export _tdir=$(dirname $(readlink -f $0))
cd $_tdir

if [[ ! -z "$1" ]]; then
    if [[ ! -d ./samples/$1 ]]; then
        echo "Ignoring $1, ./samples/$1 does not exist as a directory" >&2
        testname=""
    else
        testname="$1"
    fi
fi

export _testroot=/var/tmp/pbench-test-tools-postprocess
mkdir -p ${_testroot}
if [ $? -ne 0 ]; then
    echo "ERROR - unable to create ${_testroot} directory" >&2
    exit 1
fi
rm -rf ${_testroot}/*

function remove_path {
    # PATH ($2) => /bin:/opt/a dir/bin:/sbin
    WORK=:$2:
    # WORK => :/bin:/opt/a dir/bin:/sbin:
    REMOVE=$1
    WORK=${WORK/:$REMOVE:/:}
    # WORK => :/bin:/sbin:
    WORK=${WORK%:}
    WORK=${WORK#:}
    #PATH=$WORK
    # PATH => /bin:/sbin
    echo $WORK
}
PATH=$(remove_path /opt/pbench-agent/bench-scripts $PATH)
export PATH=$(remove_path /opt/pbench-agent/util-scripts $PATH)

# the timestamps in the data are (or should be) in UTC
# make sure that we test in that environment, else the
# timestamps will show a constant offset.
export TZ=UTC

export _PBENCH_UNIT_TESTS

export _TEST_SKIP_CPU_MAP=1

# Set up for a test run:  create a directory holding the sample data, copied
# from the reference directory.
function setup_test {
    rm -rf ${_testroot}/$1
    mkdir -p ${_testroot}/$1
    cp -a samples/$1/ ${_testroot}/
}

# Returns the current time in seconds since the UNIX Epoch.
function get_epoch_time {
    date -u "+%s"
}

# Remove any file from the test directory that is the same as the
# corresponding file in the sample directory (or is a symlink), and
# then, bottom-up, remove any empty directories.
function remove_sample_data {
    # We change the working directory to the test output directory so that the
    # find command will generate relative paths which we can apply to our
    # current directory for running the diff command.
    local cur_dir=${PWD}
    (cd ${_testroot}/$1 && \
        find . \
            \( -type l \
            -or -type f -and -exec diff -q ${cur_dir}/samples/$1/{} {} \; \
            -or -type d -and -empty \) \
            -delete >/dev/null 2>&1)
}

# Evaluate the results of a unit test:  remove any extraneous files, then
# compare the rest to the "gold" results.
function evaluate_results {
    local diff_switch=$1
    local test_name=$2

    remove_sample_data ${test_name}

    if [[ ${test_name} == process-iteration-samples-* ]]; then
        # In this special case, a successful test is one where the
        # "reference-result" symlink is created to the sample1 directory.  Since
        # the sample1 directory is removed when the sample data is removed
        # above, unconditionally add an empty file for sample one so that the
        # diff will succeed when the symlink is properly created.
        touch ${_testroot}/${test_name}/sample1
    fi

    diff -r ${diff_switch} gold/${test_name}/ ${_testroot}/${test_name}/
    sts=$?
    if [ $sts -eq 0 ]; then
        rm -rf ${_testroot}/${test_name}
    fi

    return $sts
}

# Evaluate the results of a performance test:  remove all extraneous files,
# since we're not actually interested in correctness, per se (we presumably
# have other tests which check that), and return a status which indicates
# whether the performance of this test was acceptable.
function evaluate_performance_results {
    rm -rf ${_testroot}/$1/*

    declare -i end_time=$(get_epoch_time)
    if ((end_time - start_time > MAX_PERFTEST_SECONDS)); then
        tee ${_testroot}/$1/result.txt \
            <<< "Duration: $(((end_time - start_time))) seconds"
        return 1
    else
        return 0
    fi
}

function testit {
    echo "Testing ${2}-postprocess under sample ${1} ..."
    pbench_lib_dir=$(pwd)/../../lib \
        ./${2}-postprocess ${_testroot}/${1} \
        > ${_testroot}/${1}/stdout 2> ${_testroot}/${1}/stderr
}

function testjstack {
    echo "Testing jstack under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib \
        ./jstack-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testprocessiterationsamples {
    echo "Testing bench-scripts/postprocess/process-iteration-samples under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/process-iteration-samples ${_testroot}/$1 `cat ${_testroot}/$1/process-iteration-samples-args` \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testgeneratebenchmarksummary {
    echo "Testing bench-scripts/postprocess/generate-benchmark-summary under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/generate-benchmark-summary $1 unused-orig-cmd ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testuperf {
    echo "Testing bench-scripts/postprocess/uperf-postprocess under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/uperf-postprocess ${_testroot}/$1 `cat ${_testroot}/$1/uperf-postprocess-args` \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testpcp {
    echo "Testing pcp-postprocess under sample $1 ..."
    PATH=$(pwd)/mock-bin:$PATH pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ./pcp-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testfio {
    echo "Testing bench-scripts/postprocess/fio-postprocess under sample $1 ..."
    result_dir=${_testroot}/$1/result/reference-result
    pbench_lib_dir=$(pwd)/../../lib pbench_bspp_dir=$(pwd)/../../bench-scripts/postprocess \
        ../../bench-scripts/postprocess/fio-postprocess ${result_dir} fio- default \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testhaproxy_ocp {
    echo "Testing haproxy-ocp-postprocess under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib \
        ./haproxy-ocp-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testjmap {
    echo "Testing jmap under sample $1 ..."
    pbench_lib_dir=$(pwd)/../../lib \
        ./jmap-postprocess ${_testroot}/$1 \
        > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr
}

function testcmpr {
    echo "Testing bench-scripts/postprocess/compare-bench-results under sample $1 ..."
    fullcmd=$(realpath ../../bench-scripts/postprocess/compare-bench-results)
    (cd ${_testroot}/$1; $fullcmd comparison * > ${_testroot}/$1/stdout 2> ${_testroot}/$1/stderr)
}

function testpidstat {
    echo "Testing pidstat-postprocess under sample ${1} ..."
    export pidstat_data="${_tdir}/samples/${1}/pidstat-stdout.txt"
    # Generate pidstat converted data as tool would.
    datalog_path=$(dirname $(realpath ../datalog/pidstat-datalog))
    (cd ${_testroot}/${1}; _tool_bin=${_tdir}/mock-bin/pidstat pbench_lib_dir=${datalog_path}/../../lib ${datalog_path}/pidstat-datalog ${_testroot}/${1} 42 false "p=")

    pbench_lib_dir=$(pwd)/../../lib \
        ./pidstat-postprocess ${_testroot}/${1} \
        > ${_testroot}/${1}/stdout 2> ${_testroot}/${1}/stderr

    rm -rf ${_testroot}/${1}/pids
}

res=0

for i in $(ls -1 samples/) ; do
    if [[ ! -z "$testname" ]]; then
        if [[ "$testname" != "$i" ]]; then
            continue
        fi
    fi
    setup_test $i
    t=${i#perf-}
    declare -i start_time=$(get_epoch_time)
    case $t in
        uperf-*)                       testuperf $i;;
        fio-*)                         testfio $i;;
        haproxy-ocp)                   testhaproxy_ocp $i;;
        jmap)                          testjmap $i;;
        jstack)                        testjstack $i;;
        pcp-*)                         testpcp $i;;
        compare-bench-results-*)       testcmpr $i;;
        process-iteration-samples-*)   testprocessiterationsamples $i;;
        generate-benchmark-summary-*)  testgeneratebenchmarksummary $i;;
        pidstat-*)                     testpidstat $i;;
        *)                             testit $i ${i%-[0-9]};;
    esac

    # If the test name does not have the "perf-" prefix, then evaluate its
    # results normally; otherwise, evaluate the run as performance test.
    if [[ $i == $t ]]; then
        evaluate_results -c $i
    else
        evaluate_performance_results $i
    fi
    sts=$?
    if [[ $sts -eq 0 ]]; then
        echo "PASS - $i"
    else
        echo "FAIL - $i"
    fi
    let res=$res+$sts
done

# Attempt to remove the test directory; if it fails we ignore the failure
# as it indicates failed test output left for review.
rmdir ${_testroot} > /dev/null 2>&1

exit $res
