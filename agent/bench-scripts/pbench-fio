#!/bin/bash
# -*- mode: shell-script; indent-tabs-mode: t; sh-basic-offset: 8; sh-indentation: 8; sh-indent-for-case-alt: + -*-

# This is a script to run the fio benchmark

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark_rpm=$script_name
benchmark="fio"
# allow unit tests to override
if [[ -z "$benchmark_bin" ]]; then
	benchmark_bin=/usr/local/bin/$benchmark
fi	
ver=2.14

job_file="${script_path}/templates/fio.job"

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

orig_cmd="$*"

# Defaults
keep_failed_tool_data="y"
tar_nonref_data="y"
postprocess_only="n"
nr_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
supported_test_types="read,write,rw,randread,randwrite,randrw"
install_only="n"
remote_only="n"
config=""
rate_iops=""
test_types="read,randread"		# default is -non- destructive
block_sizes="4,64,1024"
targets="/tmp/fio"
directory=""
numjobs=""
runtime=""
ramptime=""
iodepth=""
ioengine=""
pre_iteration_script=""
job_mode="concurrent" # serial or concurrent
file_size=""
direct="" # don't cache IO's by default
sync="" # don't sync IO's by default
clients="" # A list of hostnames (hosta,hostb,hostc) where you want fio to run.  Note: if you use this, pbench must be installed on these systems already.
client_file="" # A file containing a list of hostnames
tool_label_pattern="fio-"
tool_group="default"
max_key_length=20
primary_metric="readwrite_IOPS"

function fio_usage() {
		printf "The following options are available:\n"
		printf "\n"
		printf -- "\t-t str[,str] --test-types=str[,str]\n"
		printf "\t\tone or more of %s\n" $supported_test_types
		printf "\n"
		printf -- "\t--direct=[0/1]\n"
		printf "\t\t1 = O_DIRECT enabled (default), 0 = O_DIRECT disabled\n"
		printf "\n"
		printf -- "\t--sync=[0/1]\n"
		printf "\t\t1 = O_SYNC enabled, 0 = O_SYNC disabled (defalt)\n"
		printf "\n"
		printf -- "\t--rate-iops=int\n"
		printf "\t\tdo not exceeed this IOP rate (per job, per client)\n"
		printf "\n"
		printf -- "\t-r int --runtime=int\n"
		printf "\t\truntime in seconds (default is $runtime)\n"
		printf "\n"
		printf -- "\t--ramptime=int\n"
		printf "\t\ttime in seconds to warm up test before taking measurements (default is $ramptime)\n"
		printf "\n"
		printf -- "\t-b int[,int] --block-sizes=str[,str] (default is $block_sizes)\n"
		printf "\t\tone or more block sizes in KiB (default is $block_sizes)\n"
		printf "\n"
		printf -- "\t-s int[,int] --file-size=str[,str] (default is $file_size)\n"
		printf "\t\tfile sizes in MiB: %s\n"
		printf "\n"
		printf -- "\t-d str[,str] --targets=str[,str]\n"
		printf "\t\tone or more directories or block devices (default is $targets)\n"
		printf "\t\t(persistent names for devices highly recommended)\n"
		printf "\n"
		printf -- "\t-j str --job-mode=str    str=[serial|concurrent]  (default is $job_mode)\n"
		printf "\n"
		printf -- "\t--ioengine=str           str= any ioengine fio supports (default is $ioengine)\n"
		printf "\n"
		printf -- "\t--iodepth=<int>"
                printf "\t\tSet the iodepth config variable in the fio job file\n"
		printf "\n"
		printf -- "\t-c str[,str] --clients=str[,str]      str= one or more remote systems to run fio\n"
		printf -- "\t                         If no clients are specified, fio is run locally\n"
		printf -- "\t--client-file=str        str= file (with absolute path) which contains 1 client per line\n"

		printf -- "\t--config=str\n"
		printf "\t\tname of the test configuration\n"
		printf "\n"
		printf -- "\t--tool-group=str\n"
		printf "\n"
		printf -- "\t--postprocess-only=[y|n]\n"
		printf "\t\tuse this only if you want to postprocess an existing result again\n"
		printf "\t\tyou must use --run-dir option with this\n"
		printf "\n"
		printf -- "\t--run-dir=<path>\n"
		printf "\t\tprovide the path of an existig result (typically somewhere in $pbench_run\n"
		printf -- "\t--directory=<path>\n"
		printf "\t\tprovide the path to an existing directory where fio operations will be performed\n"
		printf -- "\t--numjobs=<int>\n"
		printf "\t\tnumber of jobs to run, if not given then fio default of numjobs=1 will be used\n"
		printf -- "\t--job-file=<path>\n"
		printf "\t\tprovide the path of a fio job config file, (default is $job_file)\n"
		printf -- "\t--pre-iteration-script=str\n"
		printf -- "\t\tuse executable script/program to prepare the system for test iteration\n"
		printf -- "\t\texample: --pre-iteration-script=\$HOME/drop-cache.sh\n"
		printf -- "\t--samples=<int>\n"
		printf "\t\tnumber of samples to use per test iteration (default is $nr_samples)\n"
	        printf -- "\t--max-stddev=<int>\n"
                printf "\t\tthe maximum percent stddev allowed to pass\n"
                printf -- "\t--max-failures=<int>\n"
                printf "\t\tthe maximum number of failures to get below stddev\n"
                printf -- "\t--install\n"
                printf "\t\tinstall only\n"
                printf "\t\tDefault is n\n"
		printf -- "\t--remote-only\n"
		printf "\trun this on the remotes only\n"
}

function fio_process_options() {
	opts=$(getopt -q -o jic:t:b:s:d:r: --longoptions "help,max-stddev:,max-failures:,samples:,direct:,sync:,install,remote-only,clients:,client-file:,iodepth:,ioengine:,config:,jobs-per-dev:,job-mode:,rate-iops:,ramptime:,runtime:,test-types:,block-sizes:,file-size:,targets:,tool-group:,postprocess-only:,run-dir:,directory:,numjobs:,job-file:" -n "getopt.sh" -- "$@");

	if [ $? -ne 0 ]; then
		printf "\t${benchmark}: you specified an invalid option\n\n"
		fio_usage
		exit 1
	fi
	eval set -- "$opts";
	while true; do
		case "$1" in
			--help)
			fio_usage
			exit
			;;
			--install)
			shift;
			install_only="y"
			;;
			--remote-only)
			shift;
			remote_only="y"
			;;
			--max-stddev)
			shift;
			if [ -n "$1" ]; then
				maxstddevpct="$1"
				shift;
			fi
			;;
			--max-failures)
			shift;
			if [ -n "$1" ]; then
				max_failures="$1"
				shift;
			fi
			;;
			--samples)
			shift;
			if [ -n "$1" ]; then
				nr_samples="$1"
				shift;
			fi
			;;
			--direct)
			shift;
			if [ -n "$1" ]; then
				direct=$1
				shift;
			fi
			;;
			--sync)
			shift;
			if [ -n "$1" ]; then
				sync=$1
				shift;
			fi
			;;
			-t|--test-types)
			shift;
			if [ -n "$1" ]; then
				test_types="$1"
				shift;
			fi
			;;
			-b|--block-sizes)
			shift;
			if [ -n "$1" ]; then
				block_sizes="$1"
				shift;
			fi
			;;
			-s|--file-size)
			shift;
			if [ -n "$1" ]; then
				file_size="$1"
				shift;
			fi
			;;
			--ramptime)
			shift;
			if [ -n "$1" ]; then
				ramptime="$1"
				shift;
			fi
			;;
			--rate-iops)
			shift;
			if [ -n "$1" ]; then
				rate_iops="$1"
				shift;
			fi
			;;
			-r|--runtime)
			shift;
			if [ -n "$1" ]; then
				runtime="$1"
				shift;
			fi
			;;
			-c|--clients)
			shift;
			if [ ! -z "$client_file" ] ;then
				printf "--clients and --client-file are mutually exclusive"
				exit 1
			fi
			if [ -n "$1" ]; then
				clients="$1"
				shift;
			fi
			;;
			--client-file)
			shift;
			if [ ! -z "$clients" ] ;then
				printf "--clients and --client-file are mutually exclusive"
				exit 1
			fi
			if [ -n "$1" ]; then
				if [ -e $1 ]; then
					client_file=$1
					if [[ "$client_file" != /* ]] ;then
						# make it absolute
						client_file=$PWD/$client_file
					fi
					while read line; do
						clients="$clients,$line"
					done <$1
					clients=`echo $clients | sed -e 's/^,//'`
				fi
				shift;
			fi
			;;
			-d|--targets)
			shift;
			if [ -n "$1" ]; then
				targets="$1"
				shift;
			fi
			;;
			-j|--job-mode)
			shift;
			if [ -n "$1" ]; then
				job_mode="$1"
				shift;
			fi
			;;
			--config)
			shift;
			if [ -n "$1" ]; then
				config="$1"
				shift;
			fi
			;;
			--ioengine)
			shift;
			if [ -n "$1" ]; then
				ioengine="$1"
				shift;
			fi
			;;
			--iodepth)
			shift;
			if [ -n "$1" ]; then
				iodepth="$1"
				shift;
			fi
			;;
			--tool-group)
			shift;
			if [ -n "$1" ]; then
				tool_group="$1"
				shift;
			fi
			;;
			--postprocess-only)
			shift;
			if [ -n "$1" ]; then
				postprocess_only="$1"
				shift;
			fi
			;;
			--run-dir)
			shift;
			if [ -n "$1" ]; then
				run_dir="$1"
				shift;
			fi
			;;
			--directory)
			shift;
			if [ -n "$1" ]; then 
				directory="$1"
				shift;
			fi
			;; 
			--numjobs)
			shift;
			if [ -n "$1" ]; then
				numjobs="$1"
				shift;
			fi 
			;;
			--job-file)
			shift;
			if [ -n "$1" ]; then
				job_file="$1"
				shift;

			fi
			;;
			--pre-iteration-script)
			shift;
			if [ -n "$1" ]; then
				pre_iteration_script="$1"
				if [ ! -x $pre_iteration_script ]; then
					printf "ERROR: $pre_iteration_script must be executable\n"
					exit 1
				fi
				shift;
			fi
			;; 
			--)
			shift;
			break;
			;;
			*)
			echo "what's this? [$1]"
			shift;
			break;
			;;
		esac
	done
	if [ "$postprocess_only" = "n" ]; then
		benchmark_fullname="${benchmark}_${config}_${date}"
		benchmark_run_dir="$pbench_run/${benchmark_fullname}"
	else
		if [ -z "$run_dir" ]; then
			err_log "I need a directory if postprocessing an existing result (--run-dir=)"
			exit 1
		fi
		benchmark_fullname="$(basename $run_dir)"
		benchmark_run_dir="$run_dir"
	fi
	benchmark_iterations="$pbench_tmp/${benchmark_fullname}.iterations"

	verify_tool_group $tool_group
}

function record_iteration {
	local count=$1
	local test_type=$2
	local block_size=$3
	local iteration=$4

	mdlog=${benchmark_run_dir}/metadata.log
	echo ${iteration} >> ${benchmark_iterations}
	echo $count | pbench-add-metalog-option ${mdlog} iterations/${iteration} iteration_number
	echo $test_type | pbench-add-metalog-option ${mdlog} iterations/${iteration} test_type
	echo $block_size | pbench-add-metalog-option ${mdlog} iterations/${iteration} block_size_KiB
	echo $iteration | pbench-add-metalog-option ${mdlog} iterations/${iteration} iteration_name
}


# Ensure the right version of the benchmark is installed
function fio_install() {
	if [ "$postprocess_only" = "n" ]; then
		if check_install_rpm $benchmark_rpm $ver; then
			debug_log "[$script_name]$benchmark_rpm $ver is installed"
		else
			debug_log "[$script_name]$benchmark_rpm $ver installation failed, exiting"
			exit 1
		fi
                
	fi
	if [ ! -z "$clients" ] ; then
		debug_log "verifying clients have fio installed"
		echo "verifying clients have fio installed"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client ${pbench_install_dir}/bench-scripts/$script_name --remote-only --install &
		done
		wait
	fi
}

# install python-pandas on the controller: fiologparser_hist.py needs it
function pandas_install() {
	if [ "$postprocess_only" = "n" ]; then
		if check_install_rpm python-pandas; then
			debug_log "[$script_name]python-pandas is installed"
		else
			debug_log "[$script_name]python-pandas installation failed, exiting."
                    	debug_log "[$script_name]On RHEL, python-pandas is in the EPEL repo."
                    	debug_log "[$script_name]See https://fedoraproject.org/wiki/EPEL for details."
			exit 1
		fi
	fi
}

function print_iteration {
	# printing a iteration assumes this must be a new row, so include \n first
	printf "\n%28s" "$1" >>$benchmark_summary_txt_file
	printf "\n%s" "$1" >>$benchmark_summary_csv_file
	hists=`cat $iteration_dir/reference-result/fio.job 2>/dev/null | grep "^log_hist_msec"`
	if [ "$1" = "iteration" ]; then
		# this is just a label, so no links here
		if [ ! -z "$hist_interval" ]; then
			printf "\n%28s %s %s %s" "iteration" "summary" "hist-results" "tools">>$benchmark_summary_html_file
		else
			printf "\n%28s %s %s %s" "iteration" "summary" "tools">>$benchmark_summary_html_file
		fi
	else
		if [ ! -z "$hist_interval" ]; then
			printf "\n%28s <a href=./$iteration/reference-result/summary-result.html>%s</a> <a href='./$iteration/reference-result/hist/results.html' />hist-results</a> <a href=./$iteration/reference-result/tools-$tool_group>%s</a>" "$1" "summary" "tools">>$benchmark_summary_html_file
		else
			printf "\n%28s <a href=./$iteration/reference-result/summary-result.html>%s</a> <a href=./$iteration/reference-result/tools-$tool_group>%s</a>" "$1" "summary" "tools">>$benchmark_summary_html_file
		fi
	fi
}

function print_value {
	if [ -z "$2" ]; then
		printf "%${spacing}s" "$1" >>$benchmark_summary_txt_file
		printf "%s" ",$1,stddevpct" >>$benchmark_summary_csv_file
		printf "%${spacing}s" "$1" >>$benchmark_summary_html_file
	else
		printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_txt_file
		printf "%s" ",$1,$2" >>$benchmark_summary_csv_file
		printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_html_file
	fi
}

function print_newline {
	printf "\n" >>$benchmark_summary_txt_file
	printf "\n" >>$benchmark_summary_csv_file
	printf "\n" >>$benchmark_summary_html_file
}

# Make sure this devices exists
function fio_device_check() {
	local devs=$1
	local clients=$2
	local dev=""
	local client=""
	local rc=0
	if [ "$postprocess_only" = "n" ]; then
		debug_log "fio_device_check() $devs $clients"
		for dev in `echo $devs | sed -e s/,/" "/g`; do
			if echo $dev | grep -q "^/dev/"; then
				if [ ! -z "$clients" ]; then
					for client in `echo $clients | sed -e s/,/" "/g`; do
						debug_log "checking to see if $dev exists on client $client"
						ssh $ssh_opts $client "if [ -L $dev ]; then dev=`dirname $dev`/`readlink $dev`; fi; test -b $dev" || rc=1
					done
					wait
				else
					if [ -L $dev ]; then dev=`dirname $dev`/`readlink $dev`; fi; test -b $dev || rc=1
				fi
				if [ $rc -eq 1 ]; then
					debug_log "At least one client did not have block device $dev, exiting"
					exit 1
				fi
			fi
		done
	fi
}

function fio_create_jobfile() {
	local fio_job_file="${12}"

	mkdir -p "`dirname \"$fio_job_file\"`"
	"${script_path}/templates/make-fio-jobfile.py" -j $job_file \
		-bs="`printf \"%sk\" $3`" -rw="$1" \
		-ioengine="$2" \
		-iodepth="$4" \
		-direct="$5" \
		-sync="$6" \
		-runtime="$7" \
		-ramptime="$8" \
		-size="$9" \
		-rate_iops="${10}" \
		-targets `echo "${11}" | sed -e 's/,/" "/g'` \
		| sed -e 's/ = /=/g' > $fio_job_file

	if [ $? -ne 0 ]; then
		debug_log "Failed to create jobfile $fio_job_file."
		exit 1
	fi
	echo "The following jobfile was created: $fio_job_file"
	cat $fio_job_file
}

function fio_run_job() {
	local iteration="$1"
	local benchmark_results_dir="$2"
	local fio_job_file="$3"
	local clients="$4"
	local bench_cmd="$benchmark_bin"
	local bench_opts="--output-format=json $fio_job_file"

	if [ ! -e $fio_job_file ]; then
		debug_log "fio jobfile could not be found: $fio_job_file"
		return
	fi
	echo "running fio job: $fio_job_file"

	mkdir -p $benchmark_results_dir
	mkdir -p $benchmark_results_dir/clients
	if [ ! -z "$clients" ]; then
		debug_log "creating directories on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client mkdir -p $benchmark_results_dir &
		done
		wait
		debug_log "opening port 8765 on firewall on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "firewall-cmd --add-port=8765/tcp >/dev/null" &
		done
		wait
		debug_log "killing any old fio process on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "killall fio >/dev/null 2>&1" &
		done
		wait
		debug_log "starting new fio process on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "pushd $benchmark_results_dir >/dev/null; screen -dmS fio-server bash -c ''$bench_cmd' --server 2>&1 >client-result.txt'"
		done
		wait
	else
		mkdir -p $benchmark_results_dir/clients/localhost
	fi

	# certain test preparation steps such as cache dropping 
	# can be a bit hard on the system, give it a few
	# seconds before actually starting test
	# by putting this before pbench-start-tools, 

	if [ -n "$pre_iteration_script" ] ; then
		printf "running pre-iteration-script command: $pre_iteration_script\n"
		eval "$pre_iteration_script"
	fi
	pbench-start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	local client_opts=""

	if [ ! -z "$client_file" ] ;then
		typeset -i nclients=$(wc -l $client_file | cut -d ' ' -f 1)
		client_opts="--client=$client_file --max-jobs=$nclients"
	elif [ ! -z "$clients" ]; then
		local max_jobs=0
		for client in `echo $clients | sed -e s/,/" "/g`; do
			client_opts="$client_opts --client=$client"
			if [ ! -z $numjobs ]; then 
				let max_jobs=$numjobs
			else
				let max_jobs=$max_jobs+1
			fi 
		done
		client_opts="$client_opts --max-jobs=$max_jobs"
	fi
	
	# create a command file and keep it with the results for debugging later, or user can run outside of pbench
	echo "$bench_cmd $client_opts $bench_opts" >$benchmark_results_dir/fio.cmd
	chmod +x $benchmark_results_dir/fio.cmd
	debug_log "$benchmark: Going to run [$bench_cmd $bench_opts $client_opts]"
	pushd $benchmark_results_dir >/dev/null
	$benchmark_results_dir/fio.cmd >$benchmark_results_dir/fio-result.txt
	popd >/dev/null
	pbench-stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	if [ ! -z "$clients" ]; then
		debug_log "getting log files from clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			mkdir -p $benchmark_results_dir/clients/$client
			scp $ssh_opts $client:"$benchmark_results_dir/*.log" $benchmark_results_dir/clients/$client/ 2>/dev/null &
		done
		wait
		for client in `echo $clients | sed -e s/,/" "/g`; do
			mkdir -p $benchmark_results_dir/clients/$client
			ssh $ssh_opts $client /bin/rm -f "$benchmark_results_dir/*.log"  &
		done
		wait
	fi
	pbench-postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	echo "fio job complete"
}


# Run the benchmark and start/stop perf analysis tools
function fio_run_benchmark() {
	fio_device_check "$targets" "$clients"
	benchmark_summary_txt_file="$benchmark_run_dir/summary-result.txt"
	rm -f $benchmark_summary_txt_file
	benchmark_summary_csv_file="$benchmark_run_dir/summary-result.csv"
	rm -f $benchmark_summary_csv_file
	benchmark_summary_html_file="$benchmark_run_dir/summary-result.html"
	rm -f $benchmark_summary_html_file

	printf "# these results generated with:\n# $script_name %s\n\n" "$orig_cmd" >$benchmark_summary_txt_file
	printf "<pre>\n# these results generated with:\n# $script_name %s\n\n" "$orig_cmd" >$benchmark_summary_html_file
	printf "\n" >>$benchmark_summary_txt_file
	printf "\n" >>$benchmark_summary_html_file

	mkdir -p $benchmark_run_dir/.running
	local count=1
	if [ "$job_mode" = "serial" ]; then
		# if each target is separated by a space, there will be one job for each in next for loop
		targets=`echo $targets | sed -e s/,/" "/g`
	fi
	typeset -i ntargets=$(echo $targets | wc -w)
	typeset -i ntesttypes=$(echo $test_types | sed -e 's/,/ /g' | wc -w)
	typeset -i nblocksizes=$(echo $block_sizes | sed -e 's/,/ /g' | wc -w)
	typeset -i total_iterations=$ntargets*$ntesttypes*$nblocksizes
	for dev in $targets; do
		for test_type in `echo $test_types | sed -e s/,/" "/g`; do
			for block_size in `echo $block_sizes | sed -e s/,/" "/g`; do
				job_num=1
				iteration="${count}-${test_type}-${block_size}KiB"
				record_iteration ${count} ${test_type} ${block_size} ${iteration}
				iteration_dir=$benchmark_run_dir/$iteration
				result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
				failures=0
				while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
					if [[ $failures -gt 0 ]]; then
						echo "Restarting iteration $iteration ($count of $total_iterations)"
						log "Restarting iteration $iteration ($count of $total_iterations)"
					fi
					if [ "$postprocess_only" = "n" ]; then
						mkdir -p $iteration_dir
					else
						if [ ! -e $iteration_dir ]; then
							# if the iteration dir does not exist, look for a failed result directory or archive
							fail_nr=$failures
							((fail_nr++))
							fail_tag="-fail$fail_nr"
							failed_iteration_dir="$iteration_dir$fail_tag"
							if [ -e $failed_iteration_dir ]; then
								mv $failed_iteration_dir $iteration_dir || exit 1
							else
								failed_iteration_archive="$iteration_dir$fail_tag.tar.xz"
								if [ -e $failed_iteration_archive ]; then
									echo "using $failed_iteration_archive as $iteration_dir"
									tar -C $benchmark_run_dir -J -x -f $failed_iteration_archive || exit 1
									mv $failed_iteration_dir $iteration_dir || exit 1
								else
									echo "Could not find $iteration_dir, $failed_iteration_dir, or $failed_iteration_archive"
								fi
							fi
						fi
					fi
					if [ -e $iteration_dir ]; then
						iteration_failed=0
						# each attempt at a test config requires multiple samples to get stddev
						sample_failed=0
						for sample in `seq 1 $nr_samples`; do
							if [ "$job_mode" = "serial" ]; then
								dev_short_name="`basename $dev`"
								# easier to identify what job used what device when having 1 job per device
								iteration="$iteration-${dev_short_name}"
							fi
							benchmark_results_dir="$iteration_dir/sample$sample"
							benchmark_tools_dir="$benchmark_results_dir/tools-$tool_group"
							if [ "$postprocess_only" = "n" ]; then
								mkdir -p $benchmark_results_dir
								fio_job_file="$benchmark_results_dir/fio.job"
								fio_create_jobfile "$test_type" "$ioengine" "$block_size" "$iodepth" "$direct" "$sync" "$runtime" "$ramptime" "$file_size" "$rate_iops" "$dev" "$fio_job_file"
								fio_run_job "$iteration" "$benchmark_results_dir" "$fio_job_file" "$clients"
							else
								# if we are only postprocessing, then we might have to untar an existing result
								pushd $iteration_dir >/dev/null
								if [ ! -e sample$sample ]; then
									if [ -e sample$sample.tar.xz ]; then
										tar Jxf sample$sample.tar.xz && /bin/rm sample$sample.tar.xz
									else
										echo "sample $sample missing.  There should be $nr_samples samples"
									fi
								fi
								popd >/dev/null
							fi
							debug_log "post-processing fio result"
							$script_path/postprocess/$benchmark-postprocess $benchmark_results_dir $iteration $tool_group
							rc=$?
							# if for any reason the benchmark postprocessing script fails, consider this a failure to get a sample
							if [ $rc -ne 0 ]; then
								debug_log "failed: $script_path/postprocess/$benchmark-postprocess $benchmark_results_dir $iteration $tool_group" 
								sample_failed=1
							fi
							if [ $sample_failed -eq 1 ]; then
								# we need all samples to be good, so bust out of testing samples now
								break
							fi
						done
						if [ $sample_failed -eq 0 ]; then
							# find the keys that we will compute avg & stddev
							# NOTE: we always choose "sample1" since it is
							# always present and shares the same keys with
							# every other sample
							keys=`cat $iteration_dir/sample1/result.txt  | awk -F= '{print $1}'`
							key_nr=0
							# for each key, get the average & stddev
							for key in $keys; do
								# the s_key is used in the summary reports to save space, it is just an abbreviated key
								s_key=`echo $key | cut  -d- -f2-`
								# remove the label pattern from the s_key
								s_key=`echo $s_key | sed -e s/"$tool_label_pattern"//`
								s_key=`echo $s_key | sed -e s/"transactions"/"trans"/`
								# store these in reverse order as the keys and be sure to print values in reverse order later
								s_keys[$key_nr]="$s_key"
								s_key_length=`echo $s_key | wc -m`
								if [ $s_key_length -gt $max_key_length ]; then
									max_key_length=$s_key_length
								fi
								iteration_samples=""
								for sample in `seq 1 $nr_samples`; do
									value=`grep -- "^$key" $iteration_dir/sample$sample/result.txt | awk -F= '{print $2}'`
									iteration_samples="$iteration_samples $value"
								done
								avg_stddev_result=`pbench-avg-stddev $iteration_samples`
								samples[$key_nr]="$iteration_samples"
								avg[$key_nr]=`echo $avg_stddev_result | awk '{print $1}'`
								avg[$key_nr]=`printf "%.2f" ${avg[$key_nr]}`
								stddev[$key_nr]=`echo $avg_stddev_result | awk '{print $2}'`
								stddevpct[$key_nr]=`echo $avg_stddev_result | awk '{print $3}'`
								stddevpct[$key_nr]=`printf "%.1f" ${stddevpct[$key_nr]}`
								closest[$key_nr]=`echo $avg_stddev_result | awk '{print $4}'`
								if echo $key | grep -q "$primary_metric"; then
									tput_index=$key_nr
									tput_metric=$key
								fi
								((key_nr++))
							done
			
							# create a symlink to the result dir which most accurately represents the average result
							for sample in `seq 1 $nr_samples`; do
								sample_dir="sample$sample"
								if [ $sample -eq ${closest[$tput_index]} ]; then
									msg="$tput_metric: ${samples[$tput_index]}  average: ${avg[$tput_index]} stddev: ${stddevpct[$tput_index]}%  closest-sample: $sample"
									rm -f $iteration_dir/sample-runs-summary.txt
									echo $msg | tee $iteration_dir/sample-runs-summary.txt
									log $msg
									pushd "$iteration_dir" >/dev/null; /bin/rm -rf reference-result; ln -sf $sample_dir reference-result; popd >/dev/null
								else
									# delete the tool data [and respose time log for rr tests] from the other samples to save space
									# this option is off by default
									if [ "$keep_failed_tool_data" = "n" ]; then
										/bin/rm -rf $iteration_dir/$sample_dir/tools-* $iteration_dir/$sample_dir/response-times.txt
									fi
									# since non reference-result sample data is rarely referenced, tar it up to reduce the number of files used
									if [ "$tar_nonref_data" = "y" ]; then
										pushd "$iteration_dir" >/dev/null; tar --create --xz --force-local --file=$sample_dir.tar.xz $sample_dir && /bin/rm -rf $sample_dir; popd >/dev/null
									fi
								fi
							done
	
							# if result is not within stddevpct, consider it a failed attempt
							if [[ $(echo "if (${stddevpct[$tput_index]} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; then
								iteration_failed=1
								msg="$iteration: the percent standard deviation (${stddevpct[$tput_index]}%) was not within maximum allowed (${maxstddevpct}%)"
								echo $msg
								log $msg
								msg="This iteration will be repeated until either standard deviation is below the maximum allowed, or $max_failures failed attempts."
								echo $msg
								log $msg
								msg="Changing the standard deviation percent can be done with --max-stddev= and the maximum failures with --max-failures="
								echo $msg
								log $msg
							fi
						else
							# failed at getting sample, so this iteration attempt is a failure
							iteration_failed=1
						fi

						# failure cleanup
						if [ $iteration_failed -eq 1 ]; then
							let failures=$failures+1
							# tar up the failed iteration.  We may need to look at it later, but don't waste space by keeping it uncompressed
							# if all attempts failed, leaving no good result, leave the last attempt uncompressed
							if [ $failures -le $max_failures ]; then
								pushd $benchmark_run_dir >/dev/null
								mv $iteration $iteration-fail$failures
								tar --create --xz --force-local --file=$iteration-fail$failures.tar.xz $iteration-fail$failures &&\
								/bin/rm -rf $iteration-fail$failures
								popd >/dev/null
							fi
						fi

						# break out of this multi-attempt iteration loop if passed or too many failures
						if [ $iteration_failed -eq 0 -o $failures -ge $max_failures ]; then
							break
						fi
					else # $iteration_dir missing
						echo "$iteration_dir missing, so this iteration is being skipped (what happened?)"
						iteration_failed=1
						break
					fi
				done
				spacing=`echo "$max_key_length + 1" | bc`
				
				((key_nr--))
				# print the labels for this group
				if [ "$last_test_type" != "$test_type" ]; then
					print_newline
					print_iteration "iteration"
					for i in `seq 0 $key_nr`; do
						print_value "${s_keys[$i]}"
					done
				fi
				# print the correspnding values
				print_iteration $iteration
				if [ $iteration_failed -eq 0 ]; then
					for i in `seq 0 $key_nr`; do
						print_value "${avg[$i]}" "${stddevpct[$i]}%"
					done
				fi

				echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
	printf "</pre>\n" >>$benchmark_summary_html_file
	printf "\n" >>$benchmark_summary_txt_file

	if [ ! -z "$client_file" ] ;then
		cp $client_file $benchmark_run_dir/fio-client.file
	fi
}

function fio_print_summary() {
	cat $benchmark_summary_txt_file
}

fio_process_options "$@"
fio_install

# pandas installed only on controller
if [ "$remote_only" = "n" ] ;then
	pandas_install
fi
if [ "$install_only" = "y" ]; then
	exit 0
fi

mkdir -p $benchmark_run_dir
export benchmark config
pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
fio_run_benchmark
pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end
fio_print_summary

rmdir $benchmark_run_dir/.running
