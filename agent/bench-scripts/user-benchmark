#!/bin/bash

# This is a script to run a benchmark provided by the user.  Note that this is different from most pbench
# benchmark scripts in that only 1 "iteration" is done, whatever the user provided script does.
#
# Author: Andrew Theurer
#
# To run a custom benchmark, run "user-benchmark --config="your-config-description" -- "your benchmark's executable with any options"
#			for example: user-benchmark --config="specjbb2005-4-JVMs" -- /root/SPECjbb/run_specjbb4.sh
#
# for running in docker example would be 
#
# user-benchmark --config="specjbb2005-4-JVMs" -- docker run -it _docker_image_ /root/SPECjbb/run_specjbb4.sh
# for example: user-benchmark --config="docker-specjbb2005-4-JVMs" -- docker run -it r7perf /root/SPECjbb/run_specjbb4.sh

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark="user-benchmark"

# This bench-script does the following:
# 1) process pbench script arguments (--config)
# 2) run the benchmark and start/stop perf analysis tools
# 3) gather post-run state
# 4) postprocess analysis tool data
# post-processing of benchmark specific data is the responibility of the user supplied benchmark

# Defaults
config=""
tool_group=default

# Process options and arguments
opts=$(getopt -q -o C: --longoptions "config:,tool-group:" -n "getopt.sh" -- "$@");
if [ $? -ne 0 ]; then
	printf -- "$*\n"
	printf "\n"
	printf "\t${benchmark}: you specified an invalid option\n\n"
	printf "\tThe following options are available:\n\n"
	printf -- "\t\t-C str --config=str            name of the test config\n"
	printf -- "\t\t       --tool-group=str\n"
	exit 1
fi
eval set -- "$opts";
debug_log "[$script_name] processing options"
while true; do
	case "$1" in
		-C|--config)
		shift;
		if [ -n "$1" ]; then
			config="$1"
			shift;
		fi
		;;
		--tool-group)
		shift;
		if [ -n "$1" ]; then
			tool_group="$1"
			shift;
		fi
		;;
		--)
		shift;
		break;
		;;
		*)
		echo "what happened? [$1]"
		break;
		;;
	esac
done
verify_tool_group $tool_group
iteration="1" # there can be only 1 iteration for user-benchmark
benchmark_bin="$@"
benchmark_run_dir="$pbench_run/${benchmark}_${config}_$date"
benchmark_results_dir="$benchmark_run_dir/$iteration/reference-result"
mkdir -p $benchmark_results_dir/.running
export benchmark_results_dir=$benchmark_results_dir


# For now, since we only ever run one iteration here, don't bother
# collecting sysinfo data before and after, just do it after.
#collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg

# gather up metadata for the run - this is usually called from
# collect-sysinfo (both at the beginning and end of the run)
# but since we don't call collect-sysinfo here, we need to call
# metadata-log explicitly
export benchmark config
metadata-log --group=$tool_group --dir=$benchmark_run_dir beg

# on abnormal exit, make sure that the metadata log exists and is complete.
trap "metadata-log --group=$tool_group --dir=$benchmark_run_dir int" INT QUIT

start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
echo "Running $benchmark_bin"
log "[$script_name] Running $benchmark_bin"
$benchmark_bin 2>&1 | tee $benchmark_results_dir/result.txt
stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end

rmdir $benchmark_results_dir/.running
