#!/bin/bash
# -*- mode: shell-script; indent-tabs-mode: t; sh-basic-offset: 8; sh-indentation: 8; sh-indent-for-case-alt: + -*-

# This is a script to run the linpack benchmark

# TODO:
# 1) write results in pbench standard file names and formats
# 2) add support to run mulitple samples and get stddev
# 3) add support for multiple local or remote copies of benchmark running concurrently
# 4) add support for binding copies of benchmark to numa nodes

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark_rpm=$script_name
export benchmark="linpack"
ver=11.1.3
linpack_dir=/usr/local/${script_name}-${ver}/benchmarks/linpack
linpack_cmd=xlinpack_xeon64
threads=`cat /proc/cpuinfo | grep processor | wc -l`
nr_samples=2


# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

# Defaults
baseconfig="`uname -r`"
start_iteration_num=1
orig_cmd="$*"
tool_group="default"
export config=""
sysinfo="default"

function usage {
	printf "\tThe following options are available:\n\n"
	printf -- "\t-C str --config=str         name of the test config\n"
	printf -- "\t       --samples=<int>      number of samples to use per test iteration (default is $nr_samples)\n"
	printf -- "\t       --threads=int[,int]  number of threads to use (default is num_cpus)\n"
	printf -- "\t       --tool-group=str\n"
	printf -- "\t       --sysinfo=str,       str= comma separated values of sysinfo to be collected\n"
	printf -- "\t                                available: $(pbench-display-sysinfo-options)\n"
}

# Process options and arguments
opts=$(getopt -q -o C:h --longoptions "config:,samples:,threads:,tool-group:,sysinfo:,help" -n "getopt.sh" -- "$@");
if [ $? -ne 0 ]; then
	printf -- "${script_name} $*\n"
	printf -- "\n"
	printf -- "\tunrecognized option specified\n\n"
	usage
	exit 1
fi
eval set -- "$opts";
while true; do
	case "$1" in
		-c|--config)
		shift;
		if [ -n "$1" ]; then
			config="$1"
			shift;
		fi
		;;
		--samples)
		shift;
		if [ -n "$1" ]; then
			nr_samples="$1"
			shift;
		fi
		;;
		--threads)
		shift;
		if [ -n "$1" ]; then
			threads="$1"
			shift;
		fi
		;;
		--tool-group)
		shift;
		if [ -n "$1" ]; then
			tool_group="$1"
			shift;
		fi
		;;
		--sysinfo)
		shift;
		if [ -n "$1" ]; then
			sysinfo="$1"
			shift;
		fi
		;;
		-h|--help)
		usage
		exit 0
		;;
		--)
		shift;
		break;
		;;
		*)
		echo "what happened? [$1]"
		exit 0
		break;
		;;
	esac
done
verify_common_bench_script_options $tool_group $sysinfo

function store_and_run {
	workdir="$1"
	cmd="$2"
	filename="$3"
	(
		cd "$1" &&
		echo "$cmd" > "$filename" &&
		chmod +x "$filename" &&
		"./$filename" &> "$filename.out"
	)
	ret=$?
	if [ $ret -ne 0 ]; then
		debug_log "failed to execute: $workdir/$filename"
	fi
	return $ret
}

function postprocess-iteration {
	local _cmd="$pbench_bin/bench-scripts/postprocess/linpack-postprocess-cdm"
	store_and_run "$1" "'$_cmd' '$1' '$2' '$tool_group' 1 html" \
			"$benchmark-postprocess.cmd"
}

function postprocess-results {
	local _cmd="$pbench_bin/bench-scripts/postprocess/generate-benchmark-summary"
	store_and_run "$1" "'$_cmd' '$benchmark' '$orig_cmd' '$1'" \
			"generate-benchmark-summary.cmd"
}

## Ensure the right version of the benchmark is installed
check_install_rpm "${benchmark_rpm}" "${ver}"

benchmark_fullname="${benchmark}_${config}_${date_suffix}"
export benchmark_run_dir="$pbench_run/${benchmark_fullname}"
benchmark_iterations="${benchmark_run_dir}/.iterations"

mkdir -p ${benchmark_run_dir}/.running

# now that the benchmark_run_dir directory exists, we can initialize the iterations file
> ${benchmark_iterations}

# Start the tool meisters on each registered local/remote host
pbench-tool-meister-start --sysinfo="${sysinfo}" "${tool_group}"
if [[ ${?} != 0 ]]; then
	error_log "[${script_name}]: failed to start the tool meisters."
	exit 1
fi

trap "interrupt" INT QUIT TERM

let count=1
for thread in ${threads//,/ }; do
	iteration="$count-$thread-threads"
	echo $iteration >> $benchmark_iterations
	echo "Starting iteration ${iteration}"
	# Generate the linpack.input dirs (in /var/lib/pbench-agent/tmp)
	iteration_dir="$benchmark_run_dir/$iteration"
	mkdir -p "$iteration_dir"
	store_and_run "$iteration_dir" "'$pbench_bin/bench-scripts/postprocess/linpack-prepare-input-file' --output-dir '$iteration_dir' --threads '$thread' --run-samples 1 --linpack-binary '$linpack_dir/$linpack_cmd'" "$benchmark-preprocess.cmd"
	for sample in $(seq 1 $nr_samples); do
		echo "test sample $sample of $nr_samples"
		benchmark_results_dir="$iteration_dir/sample$sample"
		# Create the output directory expected by the postprocess script
		mkdir -p "$benchmark_results_dir/clients/localhost/"

		## Run the benchmark and start/stop perf analysis tools
		pbench-start-tools --group=$tool_group --dir="$benchmark_results_dir"

		pushd "$iteration_dir" >/dev/null
		./linpack.sh
		popd >/dev/null

		pbench-stop-tools --group=$tool_group --dir="$benchmark_results_dir"
		pbench-send-tools --group=$tool_group --dir="$benchmark_results_dir"
		# Move the results to the output directory expected by the postprocess script
		mv "$iteration_dir/"linpack.{out,meta} "$benchmark_results_dir/clients/localhost"
		pbench-postprocess-tools --group=$tool_group --dir="$benchmark_results_dir"
		postprocess-iteration "$benchmark_results_dir" "$benchmark_run_dir"
	done
	echo "Iteration $iteration complete"
	let count++
done
postprocess-results "$benchmark_run_dir"

# Stop the tool meisters on each registered local/remote host
pbench-tool-meister-stop --sysinfo="${sysinfo}" "${tool_group}"
if [[ ${?} != 0 ]]; then
	error_log "[${script_name}]: failed to stop the tool meisters."
fi

rmdir $benchmark_run_dir/.running
