#!/bin/bash

# This is a script to run the uperf benchmark
# Author: Andrew Theurer

# This script attempts to automate potentially a very large number of tests for uperf
# By default, is runs 96 different tests and can take several hours.
# To limit the number of tests, use the --protocols --test-types --instances options to reduce the number of tests

# This script will take multiple samples of the same test type and try to achieve a standard deviation of <3%
#
# This script will repeat a test type 6 times in order to try to achieve target stddev.
# If a run (with several samples) fails the stddev, its directory is appended with -fail
#
# This script will also generate a "summary-results.txt" with a table of all
# results, efficiency, and other stats.

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark_name="uperf"
benchmark_bin=/usr/local/bin/$benchmark_name
benchmark_ver=1.0.4

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

# Defaults

benchmark_run_dir=""
protocols="tcp,udp"
test_types="stream,maerts,bidirec,rr"
message_sizes="1,64,1024,16384" # in bytes
config=""
instances="1,8,64"
nr_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
runtime=60
servers=127.0.0.1
server_base_port=20000
postprocess_only=n
log_response_times=n
kvm_host=""
start_iteration_num=1
keep_failed_tool_data="y"
tar_nonref_data="n"
orig_cmd="$*"
tool_group=default
tool_label_pattern="$benchmark_name-"
server_nodes=""
client_nodes=""
install_only="n"

function gen_xml {

	case $test_type in
	rr)
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="size=$size"/>'
		echo '<flowop type="read"  options="size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	stream)
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	maerts) # "stream" in reverse direction
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="accept" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="read" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	bidirec) # A streaming test in both directions at the same time
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="accept" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="read" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	*)
	echo "This test type is not available: $test_type"
	;;
	esac
}

# Process options and arguments
opts=$(getopt -q -o i:c:t:r:m:p:M:S:C: --longoptions "server-node:,server-nodes:,client-node:,client-nodes:,client-label:,server-label:,tool-label-pattern:,install,start-iteration-num:,config:,instances:,test-types:,runtime:,message-sizes:,protocols:,samples:,client:,clients:,servers:,server:,max-stddev:,max-failures:,kvm-host:,log-response-times:,postprocess-only:,run-dir:,tool-group:" -n "getopt.sh" -- "$@")
if [ $? -ne 0 ]; then
	printf -- "$*\n"
	printf "\n"
	printf "\t${benchmark_name}: you specified an invalid option\n\n"
	printf "\tThe following options are available:\n\n"
	#              1   2         3         4         5         6         7         8         9         0         1         2         3
	#              678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
	printf -- "\t\t             --kvm-host=str\n"
	printf -- "\t\t             --tool-group=str\n"
	printf -- "\t\t-c str       --config=str               name of the test config (i.e. jumbo_frames_and_network_throughput)\n"
	printf -- "\t\t-t str[,str] --test-types=str[,str]     can be stream, maerts, bidirect, and/or rr (default $test_types)\n"
	printf -- "\t\t-r int       --runtime=int              test measurement period in seconds (default is $runtime)\n"
	printf -- "\t\t-m int[,int] --message-sizes=str[,str]  list of message sizes in bytes (default is $message_sizes)\n"
	printf -- "\t\t-p str[,str] --protocols=str[,str]      tcp and/or udp (default is $protocols)\n"
	printf -- "\t\t-i int[,int] --instances=int[,int]      list of number of $benchmark_name instances to run (default is $instances)\n"
	printf -- "\t\t-C str[,str] --client[s]=str[,str]      a list of one or more hostnames/IPs.  These systems will run the\n"
	printf    "\t\t                                        $benchmark_name client (drive the test).\n"
	printf    "\t\t                                        If this is omitted, the local system is the client\n"
	printf    "\t\t                                        Note: the number of clients and server must be the same!\n"
	printf    "\t\t                                        Clients and servers are paired according the order in the list (first\n"
	printf    "\t\t                                        client pairs with firest server, etc)\n"
	printf -- "\t\t-S str[,str] --server[s]=str[,str]      a list of one or more hostnames/IPs.  These systems will run the $benchmark_name\n"
	printf    "\t\t                                        server (listening for connections)\n"
	printf    "\t\t                                        If this is omitted, the server will listen on the local system\n"
	printf    "\t\t                                        loopback interface\n"
	printf -- "\t\t--server-node[s]=str[,str]              An ordered list of server NUMA nodes which should be used for CPU binding\n"
	printf -- "\t\t--client-node[s]=str[,str]              An ordered list of rrclient NUMA nodes which should be used for CPU binding\n"
	printf -- "\t\t                                        For both options above, the order must correspond with the --clients/--servers list\n"
	printf -- "\t\t                                        To omit a specific client/server from binding, use a value of -1\n"
	printf -- "\t\t             --samples=int              the number of times each different test is run (to compute average &\n"
	printf    "\t\t                                        standard deviations)\n"
	printf -- "\t\t             --max-failures=int         the maximm number of failures to get below stddev\n"
	printf -- "\t\t             --max-stddev=int           the maximm percent stddev allowed to pass\n"
	printf -- "\t\t             --postprocess-only=y|n     don't run the benchmark, but postprocess data from previous test\n"
	printf -- "\t\t             --run-dir=str              optionally specify what directory should be used (usually only used\n"
	printf    "\t\t                                        if postprocess-only=y)\n"
	printf -- "\t\t             --start-iteration-num=int  optionally skip the first (n-1) tests\n"
	printf -- "\t\t             --log-response-times=y|n   record the response time of every single operation\n"
	printf -- "\t\t             --tool-label-pattern=str   $benchmark_name will provide CPU and efficiency information for any tool directory\n"
	printf    "\t\t                                        with a \"^<pattern>\" in the name, provided \"sar\" is one of the\n"
	printf    "\t\t                                        registered tools.\n"
	printf    "\t\t                                        a default pattern, \"$bechmark_name-\" is used if none is provided.\n"
	printf    "\t\t                                        simply register your tools with \"--label=$benchmark_name-\$X\", and this script\n"
	printf    "\t\t                                        will genrate CPU_$benchmark_name-\$X and Gbps/CPU_$benchmark_name-\$X or\n"
	printf    "\t\t                                        trans_sec/CPU-$benchmark_name-\$X for all tools which have that pattern as a\n"
	printf    "\t\t                                        prefix.  if you don't want to register your tools with \"$benchmark_name-\" as\n"
	printf    "\t\t                                        part of the label, just use --tool-label-pattern= to tell this script\n"
	printf    "\t\t                                        the prefix pattern to use for CPU information.\n"
	exit 1
fi
eval set -- "$opts"
debug_log "[$script_name]processing options"
while true; do
	case "$1" in
		--install)
		shift
		install_only="y"
		exit
		;;
		--client-label)
		shift
		if [ -n "$1" ]; then
			shift
		fi
		warn_log "The --client-label option is deprecated, please use --tool-label-pattern"
		;;
		--server-label)
		shift
		if [ -n "$1" ]; then
			shift
		fi
		warn_log "The --server-label option is deprecated, please use --tool-label-pattern"
		;;
		--tool-label-pattern)
		shift
		if [ -n "$1" ]; then
			tool_label_pattern="$1"
			shift
		fi
		;;
		--postprocess-only)
		shift
		if [ -n "$1" ]; then
			postprocess_only="$1"
			shift
		fi
		;;
		--run-dir)
		shift
		if [ -n "$1" ]; then
			benchmark_run_dir="$1"
			shift
		fi
		;;
		--max-stddev)
		shift
		if [ -n "$1" ]; then
			maxstddevpct="$1"
			shift
		fi
		;;
		--max-failures)
		shift
		if [ -n "$1" ]; then
			max_failures="$1"
			shift
		fi
		;;
		--samples)
		shift
		if [ -n "$1" ]; then
			nr_samples="$1"
			shift
		fi
		;;
		-i|--instances)
		shift
		if [ -n "$1" ]; then
			instances="$1"
			shift
		fi
		;;
		-t|--test-types)
		shift
		if [ -n "$1" ]; then
			test_types="$1"
			shift
		fi
		;;
		--kvm-host)
		shift
		if [ -n "$1" ]; then
			kvm_host="$1"
			shift
		fi
		;;
		--tool-group)
		shift
		if [ -n "$1" ]; then
			tool_group="$1"
			shift
		fi
		;;
		-m|--message-sizes)
		shift
		if [ -n "$1" ]; then
			message_sizes="$1"
			shift
		fi
		;;
		-r|--runtime)
		shift
		if [ -n "$1" ]; then
			runtime="$1"
			shift
		fi
		;;
		--log-response-times)
		shift
		if [ -n "$1" ]; then
			log_response_times="$1"
			shift
		fi
		;;
		-p|--protocols)
		shift
		if [ -n "$1" ]; then
			protocols="$1"
			shift
		fi
		;;
		-C|--client|--clients)
		shift
		if [ -n "$1" ]; then
			clients="$1"
			shift
		fi
		;;
		-S|--server|--servers)
		shift
		if [ -n "$1" ]; then
			servers="$1"
			shift
		fi
		;;
		-c|--config)
		shift
		if [ -n "$1" ]; then
			config="$1"
			shift
		fi
		;;
		--start-iteration-num)
		shift
		if [ -n "$1" ]; then
			start_iteration_num=$1
			shift
		fi
		;;
		--client-node|--client-nodes)
		shift
		if [ -n "$1" ]; then
			client_nodes="$1"
			shift
		fi
		;;
		--server-node|--server-nodes)
		shift
		if [ -n "$1" ]; then
			server_nodes="$1"
			shift
		fi
		;;
		--)
		shift
		break
		;;
		*)
		error_log "[$script_name] bad option, \"$1 $2\""
		break
		;;
	esac
done
if [[ -z "$benchmark_run_dir" ]]; then
	# We don't have an explicit run directory, construct one
	benchmark_run_dir="$pbench_run/${benchmark_name}_${config}_$date"
else
	# We have an explicit run directory provided by --run-dir, so warn
	# the user if they also used --config
	if [[ ! -z "$config" ]]; then
		warn_log "[$script_name] ignoring --config=\"$config\" in favor of --rundir=\"$benchmark_run_dir\""
	fi
fi
mkdir -p $benchmark_run_dir/.running

# Verify the number of clients and servers match.
if [ ! -z "$clients" -a "$postprocess_only" != "y" ]; then
	let c_cnt=0
	for client in `echo $clients | sed -e s/,/" "/g`; do
		let c_cnt=c_cnt+1
	done
	let s_cnt=0
	for server in `echo $servers | sed -e s/,/" "/g`; do
		let s_cnt=s_cnt+1
	done
	if [ $c_cnt -ne $s_cnt ]; then
		error_log "Number of clients and servers specified on command line must match"
		error_log "    clients($c_cnt): $clients"
		error_log "    servers($s_cnt): $servers"
		exit 1
	fi
fi

# Verify benchmark is installed
if [ "$postprocess_only" != "y" ]; then
	if [ -z "$servers" -o -z "$clients" ]; then
		echo "installing $benchmark_name on localhost"
		if ! install_benchmark_systems "localhost" "$benchmark_name" "$benchmark_ver"; then
			error_log "installtion of $benchmake_name failed";
			exit 1
		fi
	else
		echo "installing $benchmark_name on: $clients,$servers"
		if ! install_benchmark_systems "$clients,$servers" "$benchmark_name" "$benchmark_ver"; then
			error_log "installtion of $benchmark_name failed"
			exit 1
		fi

	fi
	if [ "$install_only" == "y" ]; then
		exit 0
	fi
fi

verify_tool_group $tool_group

count=0
for protocol in `echo $protocols | sed -e s/,/" "/g`; do
	for test_type in `echo $test_types | sed -e s/,/" "/g`; do
		for message_size in `echo $message_sizes | sed -e s/,/" "/g`; do
			for instance in `echo $instances | sed -e s/,/" "/g`; do
				let count=$count+1
			done
		done
	done
done

let total_iterations=count
count=1
mkdir -p $benchmark_run_dir/.running
export benchmark_name config
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
# start the server processes
for protocol in `echo $protocols | sed -e s/,/" "/g`; do
	for test_type in `echo $test_types | sed -e s/,/" "/g`; do
		if [ "$test_type" == "rr" ]; then
			primary_metric="Throughput--aggregate_trans_sec"
		else
			primary_metric="Throughput--aggregate_Gb_sec"
		fi
		for message_size in `echo $message_sizes | sed -e s/,/" "/g`; do
			for instance in `echo $instances | sed -e s/,/" "/g`; do
				if [ $count -ge $start_iteration_num ]; then
					iteration="${count}-${protocol}_${test_type}-${message_size}B-${instance}i"
					iteration_dir="$benchmark_run_dir/$iteration"
					result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
					failures=0
					echo "Starting iteration[$iteration] ($count of $total_iterations)"
					log "Starting iteration[$iteration] ($count of $total_iterations)"
					while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
						if [[ $failures -gt 0 ]]; then
							echo "Restarting iteration[$iteration] ($count of $total_iterations)"
							log "Restarting iteration[$iteration] ($count of $total_iterations)"
						fi
						mkdir -p $iteration_dir
						# each attempt at a test config requires multiple samples to get stddev
						for sample in `seq 1 $nr_samples`; do
							benchmark_results_dir="$iteration_dir/sample$sample"
							if [ "$postprocess_only" != "y" ]; then
								mkdir -p $benchmark_results_dir
								server_nr=1
								# the following loop does all of the pre-benchmark execution work
								for server in `echo $servers | sed -e s/,/" "/g`; do

									server_node=`echo "$server_nodes," | cut -d, -f$server_nr`
									client_node=`echo "$client_nodes," | cut -d, -f$server_nr`
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									xml_file="$iteration_dir/$benchmark_name-config-$server:$server_port.xml"
									benchmark_client_cmd_file="$iteration_dir/$benchmark_name-client-$server:$server_port.cmd"
									result_file=$benchmark_results_dir/result-$server:$server_port.txt
									benchmark_server_cmd_file="$iteration_dir/$benchmark_name-server-$server:$server_port.cmd"
									gen_xml $test_type $message_size $protocol $instance $server >$xml_file

									# construct the server command
									benchmark_server_cmd="$benchmark_bin -s -P $server_port"
									# adjust server command for NUMA binding
									if [ ! -z "$server_node" ]; then
										if [ $server_node -ge 0 ]; then
											benchmark_server_cmd="numactl --cpunodebind=$server_node bash -c \"$benchmark_server_cmd\""
										fi
									fi

									# create a server command file, to be used for debugging purposes or to run this benchmark command later [without pbench]
									echo "$benchmark_server_cmd" >$benchmark_server_cmd_file
									chmod +x $benchmark_server_cmd_file

									# construct the client command
									if [ "$log_response_times" == "y" ]; then
										resp_opt=" -X $benchmark_results_dir/response-times-$server:$server_port.txt"
									fi
									benchmark_client_cmd="profile=${protocol}_$test_type test=$test_type nthr=$instance size=$message_size h=$server proto=$protocol runtime=${runtime}s $benchmark_bin -m $xml_file -x -a -i 1 $resp_opt -P $server_port >$result_file 2>&1"
									# adjust client command for NUMA binding
									if [ ! -z "$client_node" ]; then
										if [ $client_node -ge 0 ]; then
											benchmark_client_cmd="numactl --cpunodebind=$client_node bash -c \"$benchmark_client_cmd\""
										fi
									fi

									# create the client command file, to be used for debugging purposes or to run this benchmark command later [without pbench]
									echo "$benchmark_client_cmd" >$benchmark_client_cmd_file
									chmod +x $benchmark_client_cmd_file

									# prepare test files and dirs if using remote clients
									if [ ! -z "$clients" ]; then
										client=`echo $clients | cut -d, -f$server_nr`
										ssh $ssh_opts $client mkdir -p $benchmark_results_dir
										scp $ssh_opts $xml_file $client:$xml_file >/dev/null
										scp $ssh_opts $benchmark_client_cmd_file $client:$benchmark_client_cmd_file >/dev/null
										ssh $ssh_opts $client "systemctl stop firewalld"
									fi

									# prepare test files and dirs on servers
									ssh $ssh_opts $server mkdir -p $benchmark_results_dir
									scp $ssh_opts $benchmark_server_cmd_file $server:$benchmark_server_cmd_file >/dev/null

									# start the benchmark server(s)
									stop_server $server $server_port 1
									ssh $ssh_opts $server "systemctl stop firewalld"
									ssh $ssh_opts $server "screen -dmS $benchmark_name-server $benchmark_server_cmd_file"

									((server_nr++))
								done
								start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir

								# start the $benchmark_name clients
								echo "test sample $sample of $nr_samples"
								log "test sample $sample of $nr_samples "
								server_nr=1
								for server in `echo $servers | sed -e s/","/" "/g`; do
									server_nodeinfo=""
									client_nodeinfo=""
									server_node=`echo "$server_nodes," | cut -d, -f$server_nr`
									client_node=`echo "$client_nodes," | cut -d, -f$server_nr`
									if [ ! -z "$client_node" ]; then
										if [ $client_node -ge 0 ]; then
											client_nodeinfo="node[$client_node]"
										fi
									fi
									if [ ! -z "$server_node" ]; then
										if [ $server_node -ge 0 ]; then
											server_nodeinfo="node[$server_node]"
										fi
									fi
									client=`echo $clients | cut -d, -f$server_nr`
									if [ -z "$client" ]; then
										client="127.0.0.1"
									fi
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									benchmark_client_cmd_file="$iteration_dir/$benchmark_name-client-$server:$server_port.cmd"
									result_file=$benchmark_results_dir/result-$server:$server_port.txt
									debug_log "client[$client]${client_nodeinfo}protocol[$protocol]test[$test_type]instances[$instance]size[$message_size] <-> server[$server]${server_nodeinfo}"
									echo "client[$client]${client_nodeinfo}protocol[$protocol]test[$test_type]instances[$instance]size[$message_size] <-> server[$server]${server_nodeinfo}"

									# using local clients
									if [ -z "$clients" ]; then
										debug_log "screen -dmS $benchmark_name-client $benchmark_client_cmd_file"
										screen -dmS $benchmark_name-client $benchmark_client_cmd_file
									else # using remote clients
										debug_log "ssh $ssh_opts $client screen -dmS $benchmark_name-client $benchmark_client_cmd_file"
										ssh $ssh_opts $client "screen -dmS $benchmark_name-client $benchmark_client_cmd_file" 
									fi
									((server_nr++))
								done
								sleep $runtime

								# stop tools and clean up
								stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
								postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir

								server_nr=1
								for server in `echo $servers | sed -e s/,/" "/g`; do
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									stop_server $server $server_port 0
									if [ ! -z "$clients" ]; then
										client=`echo $clients | cut -d, -f$server_nr`
										result_file=$benchmark_results_dir/result-$server:$server_port.txt
										scp $ssh_opts $client:$result_file $result_file >/dev/null
									fi
									((server_nr++))
								done
							else
								if [[ ! -d $benchmark_results_dir ]]; then
									error_log "Results directory $benchmark_results_dir does not exist, skipping post-processing"
									continue
								fi
								echo "Not going to run $benchmark_name.  Only postprocesing existing data"
								log "Not going to run $benchmnark_name.  Only postprocesing existing data"
							fi
							$script_path/postprocess/$benchmark_name-postprocess $benchmark_results_dir $iteration "$tool_label_pattern" "$tool_group"
							mv $benchmark_results_dir/$benchmark_name-average.txt $benchmark_results_dir/result.txt
						done
						process_benchmark_iteration_samples "$iteration_dir" "$nr_samples" "$maxstddevpct" "$primary_metric" "$failures" "$max_failures"
						fail=$?
						if [ $fail -eq 1 ]; then
							((failures++))
						fi
						if [ $fail -eq 0 -o $failures -ge $max_failures ]; then
							break
						fi
					done # break out of this loop only if the $result_stddevpct is lower than $maxstddevpct
					echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
					log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				else
					echo "Skipping iteration $iteration ($count of $total_iterations)"
					log "Skipping iteration $iteration ($count of $total_iterations)"
				fi
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
done
$script_path/postprocess/generate-benchmark-summary "$benchmark_name" "$orig_cmd" "$benchmark_run_dir"
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end
rmdir $benchmark_run_dir/.running
