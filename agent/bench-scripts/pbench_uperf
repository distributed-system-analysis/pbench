#!/bin/bash

# This is a script to run the uperf benchmark
# Author: Andrew Theurer

# This script attempts to automate potentially a very large number of tests for uperf
# By default, is runs 96 different tests and can take several hours.
# To limit the number of tests, use the --protocols --test-types --instances options to reduce the number of tests

# This script will take multiple samples of the same test type and try to achieve a standard deviation of <3%
#
# This script will repeat a test type 6 times in order to try to achieve target stddev.
# If a run (with several samples) fails the stddev, its directory is appended with -fail
#
# This script will also generate a "summary-results.txt" with a table of all
# results, efficiency, and other stats.

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark="uperf"
if [[ -z "$benchmark_bin" ]]; then
    benchmark_bin=/usr/local/bin/$benchmark
fi
ver=1.0.4

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

# Defaults

benchmark_run_dir=""
protocols="tcp,udp"
test_types="stream,maerts,bidirec,rr"
message_sizes="1,64,1024,16384" # in bytes
config=""
instances="1,8,64"
nr_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
runtime=60
mode="loopback"
servers=127.0.0.1
server_base_port=20000
postprocess_only=n
log_response_times=n
kvm_host=""
start_iteration_num=1
keep_failed_tool_data="y"
tar_nonref_data="y"
orig_cmd="$*"
tool_group=default
tool_label_pattern="uperf-"
server_nodes=""
client_nodes=""

function gen_xml {

	case $test_type in
	rr)
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="size=$size"/>'
		echo '<flowop type="read"  options="size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	stream)
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	maerts) # "stream" in reverse direction
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="accept" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="read" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	bidirec) # A streaming test in both directions at the same time
	echo '<?xml version="1.0"?>'
	echo '<profile name="$profile">'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="connect" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="write" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo '<group nthreads="$nthr">'
		echo '<transaction iterations="1">'
		echo '<flowop type="accept" options="remotehost=$h protocol=$proto"/>'
		echo '</transaction>'
		echo '<transaction duration="$runtime">'
		echo '<flowop type="read" options="count=16 size=$size"/>'
		echo '</transaction>'
		echo '<transaction iterations="1">'
		echo '<flowop type="disconnect" />'
		echo '</transaction>'
	echo '</group>'
	echo ''
	echo '</profile>'
	;;
	*)
	echo "This test type is not available: $test_type"
	;;
	esac
}

function stop_server {
	local server=$1
	local server_port=$2
	local uperf_pid=`ssh $ssh_opts $server netstat -tlnp | grep ":$server_port " | awk '{print $7}' | awk -F/ '{print $1}'`
	if [ ! -z "$uperf_pid" ]; then
		echo found uperf pid $uperf_pid on $server:$server_port, killing
		ssh $ssh_opts $server kill $uperf_pid
	fi
}

function install_uperf {
	if check_install_rpm $benchmark $ver; then
		debug_log "[$script_name]$benchmark is installed"
	else
		error_log "[$script_name]$benchmark installation failed, exiting"
		exit 1
	fi
}

function print_iteration {
	# printing a iteration assumes this must be a new row, so include \n first
	printf "\n%28s" "$1" >>$benchmark_summary_txt_file
	printf "\n%s" "$1" >>$benchmark_summary_csv_file
	printf "\n%28s <a href=./$iteration/reference-result/uperf.html>%s</a> <a href=./$iteration/reference-result/tools-$tool_group>%s</a>" "$1" "summary" "tools">>$benchmark_summary_html_file
}

function print_value {
	printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_txt_file
	printf "%s" ",$1,$2" >>$benchmark_summary_csv_file
	printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_html_file
}

function print_header {
	printf "\n\n%28s" "iteration" >>$benchmark_summary_txt_file
	printf "\n\n%s" "iteration" >>$benchmark_summary_csv_file
	printf "\n\n%28s %s %s" "iteration" "       " "     " >>$benchmark_summary_html_file
	for i in $1; do
		printf "%${spacing}s" "$i" >>$benchmark_summary_txt_file
		printf "%s" ",$i,stddevpct" >>$benchmark_summary_csv_file
		printf "%${spacing}s" "$i" >>$benchmark_summary_html_file
	done
}


# Process options and arguments
opts=$(getopt -q -o i:c:t:r:m:p:M:S:C: --longoptions "server-node:,server-nodes:,client-node:,client-nodes:,client-label:,server-label:,tool-label-pattern:,install,start-iteration-num:,config:,instances:,test-types:,runtime:,message-sizes:,protocols:,samples:,client:,clients:,servers:,server:,max-stddev:,max-failures:,kvm-host:,log-response-times:,postprocess-only:,run-dir:,tool-group:" -n "getopt.sh" -- "$@")
if [ $? -ne 0 ]; then
	printf -- "$*\n"
	printf "\n"
	printf "\t${benchmark}: you specified an invalid option\n\n"
	printf "\tThe following options are available:\n\n"
	#              1   2         3         4         5         6         7         8         9         0         1         2         3
	#              678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
	printf -- "\t\t             --kvm-host=str\n"
	printf -- "\t\t             --tool-group=str\n"
	printf -- "\t\t-c str       --config=str               name of the test config (i.e. jumbo_frames_and_network_throughput)\n"
	printf -- "\t\t-t str[,str] --test-types=str[,str]     can be stream, maerts, bidirect, and/or rr (default $test_types)\n"
	printf -- "\t\t-r int       --runtime=int              test measurement period in seconds (default is $runtime)\n"
	printf -- "\t\t-m int[,int] --message-sizes=str[,str]  list of message sizes in bytes (default is $message_sizes)\n"
	printf -- "\t\t-p str[,str] --protocols=str[,str]      tcp and/or udp (default is $protocols)\n"
	printf -- "\t\t-i int[,int] --instances=int[,int]      list of number of uperf instances to run (default is $instances)\n"
	printf -- "\t\t-C str[,str] --client[s]=str[,str]      a list of one or more hostnames/IPs.  These systems will run the\n"
	printf    "\t\t                                        uperf client (drive the test).\n"
	printf    "\t\t                                        If this is omitted, the local system is the client\n"
	printf    "\t\t                                        Note: the number of clients and server must be the same!\n"
	printf    "\t\t                                        Clients and servers are paired according the order in the list (first\n"
	printf    "\t\t                                        client pairs with firest server, etc)\n"
	printf -- "\t\t-S str[,str] --server[s]=str[,str]      a list of one or more hostnames/IPs.  These systems will run the uperf\n"
	printf    "\t\t                                        server (listening for connections)\n"
	printf    "\t\t                                        If this is omitted, the server will listen on the local system\n"
	printf    "\t\t                                        loopback interface\n"
	printf -- "\t\t--server-node[s]=str[,str]              An ordered list of server NUMA nodes which should be used for CPU binding\n"
	printf -- "\t\t--client-node[s]=str[,str]              An ordered list of rrclient NUMA nodes which should be used for CPU binding\n"
	printf -- "\t\t                                        For both options above, the order must correspond with the --clients/--servers list\n"
	printf -- "\t\t                                        To omit a specific client/server from binding, use a value of -1\n"
	printf -- "\t\t             --samples=int              the number of times each different test is run (to compute average &\n"
	printf    "\t\t                                        standard deviations)\n"
	printf -- "\t\t             --max-failures=int         the maximm number of failures to get below stddev\n"
	printf -- "\t\t             --max-stddev=int           the maximm percent stddev allowed to pass\n"
	printf -- "\t\t             --postprocess-only=y|n     don't run the benchmark, but postprocess data from previous test\n"
	printf -- "\t\t             --run-dir=str              optionally specify what directory should be used (usually only used\n"
	printf    "\t\t                                        if postprocess-only=y)\n"
	printf -- "\t\t             --start-iteration-num=int  optionally skip the first (n-1) tests\n"
	printf -- "\t\t             --log-response-times=y|n   record the response time of every single operation\n"
	printf -- "\t\t             --tool-label-pattern=str   uperf will provide CPU and efficiency information for any tool directory\n"
	printf    "\t\t                                        with a \"^<pattern>\" in the name, provided \"sar\" is one of the\n"
	printf    "\t\t                                        registered tools.\n"
	printf    "\t\t                                        a default pattern, \"uperf-\" is used if none is provided.\n"
	printf    "\t\t                                        simply register your tools with \"--label=uperf-\$X\", and this script\n"
	printf    "\t\t                                        will genrate CPU_uperf-\$X and Gbps/CPU_uperf-\$X or\n"
	printf    "\t\t                                        trans_sec/CPU-uperf-\$X for all tools which have that pattern as a\n"
	printf    "\t\t                                        prefix.  if you don't want to register your tools with \"uperf-\" as\n"
	printf    "\t\t                                        part of the label, just use --tool-label-pattern= to tell this script\n"
	printf    "\t\t                                        the prefix pattern to use for CPU information.\n"
	exit 1
fi
eval set -- "$opts"
debug_log "[$script_name]processing options"
while true; do
	case "$1" in
		--install)
		shift
		install_uperf
		exit
		;;
		--client-label)
		shift
		if [ -n "$1" ]; then
			shift
		fi
		warn_log "The --client-label option is deprecated, please use --tool-label-pattern"
		;;
		--server-label)
		shift
		if [ -n "$1" ]; then
			shift
		fi
		warn_log "The --server-label option is deprecated, please use --tool-label-pattern"
		;;
		--tool-label-pattern)
		shift
		if [ -n "$1" ]; then
			tool_label_pattern="$1"
			shift
		fi
		;;
		--postprocess-only)
		shift
		if [ -n "$1" ]; then
			postprocess_only="$1"
			shift
		fi
		;;
		--run-dir)
		shift
		if [ -n "$1" ]; then
			benchmark_run_dir="$1"
			shift
		fi
		;;
		--max-stddev)
		shift
		if [ -n "$1" ]; then
			maxstddevpct="$1"
			shift
		fi
		;;
		--max-failures)
		shift
		if [ -n "$1" ]; then
			max_failures="$1"
			shift
		fi
		;;
		--samples)
		shift
		if [ -n "$1" ]; then
			nr_samples="$1"
			shift
		fi
		;;
		-i|--instances)
		shift
		if [ -n "$1" ]; then
			instances="$1"
			shift
		fi
		;;
		-t|--test-types)
		shift
		if [ -n "$1" ]; then
			test_types="$1"
			shift
		fi
		;;
		--kvm-host)
		shift
		if [ -n "$1" ]; then
			kvm_host="$1"
			shift
		fi
		;;
		--tool-group)
		shift
		if [ -n "$1" ]; then
			tool_group="$1"
			shift
		fi
		;;
		-m|--message-sizes)
		shift
		if [ -n "$1" ]; then
			message_sizes="$1"
			shift
		fi
		;;
		-r|--runtime)
		shift
		if [ -n "$1" ]; then
			runtime="$1"
			shift
		fi
		;;
		--log-response-times)
		shift
		if [ -n "$1" ]; then
			log_response_times="$1"
			shift
		fi
		;;
		-p|--protocols)
		shift
		if [ -n "$1" ]; then
			protocols="$1"
			shift
		fi
		;;
		-C|--client|--clients)
		shift
		if [ -n "$1" ]; then
			clients="$1"
			shift
		fi
		;;
		-S|--server|--servers)
		shift
		if [ -n "$1" ]; then
			servers="$1"
			shift
		fi
		;;
		-c|--config)
		shift
		if [ -n "$1" ]; then
			config="$1"
			shift
		fi
		;;
		--start-iteration-num)
		shift
		if [ -n "$1" ]; then
			start_iteration_num=$1
			shift
		fi
		;;
		--client-node|--client-nodes)
		shift
		if [ -n "$1" ]; then
			client_nodes="$1"
			shift
		fi
		;;
		--server-node|--server-nodes)
		shift
		if [ -n "$1" ]; then
			server_nodes="$1"
			shift
		fi
		;;
		--)
		shift
		break
		;;
		*)
		error_log "[$script_name] bad option, \"$1 $2\""
		break
		;;
	esac
done
if [[ -z "$benchmark_run_dir" ]]; then
	# We don't have an explicit run directory, construct one
	benchmark_run_dir="$pbench_run/${benchmark}_${config}_$date"
else
	# We have an explicit run directory provided by --run-dir, so warn
	# the user if they also used --config
	if [[ ! -z "$config" ]]; then
		warn_log "[$script_name] ignoring --config=\"$config\" in favor of --rundir=\"$benchmark_run_dir\""
	fi
fi

# Verify the tool group exists
verify_tool_group $tool_group

# Verify the number of clients and servers match.
if [ ! -z "$clients" -a "$postprocess_only" != "y" ]; then
	let c_cnt=0
	for client in `echo $clients | sed -e s/,/" "/g`; do
		let c_cnt=c_cnt+1
	done
	let s_cnt=0
	for server in `echo $servers | sed -e s/,/" "/g`; do
		let s_cnt=s_cnt+1
	done
	if [ $c_cnt -ne $s_cnt ]; then
		error_log "Number of clients and servers specified on command line must match"
		error_log "    clients($c_cnt): $clients"
		error_log "    servers($s_cnt): $servers"
		exit 1
	fi
fi
# before we run a test, verify clients and servers are accessible and install
# uperf if necessary
if [ "$postprocess_only" != "y" ]; then
	# Always install locally
	# FIXME: why is this necessary?
	install_uperf
	let err_cnt=0
	err_clients=""
	for client in `echo $clients | sed -e s/,/" "/g`; do
		debug_log "checking for uperf on client $client"
		ssh $ssh_opts $client pbench_uperf --install
		if [ $? -ne 0 ]; then
			let err_cnt=err_cnt+1
			err_clients="$err_clients $client"
		fi
	done
	err_servers=""
	for server in `echo $servers | sed -e s/,/" "/g`; do
		debug_log "checking for uperf on server $server"
		ssh $ssh_opts $server pbench_uperf --install
		if [ $? -ne 0 ]; then
			let err_cnt=err_cnt+1
			err_servers="$err_servers $server"
		fi
	done
	if [ $err_cnt -gt 0 ]; then
		error_log "Unable to verify connectivity and uperf installation on the following clients and servers:"
		error_log "    clients: $err_clients"
		error_log "    servers: $err_servers"
		exit 1
	fi
fi

mkdir -p $benchmark_run_dir/.running
benchmark_summary_txt_file="$benchmark_run_dir/summary-result.txt"
rm -f $benchmark_summary_txt_file
benchmark_summary_csv_file="$benchmark_run_dir/summary-result.csv"
rm -f $benchmark_summary_csv_file
benchmark_summary_html_file="$benchmark_run_dir/summary-result.html"
rm -f $benchmark_summary_html_file

## Run the benchmark and start/stop perf analysis tools
printf "# these results generated with:\n# uperf %s\n\n" "$orig_cmd" >$benchmark_summary_txt_file
printf "<pre>\n# these results generated with:\n# uperf %s\n\n" "$orig_cmd" >$benchmark_summary_html_file
printf "\n" >>$benchmark_summary_txt_file
printf "\n" >>$benchmark_summary_html_file

count=0
for protocol in `echo $protocols | sed -e s/,/" "/g`; do
	for test_type in `echo $test_types | sed -e s/,/" "/g`; do
		for message_size in `echo $message_sizes | sed -e s/,/" "/g`; do
			for instance in `echo $instances | sed -e s/,/" "/g`; do
				let count=$count+1
			done
		done
	done
done
let total_iterations=count
count=1
max_key_length=0
# disable firewall on local system
systemctl stop firewalld
export benchmark config
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
# start the server processes
for protocol in `echo $protocols | sed -e s/,/" "/g`; do
	for test_type in `echo $test_types | sed -e s/,/" "/g`; do
		for message_size in `echo $message_sizes | sed -e s/,/" "/g`; do
			for instance in `echo $instances | sed -e s/,/" "/g`; do
				if [ $count -ge $start_iteration_num ]; then
					echo "Starting iteration $iteration ($count of $total_iterations)"
					log "Starting iteration $iteration ($count of $total_iterations)"
					iteration="${count}-${protocol}_${test_type}-${message_size}B-${instance}i"
					iteration_dir="$benchmark_run_dir/$iteration"
					result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
					failures=0
					while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
						if [[ $failures -gt 0 ]]; then
							echo "Restarting iteration $iteration ($count of $total_iterations)"
							log "Restarting iteration $iteration ($count of $total_iterations)"
						fi
						mkdir -p $iteration_dir
						# each attempt at a test config requires multiple samples to get stddev
						for sample in `seq 1 $nr_samples`; do
							benchmark_results_dir="$iteration_dir/sample$sample"
							if [ "$postprocess_only" != "y" ]; then
								mkdir -p $benchmark_results_dir
								server_nr=1
								# the following loop does all of the pre-benchmark execution work
								for server in `echo $servers | sed -e s/,/" "/g`; do

									server_node=`echo "$server_nodes," | cut -d, -f$server_nr`
									client_node=`echo "$client_nodes," | cut -d, -f$server_nr`
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									xml_file="$iteration_dir/uperf-config-$server:$server_port.xml"
									benchmark_client_cmd_file="$iteration_dir/uperf-client-$server:$server_port.cmd"
									result_file=$benchmark_results_dir/result-$server:$server_port.txt
									benchmark_server_cmd_file="$iteration_dir/uperf-server-$server:$server_port.cmd"
									gen_xml $test_type $message_size $protocol $instance $server >$xml_file

									# construct the server command
									benchmark_server_cmd="/usr/local/bin/uperf -s -P $server_port"
									# adjust server command for NUMA binding
									if [ ! -z "$server_node" ]; then
										if [ $server_node -ge 0 ]; then
											benchmark_server_cmd="numactl --cpunodebind=$server_node bash -c \"$benchmark_server_cmd\""
										fi
									fi

									# create a server command file, to be used for debugging purposes or to run this uperf command later [without pbench]
									echo "$benchmark_server_cmd" >$benchmark_server_cmd_file
									chmod +x $benchmark_server_cmd_file

									# construct the client command
									if [ "$log_response_times" == "y" ]; then
										resp_opt=" -X $benchmark_results_dir/response-times-$server:$server_port.txt"
									fi
									benchmark_client_cmd="profile=${protocol}_$test_type test=$test_type nthr=$instance size=$message_size h=$server proto=$protocol runtime=${runtime}s $benchmark_bin -m $xml_file -x -a -i 1 $resp_opt -P $server_port >$result_file 2>&1"
									# adjust client command for NUMA binding
									if [ ! -z "$client_node" ]; then
										if [ $client_node -ge 0 ]; then
											benchmark_client_cmd="numactl --cpunodebind=$client_node bash -c \"$benchmark_client_cmd\""
										fi
									fi

									# create the client command file, to be used for debugging purposes or to run this uperf command later [without pbench]
									echo "$benchmark_client_cmd" >$benchmark_client_cmd_file
									chmod +x $benchmark_client_cmd_file

									# prepare test files and dirs if using remote clients
									if [ ! -z "$clients" ]; then
										client=`echo $clients | cut -d, -f$server_nr`
										ssh $ssh_opts $client mkdir -p $benchmark_results_dir
										scp $ssh_opts $xml_file $client:$xml_file >/dev/null
										scp $ssh_opts $benchmark_client_cmd_file $client:$benchmark_client_cmd_file >/dev/null
										ssh $ssh_opts $client "systemctl stop firewalld"
									fi

									# prepare test files and dirs on servers
									ssh $ssh_opts $server mkdir -p $benchmark_results_dir
									scp $ssh_opts $benchmark_server_cmd_file $server:$benchmark_server_cmd_file >/dev/null

									# start the uperf server(s)
									stop_server $server $server_port
									ssh $ssh_opts $server "systemctl stop firewalld"
									ssh $ssh_opts $server "screen -dmS uperf-server $benchmark_server_cmd_file"

									((server_nr++))
								done
								start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir

								# start the uperf clients
								echo "test sample $sample of $nr_samples"
								log "test sample $sample of $nr_samples "
								server_nr=1
								for server in `echo $servers | sed -e s/","/" "/g`; do
									server_nodeinfo=""
									client_nodeinfo=""
									server_node=`echo "$server_nodes," | cut -d, -f$server_nr`
									client_node=`echo "$client_nodes," | cut -d, -f$server_nr`
									if [ ! -z "$client_node" ]; then
										if [ $client_node -ge 0 ]; then
											client_nodeinfo="node[$client_node]"
										fi
									fi
									if [ ! -z "$server_node" ]; then
										if [ $server_node -ge 0 ]; then
											server_nodeinfo="node[$server_node]"
										fi
									fi
									client=`echo $clients | cut -d, -f$server_nr`
									if [ -z "$client" ]; then
										client="127.0.0.1"
									fi
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									benchmark_client_cmd_file="$iteration_dir/uperf-client-$server:$server_port.cmd"
									result_file=$benchmark_results_dir/result-$server:$server_port.txt
									debug_log "client[$client]${client_nodeinfo}test[$test_type]instances[$instance]size[$message_size] <-> server[$server]${server_nodeinfo}"
									echo "client[$client]${client_nodeinfo}test[$test_type]instances[$instance]size[$message_size] <-> server[$server]${server_nodeinfo}"

									# using local clients
									if [ -z "$clients" ]; then
										debug_log "screen -dmS uperf-client $benchmark_client_cmd_file"
										screen -dmS uperf-client $benchmark_client_cmd_file
									else # using remote clients
										debug_log "ssh $ssh_opts $client screen -dmS uperf-client $benchmark_client_cmd_file"
										ssh $ssh_opts $client "screen -dmS uperf-client $benchmark_client_cmd_file" 
									fi
									((server_nr++))
								done
								sleep $runtime

								# stop tools and clean up
								stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
								postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir

								server_nr=1
								for server in `echo $servers | sed -e s/,/" "/g`; do
									server_port=`echo "1000 * $server_nr + $server_base_port" | bc`
									stop_server $server $server_port
									if [ ! -z "$clients" ]; then
										client=`echo $clients | cut -d, -f$server_nr`
										result_file=$benchmark_results_dir/result-$server:$server_port.txt
										echo scp $ssh_opts $client:$result_file $result_file 
										scp $ssh_opts $client:$result_file $result_file >/dev/null
									fi
									((server_nr++))
								done
							else
								if [[ ! -d $benchmark_results_dir ]]; then
									error_log "Results directory $benchmark_results_dir does not exist, skipping post-processing"
									continue
								fi
								echo "Not going to run uperf.  Only postprocesing existing data"
								log "Not going to run uperf.  Only postprocesing existing data"
							fi
							$script_path/postprocess/$benchmark-postprocess $benchmark_results_dir $iteration "$tool_label_pattern" "$tool_group"
							mv $benchmark_results_dir/uperf-average.txt $benchmark_results_dir/result.txt
						done

						# find the keys that we will compute avg & stddev
						# NOTE: we always choose "sample1" since it is
						# always present and shares the same keys with
						# every other sample
						keys=`cat $iteration_dir/sample1/result.txt  | awk -F= '{print $1}'`
						s_keys=""
						key_nr=0
						# for each key, get the average & stddev
						for key in $keys; do
							# the s_key is used in the summary reports to save space, it is just an abbreviated key
							s_key=`echo $key | cut  -d- -f2-`
							# remove the label pattern from the s_key
							s_key=`echo $s_key | sed -e s/"$tool_label_pattern"//`
							s_key=`echo $s_key | sed -e s/"transactions"/"trans"/`
							# store these in reverse order as the keys and be sure to print values in reverse order later
							s_keys="$s_key $s_keys"
							s_key_length=`echo $s_key | wc -m`
							if [ $s_key_length -gt $max_key_length ]; then
								max_key_length=$s_key_length
							fi
							iteration_samples=""
							for sample in `seq 1 $nr_samples`; do
								value=`grep -- "$key" $iteration_dir/sample$sample/result.txt | awk -F= '{print $2}'`
								iteration_samples="$iteration_samples $value"
							done
							avg_stddev_result=`avg-stddev $iteration_samples`
							samples[$key_nr]="$iteration_samples"
							avg[$key_nr]=`echo $avg_stddev_result | awk '{print $1}'`
							stddev[$key_nr]=`echo $avg_stddev_result | awk '{print $2}'`
							stddevpct[$key_nr]=`echo $avg_stddev_result | awk '{print $3}'`
							closest[$key_nr]=`echo $avg_stddev_result | awk '{print $4}'`
							if echo $key | grep -q "Throughput"; then
								if echo $key | grep -q -v "Per_Server"; then
									tput_index=$key_nr
									tput_metric=$key
								fi
							fi
							((key_nr++))
						done

						for sample in `seq 1 $nr_samples`; do
							sample_dir="sample$sample"
							# create a symlink to the result dir which most accurately represents the average result
							if [ $sample -eq ${closest[$tput_index]} ]; then
								msg="'######' $tput_metric: ${samples[$tput_index]}  average: ${avg[$tput_index]} stddev: ${stddevpct[$tput_index]}%  closest-sample: $sample"
								echo $msg | tee $iteration_dir/sample-runs-summary.txt
								log $msg
								pushd "$iteration_dir" >/dev/null; /bin/rm -rf reference-result; ln -sf $sample_dir reference-result; popd >/dev/null
							else
								# delete the tool data [and respose time log for rr tests] from the other samples to save space
								# this option is off by default
								if [ "$keep_failed_tool_data" == "n" ]; then
									/bin/rm -rf $iteration_dir/$sample_dir/tools-* $iteration_dir/$sample_dir/response-times.txt
								fi
								# since non reference-result sample data is rarely referenced, tar it up to reduce the number of files used
								if [ "$tar_nonref_data" == "y" ]; then
									pushd "$iteration_dir" >/dev/null; tar --create --xz --force-local --file=$sample_dir.tar.xz $sample_dir && /bin/rm -rf $sample_dir; popd >/dev/null
								fi
							fi
						done
						# if we did not achieve the stddevpct, then move this result out of the way and try again
						fail=0
						if [[ $(echo "if (${stddevpct[$tput_index]} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; then
							fail=1
						fi
						if [ $fail -eq 1 ]; then
							let failures=$failures+1
							msg="$iteration: the percent standard deviation (${stddevpct[$tput_index]}%) was not within maximum allowed (${maxstddevpct}%)"
							echo $msg
							log $msg
							msg="This iteration will be repeated until either standard deviation is below the maximum allowed, or $max_failures failed attempts."
							echo $msg
							log $msg
							msg="Changing the standard deviation percent can be done with --max-stddev= and the maximum failures with --max-failures="
							echo $msg
							log $msg
							# tar up the failed iteration.  We may need to look at it later, but don't waste space by keeping it uncompressed
							# if all attempts failed, leaving no good result, leave the last attempt uncompressed
							if [ $failures -le $max_failures ]; then
								pushd $benchmark_run_dir >/dev/null
								mv $iteration $iteration-fail$failures
								tar --create --xz --force-local --file=$iteration-fail$failures.tar.xz $iteration-fail$failures &&\
								/bin/rm -rf $iteration-fail$failures
								popd >/dev/null
							fi
						fi
						if [ $fail -eq 0 -o $failures -ge $max_failures ]; then
							break
						fi
					done # break out of this loop only if the $result_stddevpct & $eff_stddevpct lower than $maxstddevpct
					spacing=`echo "$max_key_length + 1" | bc`
					if [ "$last_test_type" != "$test_type" ]; then
						print_header "$s_keys"
					fi
					print_iteration $iteration
					((key_nr--))
					for s_key in $s_keys; do
						print_value "${avg[$key_nr]}" "${stddevpct[$key_nr]}%"
						((key_nr--))
					done
					echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
					log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				else
					echo "Skipping iteration $iteration ($count of $total_iterations)"
					log "Skipping iteration $iteration ($count of $total_iterations)"
				fi
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
done
printf -- "</pre>\n" >>$benchmark_summary_html_file
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end

rmdir $benchmark_run_dir/.running
