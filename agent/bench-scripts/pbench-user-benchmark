#!/bin/bash

# This is a script to run a benchmark provided by the user.  Note that this is different from most pbench
# benchmark scripts in that only 1 "iteration" is done, whatever the user provided script does.
#
# Author: Andrew Theurer
#
# To run a custom benchmark, run "user-benchmark --config="your-config-description" -- "your benchmark's executable with any options"
#			for example: user-benchmark --config="specjbb2005-4-JVMs" -- /root/SPECjbb/run_specjbb4.sh
#
# for running in docker example would be
#
# user-benchmark --config="specjbb2005-4-JVMs" -- docker run -it _docker_image_ /root/SPECjbb/run_specjbb4.sh
# for example: user-benchmark --config="docker-specjbb2005-4-JVMs" -- docker run -it r7perf /root/SPECjbb/run_specjbb4.sh

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark="pbench-user-benchmark"

# This bench-script does the following:
# 1) process pbench script arguments (--config)
# 2) run the benchmark and start/stop perf analysis tools
# 3) gather post-run state
# 4) postprocess analysis tool data
# post-processing of benchmark specific data is the responibility of the user supplied benchmark

# Defaults
config=""
tool_group=default
sysinfo=default
use_tool_triggers=0
capture_stderr=" 2>&1"

function usage {
	printf "Usage: $script_name [options] -- <script to run>\n\n"
	printf "\tThe following options are available:\n\n"
	printf -- "\t\t-C str --config=str            name of the test config\n"
	printf -- "\t\t       --tool-group=str\n"
	printf -- "\t\t       --sysinfo=str,          str = comma separated values of system information to be collected\n"
	printf -- "\t\t                                     available: $(pbench-collect-sysinfo --options)\n"
	printf -- "\t\t       --pbench-pre=str        path to the script which will be executed before tools are started\n"
	printf -- "\t\t       --pbench-post=str       path to the script which will be executed after tools are stopped and postprocessing is complete\n"
	printf -- "\t\t       --use-tool-triggers     use tool triggers instead of normal start/stop around script\n"
	printf -- "\t\t       --no-stderr-capture     do not capture stderr of the script to the result.txt file\n"
}

# Process options and arguments
opts=$(getopt -q -o C:h --longoptions "config:,help,tool-group:,sysinfo:,pbench-post:,pbench-pre:,use-tool-triggers,no-stderr-capture" -n "getopt.sh" -- "$@");
if [ $? -ne 0 ]; then
	printf -- "${script_name} $*\n"
	printf -- "\n"
	printf -- "\tunrecognized option specified\n\n"
	usage
	exit 1
fi
eval set -- "$opts";
while true; do
	case "$1" in
		-C|--config)
		shift
		if [ -n "$1" ]; then
			config="$1"
			shift
		fi
		;;
		--tool-group)
		shift
		if [ -n "$1" ]; then
			tool_group="$1"
			shift
		fi
		;;
		--sysinfo)
		shift
		if [ -n "$1" ]; then
			sysinfo="$1"
			shift
		fi
		;;
		--pbench-post)
		shift
		if [ -n "$1" ]; then
			pbench_post="$1"
			shift
		fi
		;;
		--pbench-pre)
		shift
		if [ -n "$1" ]; then
			pbench_pre="$1"
			shift
		fi
		;;
		--use-tool-triggers)
		shift
		use_tool_triggers=1
		;;
		--no-stderr-capture)
		shift
		capture_stderr=""
		;;
		-h|--help)
		usage
		exit 0
		;;
		--)
		shift
		break
		;;
		*)
		echo "what happened? [$1]"
		break
		;;
	esac
done
verify_common_bench_script_options $tool_group $sysinfo

iteration="1" # there can be only 1 iteration for user-benchmark
iteration_name="1"
benchmark_bin="$@"
benchmark_fullname="${benchmark}_${config}_${date_suffix}"
benchmark_run_dir="$pbench_run/${benchmark_fullname}"

# we'll record the iterations in this file
benchmark_iterations="$pbench_tmp/${benchmark_fullname}.iterations"
> ${benchmark_iterations}
mdlog=${benchmark_run_dir}/metadata.log

# make the run dir available to the user script
export benchmark_run_dir
benchmark_results_dir="$benchmark_run_dir/$iteration/sample1"
mkdir -p $benchmark_results_dir/.running
export benchmark_results_dir=$benchmark_results_dir

# For now, since we usually only ever run one iteration here, don't bother
# collecting sysinfo data before and after, just do it after.
# pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg

# gather up metadata for the run - this is usually called from
# pbench-collect-sysinfo (both at the beginning and end of the run)
# but since we don't call pbench-collect-sysinfo here, we need to call
# metadata-log explicitly
export benchmark config
pbench-metadata-log --group=$tool_group --dir=$benchmark_run_dir beg

# Add data to metadata.log
function record_iteration {
	local iteration=$1
	local iteration_name=$2
	local benchmark_bin=$3

	echo ${iteration} >> ${benchmark_iterations}
	echo $iteration_name | pbench-add-metalog-option ${mdlog} iterations/${iteration} iteration_name
	echo $benchmark_bin | pbench-add-metalog-option ${mdlog} iterations/${iteration} user_script
}
record_iteration ${iteration} ${iteration_name} ${benchmark_bin}
# on abnormal exit, make sure that the metadata log exists and is complete.
trap "pbench-metadata-log --group=$tool_group --dir=$benchmark_run_dir int" INT QUIT

if [[ ! -z $pbench_pre ]]; then
	log "[$script_name]: Running $pbench_pre"
	eval $pbench_pre
	if [[ $? != 0 ]]; then
		error_log "[$script_name]: the script executed using --pbench-pre flag failed"
		exit 1
	fi
fi

benchmark_run_cmd="${benchmark_run_dir}/user-benchmark.cmd"
if [ $use_tool_triggers -eq 0 ]; then
	# We are not using tool triggers, so we'll only have one iteration,
	# "1/sample1/", so for compatibility we'll place the results
	# output file there.
	_final_results_dir="${benchmark_results_dir}"
else
	# We are using tool triggers, so we'll potentially have more than
	# one iteration, so just place the results output file in the run
	# directory.
	_final_results_dir="${benchmark_run_dir}"
fi
printf "%s%s | tee %s/result.txt" "${benchmark_bin}" "${capture_stderr}" "${_final_results_dir}" > $benchmark_run_cmd
if [ $use_tool_triggers -ne 0 ]; then
	printf " | pbench-tool-trigger ${iteration} ${benchmark_run_dir} ${tool_group}" >> $benchmark_run_cmd
fi
printf "\n" >> $benchmark_run_cmd
chmod +x $benchmark_run_cmd

if [ $use_tool_triggers -eq 0 ]; then
	pbench-start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
fi

echo "Running $benchmark_bin"
log "[$script_name] Running $benchmark_bin"

SECONDS=0
$benchmark_run_cmd
benchmark_duration=$SECONDS
# make it predictable in the unittest environment
if [[ $_PBENCH_BENCH_TESTS == 1 ]]; then
	benchmark_duration=0
fi

if [ $use_tool_triggers -eq 0 ]; then
	pbench-stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	pbench-postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	ln -s $(basename ${benchmark_results_dir}) $(dirname ${benchmark_results_dir})/reference-result
else
	result_dirs=$(ls -1d ${benchmark_run_dir}/*/sample1)
	for rdir in $result_dirs; do
		iteration_dir=$(basename $(dirname ${rdir}))
		pbench-postprocess-tools --group=${tool_group} --iteration=${iteration_dir} --dir=${rdir}
		ln -s $(basename ${rdir}) $(dirname ${rdir})/reference-result
	done
fi
pbench-metadata-log --group=$tool_group --dir=$benchmark_run_dir end

if [[ ! -z $pbench_post ]]; then
	log "[$script_name]: Running $pbench_post"
	eval $pbench_post
	if [[ $? != 0 ]]; then
		error_log "[$script_name]: the script executed using pbench_post flag failed"
		exit 1
	fi
fi

$script_path/postprocess/user-benchmark-wrapper "$benchmark_run_dir" "$benchmark_duration"

pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir --sysinfo=$sysinfo end

rmdir $benchmark_results_dir/.running
