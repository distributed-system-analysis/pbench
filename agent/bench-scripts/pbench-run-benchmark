#!/usr/bin/perl
# -*- mode: perl; indent-tabs-mode: t; perl-indent-level: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=perl
#
# Author: Andrew Theurer
#
# This is a wrapper script that will run a benchmark for the user by doing the following:
# - validating the benchmark exists
# - validating the benchmark parameters and pbench parameters (via pbench-gen-iterations)
# - constructing a list of benchmark-iterations (via pbench-gen-iterations)
# - executing those iterations, with N sample-executions per iteration (via pbench-run-iteration)
# - run any post-processing for those executions
# - bundle all the data in a JSON document

use strict;
use warnings;
use File::Basename;
my $pbench_lib_path;
my $pbench_install_path;
BEGIN {
    $pbench_install_path = `getconf.py pbench_install_dir pbench-agent`;
    chomp $pbench_install_path;
    $pbench_lib_path = $pbench_install_path . "/lib";
}
use lib "$pbench_lib_path";
use JSON;
use Data::Dumper;
use PbenchCDM        qw(create_run_doc create_config_osrelease_doc create_config_cpuinfo_doc
                        create_config_netdevs_doc create_config_ethtool_doc create_config_base_doc
                        get_uuid create_bench_iter_doc create_config_doc);
use PbenchBase       qw(get_json_file put_json_file get_benchmark_names get_pbench_run_dir
                        get_pbench_install_dir get_pbench_config_dir get_pbench_bench_config_dir
                        get_benchmark_results_dir get_params remove_params get_hostname);
use PbenchAnsible    qw(ssh_hosts ping_hosts copy_files_to_hosts copy_files_from_hosts
                        remove_files_from_hosts remove_dir_from_hosts create_dir_hosts
                        sync_dir_from_hosts verify_success stockpile_hosts);

my %defaults = ( "num_samples" => 1,
                 "tool_group" => "default" );

# The only required [positional] argument is the benchmark name; verify it now
if (scalar @ARGV == 0) {
    print "You must supply at least a benchmark name:\n";
    my @benchmarks = get_benchmark_names(get_pbench_bench_config_dir);
    printf "%s\n",  join(" ", @benchmarks);
    exit;
}
my $benchmark = shift(@ARGV);
if ($benchmark eq "list") {
    my @benchmarks = get_benchmark_names(get_pbench_bench_config_dir);
    printf "%s\n",  join(" ", @benchmarks);
    exit;
}

# The rest of the parameters are --arg=val, most of which we just pass to other scripts,
my %params = get_params(@ARGV);

# Every benchmark must have at least 1 client, even if the client is the same host as the controller
if (! $params{'clients'}) {
    print "You must specify at least 1 client with --clients\n";
    exit 1;
}

# Warn if a few optional but highly recommended params are missing (can also be defined with env vars)
for my $param (qw(user-name user-email user-desc)) {
    my $env_var = uc $param;
    $env_var =~ s/-/_/g;
    if (!$params{$param}) { # if --arg_name was used, $ARG_NAME will not be used
        if (!$ENV{$env_var}) {
            printf "\n***** it is highly recommended you do one of the following:\n" .
                   "- export this variable before calling this script %s\n" .
                   "- use this parameter when calling this script: --%s\n\n", $env_var, $param;
            $params{$param} = "";
        } else {
            $params{$param} = $ENV{$env_var};
        }
    }
}

# Apply the Pbench defaults (not the benchmark defaults)
for my $def (keys %defaults) {
    if (!$params{$def}) {
        $params{$def} = $defaults{$def};
    }
}

# Prepare all the dirs for the run
my $base_bench_dir = get_benchmark_results_dir($benchmark, $params{"user-desc"});
mkdir($base_bench_dir);
my $es_dir = $base_bench_dir . "/es";
mkdir($es_dir);
for my $es_subdir (qw(run bench config metrics)) {
    mkdir($es_dir . "/" . $es_subdir);
}

# Use stockpile to collect configuration information
my @config_hosts = split(/,/, $params{"clients"});
if (-e "/tmp/stockpile") {
    print "Collecting confguration information with stockpile\n";
    stockpile_hosts(\@config_hosts, $base_bench_dir,"stockpile_output_path=".
                    $base_bench_dir . "/stockpile.json");
}

my $iteration_id = 0;
my $run_id = get_uuid;
my %last_run_doc;
print "Generating all benchmark iterations\n";
# We don't want these params passed to gen-iterations because they are not bechmark-native options
remove_params(\@ARGV, qw(user-desc user-name user-email)); 
# First call pbench-gen-iterations and only resolve any --defaults=, so that the run
# doc has the full set of params
my $get_defs_cmd = "pbench-gen-iterations " . $benchmark . " --defaults-only " . join(" ", @ARGV);
# Escape any quotes so they don't get lost before calling pbench-gen-iterations
$get_defs_cmd =~ s/\"/\\\"/g;
my @output = `$get_defs_cmd`;
if ($? != 0) {
    printf "%s\nCalling pbench-gen-iterations failed, exiting\n", $get_defs_cmd;
    exit 1;
}
mkdir($base_bench_dir);
open(my $fh, ">" . $base_bench_dir . "/iteration-list.txt");
# There can be multiple parts of a run if the user generated multiple parameter-sets
# (used a "--" in their cmdline).  Each part of the run has it's own run document, 
# but all of the run documents share the same run ID.
my $run_part = 0;
while (scalar @output > 0) {
    my $num_samples;
    my $line = shift(@output);
    if ($line =~ /^#/) {
        printf "%s\n", $line;
        next;
    }
    # Process --samples= here, since it can be different for each parameter-set,
    # and we need this below for running pbench-run-benchmark-sample N times
    if ($line =~ s/--samples=(\S+)\s*//) {
        $num_samples = $1;
    } else {
        $num_samples = $defaults{'num_samples'};
    }
    my $get_iters_cmd = "pbench-gen-iterations " . $benchmark . " " . $line;
    my @iterations = `$get_iters_cmd`;
    if ($? != 0) {
        printf "%s\nCalling pbench-gen-iterations failed, exiting\n", $get_iters_cmd;
        exit 1;
    }
    my %run_doc = create_run_doc($benchmark, $line, $params{"clients"}, $params{"servers"},
                    $params{"user-desc"}, $params{"user-name"}, $params{"user-email"},
                    "pbench", ""); #todo: include tool names
    $run_doc{'run'}{'id'} = $run_id; # use the same run ID for all run docs
    # Now run the iterations for this parameter-set
    for my $iteration_params (@iterations) {
        if ($iteration_params =~ /^#/) {
            printf "%s\n", $line;
            next;
        }
        my %iter_doc = create_bench_iter_doc(\%run_doc, $iteration_params,); 
        put_json_file(\%iter_doc, $es_dir . "/bench/iteration-" . $iter_doc{'iteration'}{'id'} . ".json");
        printf $fh "%d %s\n", $iteration_id, $iteration_params;
        printf "\n\n\niteration_ID: %d\niteration_params: %s\n", $iteration_id, $iteration_params;
        for (my $sample_id=0; $sample_id<$num_samples; $sample_id++) {
            printf "sample_ID: %d\n", $sample_id;
            my $iteration_dir = $base_bench_dir . "/iteration" . $iteration_id;
            mkdir($iteration_dir);
            my $iteration_sample_dir = $iteration_dir . "/sample" . $sample_id;
            mkdir($iteration_sample_dir);
            my $last_sample = "0";
            if ($sample_id == ($params{"num_samples"} - 1)) {
                $last_sample = "1";
            }
            my $benchmark_cmd = "pbench-run-benchmark-sample " . $es_dir . "/bench/iteration-" .
                                $iter_doc{'iteration'}{'id'} . ".json " . $iteration_sample_dir .
                                " " . $base_bench_dir . " " . $params{"tool_group"} . " " . $last_sample;
            my $benchmark_sample_output = `$benchmark_cmd`;
            my $exit_code = $?;
            print "output:\n$benchmark_sample_output";
            if ($exit_code != 0) {
                printf "Stopping because of iteration-sample exit code: %d\n", $exit_code;
                exit 1;
            }
        }
        $iteration_id++;
    }
    $run_doc{'run'}{'end'} = time * 1000; # time in milliseconds
    put_json_file(\%run_doc, $es_dir . "/run/run" . $run_part . "-" . $run_doc{'run'}{'id'} . ".json");
    %last_run_doc = %run_doc; # We need to keep at least 1 run doc to create the config docs later
    $run_part++;
}
close $fh;
# Generate the result.html (to go away once CDM/Elastic UI is available)
system($pbench_install_path . "/bench-scripts/postprocess/generate-benchmark-summary " . $benchmark . " " . $benchmark . " " . $base_bench_dir);

# Convert the stockpile data with scribe, then create CDM docs in ./es/config
if (-e $base_bench_dir . "/stockpile.json") {
    system('scl enable rh-python36 \'python3 -m venv /var/lib/pbench-agent/tmp/scribe && cd ' .
        $base_bench_dir . ' && scribe -t stockpile -ip ./stockpile.json >scribe.json\'');
    open(my $scribe_fh, "<" . $base_bench_dir . "/scribe.json") || die "Could not open " .
        $base_bench_dir . "/scribe.json";
    my $json_text = "";
    # Instead of 1 json document, there are actually multiple documents, but no seperator
    # between them or organized in an array
    while (<$scribe_fh>) {
        $json_text .= $_;
        if (/^\}/) { # Assume this is the end of a json doc
            my %config_doc = create_config_doc(\%last_run_doc, from_json($json_text));
            if ($config_doc{'cdm'}{'doctype'} =~ /^config_(.+)/) {
                my $config_subname = $1;
                if ($config_doc{'config'}{'id'}) {
                    put_json_file(\%config_doc, $es_dir . "/config/" . $config_doc{'cdm'}{'doctype'} .
                                "-" . $config_doc{'config'}{'id'} . ".json");
                } else {
                    printf "Error: config doc's config.%s not found\n", $config_subname;
                }
            } else {
                printf "Error: config doc's cdm.doctype does not start with \"config_\"\n";
                    }
            $json_text = "";
        }
    }
    close($scribe_fh);
}
