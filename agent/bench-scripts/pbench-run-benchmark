#!/usr/bin/perl
#
# Author: Andrew Theurer
#
# This is a wrapper script that will run a benchmark for the user by doing the following:
# - validating the benchmark exists
# - validating the benchmark parameters and pbench parameters (via pbench-gen-iterations)
# - constructing a list of benchmark-iterations (via pbench-gen-iterations)
# - executing those iterations, with N sample-executions per iteration (via pbench-run-iteration)
# - run any post-processing for those executions
# - bundle all the data in a JSON document

use strict;
use warnings;
use File::Basename;
my $pbench_lib_path;
BEGIN {
	$pbench_lib_path = `getconf.py pbench_install_dir pbench-agent`;
	chomp $pbench_lib_path;
	$pbench_lib_path .= "/lib";
}
use lib "$pbench_lib_path";
use JSON;
use Data::Dumper;
use PbenchCDM qw(create_run_doc create_config_osrelease_doc create_config_cpuinfo_doc create_config_netdevs_doc create_config_ethtool_doc create_config_base_doc get_hostname get_uuid create_bench_iter_doc);
use PbenchBase qw(get_json_file put_json_file get_benchmark_names get_pbench_run_dir get_pbench_install_dir get_pbench_config_dir get_pbench_bench_config_dir get_benchmark_results_dir get_params remove_params);
use PbenchAnsible qw(ssh_hosts ping_hosts copy_files_to_hosts copy_files_from_hosts remove_files_from_hosts remove_dir_from_hosts create_dir_hosts sync_dir_from_hosts verify_success);

# defaults
my $num_samples = 1; 

# the only required [positional] argument is the benchmark name; verify it now
if (scalar @ARGV == 0) {
	print "You must supply at least a benchmark name:\n";
	get_benchmark_names(get_pbench_bench_config_dir);
	exit;
}
my $benchmark = shift(@ARGV);
if ($benchmark eq "list") {
	get_benchmark_names(get_pbench_bench_config_dir);
	exit;
}
# the rest of the parameters are --arg=val, most of which we just pass to other scripts,
# but we need to know a few of them now
my $user_name;
my $user_email;
my $user_desc;
my %params = get_params(@ARGV);
if ($params{"samples"}) {
	$num_samples = $params{"samples"};
}
if ((!$ENV{"USER_NAME"}) and (!$params{"user-name"})) {
	print "\n\n***** Please export USER_NAME=\"<your first and last name>\" (or --user-name=) so it is included in the run data *****\n\n";
	sleep(1);
} else {
	$user_name = $params{"user-name"};
}
if ((!$ENV{"USER_EMAIL"}) and (!$params{"user-email"})) {
	print "\n\n***** Please export USER_EMAIL=\"<your email address\" (or --user-email=) so it is included in the run data *****\n\n";
	sleep(1);
} else {
	$user_email = $params{"user-email"};
}
if (!$params{"desc"}) {
	$user_desc = "";
	print "\n\n***** Please use --desc=key1:value2,key2:value2...keyN:valueN to describe your test configuration, environment, and/or purpose so it is included in the run data *****\n\n";
	sleep(1);
} else {
	$user_desc = $params{"desc"};
}
remove_params(\@ARGV, qw(samples desc user-name user-email)); # we don't want these params passed on to other utils
my %run_doc = create_run_doc($benchmark, join(" ", @ARGV), $params{"clients"}, $params{"servers"},
				$user_desc, $user_name, $user_email,
				"", #todo: include tool hostnames
				""); #todo: include tool names


my $base_bench_dir = get_benchmark_results_dir($benchmark, $user_desc);
mkdir($base_bench_dir);
my $es_dir = $base_bench_dir . "/es";
mkdir($es_dir);
my $es_run_dir = $es_dir . "/run";
mkdir($es_run_dir);
my $es_config_dir = $es_dir . "/config";
mkdir($es_config_dir);
my $es_bench_dir = $es_dir . "/bench";
mkdir($es_bench_dir);
my $es_metrics_dir = $es_dir . "/metrics";
mkdir($es_metrics_dir);

my @config_hosts = split(/,/, $params{"clients"});  # todo: use "config_hostnames_all" when working properly
my $remote_collection_dir = $base_bench_dir . "-config"; # we use a slightly different dir than default in case one of the hosts is also the controller
create_dir_hosts(\@config_hosts, $remote_collection_dir, $base_bench_dir);
#my $output = ssh_hosts(\@config_hosts, "/opt/pbench-agent/bench-scripts/pbench-get-config " . $run_doc{"doc_id"} . " " . $remote_collection_dir, $remote_collection_dir, $base_bench_dir);
#sync_dir_from_hosts(\@config_hosts, $remote_collection_dir . "/es/config/", $es_config_dir, $base_bench_dir);
remove_dir_from_hosts(\@config_hosts, $remote_collection_dir, $base_bench_dir);
	
# pass the rest of the args to pbench-gen-iterations and store the iterations in an array
my $gen_iterations_cmd = "pbench-gen-iterations " . $benchmark . " " . join(" ", @ARGV);
my @iterations = split(/\n/, `$gen_iterations_cmd`);
if ($? == 0) {
	printf "got %d iterations\n", scalar @iterations;
	} else {
	printf "Calling pbench-gen-iterations failed.  Exiting\n";
	printf "From pbench-gen-iterations: %s\n", @iterations;
	exit 1;
	}

# now run the iterations
mkdir($base_bench_dir);
open(my $fh, ">" . $base_bench_dir . "/iteration-list.txt");
my $iteration_id = 0;
for my $iteration_params (@iterations) {
	my %iter_doc = create_bench_iter_doc(\%run_doc, $iteration_params,); 
	put_json_file(\%iter_doc, $es_bench_dir . "/iteration-" . $iter_doc{"doc_id"} . ".json");
	printf $fh "%d %s\n", $iteration_id, $iteration_params;
	for (my $sample_id=0; $sample_id<$num_samples; $sample_id++) {
		my $iteration_dir = $base_bench_dir . "/iteration" . $iteration_id;
		mkdir($iteration_dir);
		my $iteration_sample_dir = $iteration_dir . "/sample" . $sample_id;
		mkdir($iteration_sample_dir);
		my $benchmark_cmd = "pbench-run-benchmark-sample " . $es_bench_dir . "/iteration-" . $iter_doc{"doc_id"} . ".json " . $iteration_sample_dir . " " . $base_bench_dir;
		my $benchmark_sample_output = `$benchmark_cmd`;
		my $exit_code = $?;
		print "output:\n$benchmark_sample_output";
		# todo: bail if non-zero exit code
		printf "exit code: %d\n", $exit_code;
	}
	#todo: get stdddev and repeat iteration if too high
	$iteration_id++;
}
close $fh;
put_json_file(\%run_doc, $es_run_dir . "/run.json");
