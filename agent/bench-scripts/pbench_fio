#!/bin/bash

# This is a script to run the fio benchmark

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark="fio"
benchmark_bin=/usr/local/bin/$benchmark
ver=2.2.5

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

orig_cmd="$*"

# Defaults
keep_failed_tool_data="y"
tar_nonref_data="y"
postprocess_only="n"
nr_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
supported_test_types="read,write,rw,randread,randwrite,randrw"
install_only="n"
noinstall="n"
config=""
rate_iops=""
test_types="read,randread"		# default is -non- destructive
block_sizes="4,64,1024"
targets="/tmp/fio"
directory=""
numjobs=""
runtime=30
ramptime=5
iodepth=32
ioengine="libaio"
job_mode="concurrent" # serial or concurrent
file_size="4096M"
direct=1 # don't cache IO's by default
sync=0 # don't sync IO's by default
clients="" # A list of hostnames (hosta,hostb,hostc) where you want fio to run.  Note: if you use this, pbench must be installed on these systems already.
tool_label_pattern="fio-"
tool_group="default"
max_key_length=20
primary_metric="readwrite_IOPS"

function fio_usage() {
		printf "The following options are available:\n"
		printf "\n"
		printf -- "\t-t str[,str] --test-types=str[,str]\n"
		printf "\t\tone or more of %s\n" $supported_test_types
		printf "\n"
		printf -- "\t--direct=[0/1]\n"
		printf "\t\t1 = O_DIRECT enabled (default), 0 = O_DIRECT disabled\n"
		printf "\n"
		printf -- "\t--sync=[0/1]\n"
		printf "\t\t1 = O_SYNC enabled, 0 = O_SYNC disabled (defalt)\n"
		printf "\n"
		printf -- "\t--rate-iops=int\n"
		printf "\t\tdo not exceeed this IOP rate (per job, per client)\n"
		printf "\n"
		printf -- "\t-r int --runtime=int\n"
		printf "\t\truntime in seconds (default is $runtime)\n"
		printf "\n"
		printf -- "\t--ramptime=int\n"
		printf "\t\ttime in seconds to warm up test before taking measurements (default is $ramptime)\n"
		printf "\n"
		printf -- "\t-b int[,int] --block-sizes=str[,str] (default is $block_sizes)\n"
		printf "\t\tone or more block sizes in KiB (default is $block_sizes)\n"
		printf "\n"
                printf -- "\t-s int[,int] --file-size=str[,str] (default is $file_size)\n"
                printf "\t\tfile sizes in MiB: %s\n"
                printf "\n"
		printf -- "\t-d str[,str] --targets=str[,str]\n"
		printf "\t\tone or more directories or block devices (default is $targets)\n"
		printf "\t\t(persistent names for devices highly recommended)\n"
		printf "\n"
		printf -- "\t-j str --job-mode=str    str=[serial|concurrent]  (default is $jon_mode)\n"
		printf "\n"
		printf -- "\t--ioengine=str           str= any ioengine fio supports (default is $ioengine)\n"
		printf "\n"
		printf -- "\t--clients=str[,str]      str= one or more remote systems to run fio\n"
		printf -- "\t                         If no clients are specified, fio is run locally\n"
		printf -- "\t--tool-group=str\n"
		printf "\n"
		printf -- "\t--postprocess_only=[y|n]\n"
		printf "\t\tuse this only if you want to postprocess an existing result again\n"
		printf "\t\tyou must use --run-dir option with this\n"
		printf "\n"
		printf -- "\t--run-dir=<path>\n"
		printf "\t\tprovide the path of an existig result (typically somewhere in $pbench_run\n"
		printf -- "\t--directory=<path>\n"
		printf "\t\tprovide the path to an existing directory where fio operations will be performed\n"
		printf -- "\t--numjobs=<int>\n"
		printf "\t\tnumber of jobs to run, if not given then fio default of numjobs=1 will be used\n"
}

function fio_process_options() {
	opts=$(getopt -q -o jic:t:b:s:d:r: --longoptions "help,max-stddev:,max-failures:,samples:,direct:,sync:,install,clients:,iodepth:,ioengine:,config:,jobs-per-dev:,job-mode:,rate-iops:,ramptime:,runtime:,test-types:,block-sizes:,file-size:,targets:,tool-group:,postprocess-only:,run-dir:,directory:,numjobs:,noinstall:" -n "getopt.sh" -- "$@");
	if [ $? -ne 0 ]; then
		printf "\t${benchmark}: you specified an invalid option\n\n"
		fio_usage
		exit 1
	fi
	eval set -- "$opts";
	while true; do
		case "$1" in
			--help)
			fio_usage
			exit
			;;
			--install)
			shift;
			install_only="y"
			;;
			--max-stddev)
			shift;
			if [ -n "$1" ]; then
				maxstddevpct="$1"
				shift;
			fi
			;;
			--max-failures)
			shift;
			if [ -n "$1" ]; then
				max_failures="$1"
				shift;
			fi
			;;
			--samples)
			shift;
			if [ -n "$1" ]; then
				nr_samples="$1"
				shift;
			fi
			;;
			--direct)
			shift;
			if [ -n "$1" ]; then
				direct=$1
				shift;
			fi
			;;
			--sync)
			shift;
			if [ -n "$1" ]; then
				sync=$1
				shift;
			fi
			;;
			-t|--test-types)
			shift;
			if [ -n "$1" ]; then
				test_types="$1"
				shift;
			fi
			;;
			-b|--block-sizes)
			shift;
			if [ -n "$1" ]; then
				block_sizes="$1"
				shift;
			fi
			;;
                        -s|--file-size)
                        shift;
                        if [ -n "$1" ]; then
                                file_size="$1"
                                shift;
                        fi
                        ;;
			--ramptime)
			shift;
			if [ -n "$1" ]; then
				ramptime="$1"
				shift;
			fi
			;;
			--rate-iops)
			shift;
			if [ -n "$1" ]; then
				rate_iops="$1"
				shift;
			fi
			;;
			-r|--runtime)
			shift;
			if [ -n "$1" ]; then
				runtime="$1"
				shift;
			fi
			;;
			-c|--clients)
			shift;
			if [ -n "$1" ]; then
				clients="$1"
				shift;
			fi
			;;
			-d|--targets)
			shift;
			if [ -n "$1" ]; then
				targets="$1"
				shift;
			fi
			;;
			-j|--job-mode)
			shift;
			if [ -n "$1" ]; then
				job_mode="$1"
				shift;
			fi
			;;
			-j|--config)
			shift;
			if [ -n "$1" ]; then
				config="$1"
				shift;
			fi
			;;
			--ioengine)
			shift;
			if [ -n "$1" ]; then
				ioengine="$1"
				shift;
			fi
			;;
			--iodepth)
			shift;
			if [ -n "$1" ]; then
				iodepth="$1"
				shift;
			fi
			;;
			--tool-group)
			shift;
			if [ -n "$1" ]; then
				tool_group="$1"
				shift;
			fi
			;;
			--postprocess-only)
			shift;
			if [ -n "$1" ]; then
				postprocess_only="$1"
				shift;
			fi
			;;
			--run-dir)
			shift;
			if [ -n "$1" ]; then
				run_dir="$1"
				shift;
			fi
			;;
			--directory)
			shift;
			if [ -n "$1" ]; then 
				directory="$1"
				shift;
			fi
			;; 
			--numjobs)
			shift;
			if [ -n "$1" ]; then
				numjobs="$1"
				shift;
			fi 
			;;
			--noinstall)
				shift;
				if [ -n "$1" ]; then 
					noinstall="$1"
					shift;
				fi
			;; 
			--)
			shift;
			break;
			;;
			*)
			echo "what's this? [$1]"
			shift;
			break;
			;;
		esac
	done
	if [ $postprocess_only == "n" ]; then
		benchmark_run_dir="$pbench_run/${benchmark}_${config}_$date"
	else
		if [ -z "$run_dir" ]; then
			echo "I need a directory if postprocessing an existing result (--run-dir=)"
		else
			benchmark_run_dir="$run_dir"
		fi
	fi

	verify_tool_group $tool_group
}


# Ensure the right version of the benchmark is installed
function fio_install() {
	if [ "$postprocess_only" == "n" ]; then
		if check_install_rpm $benchmark $ver; then
			debug_log "[$script_name]$benchmark is installed"
		else
			debug_log "[$script_name]$benchmark installation failed, exiting"
			exit 1
		fi 
	fi
	if [ ! -z "$clients" ] && [ "$noinstall" == "n" ] ; then
		debug_log "verifying clients have fio installed"
		echo "verifying clients have fio installed"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client ${pbench_install_dir}/bench-scripts/pbench_fio --install &
		done
		wait
	elif [ ! -z "$clients" ] && [ "$noinstall" == "y" ]; then 
		debug_log "verifying clients have fio installed" 
                echo "verifying clients have fio installed"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			if ( ssh $ssh_opts $clients "[ ! -e $benchmark_bin ]" ); then
				debug_log "[$script_name]$benchmark is not installed on $client and option noinstall specified. Cannot proceed, remove noinstall or install $benchmark manually"
                                echo "$script_name $benchmark is not installed on $client and option noinstall specified. Cannot proceed, remove noinstall or install $benchmark manually"
				exit 0 
		
			fi 
		done 
	fi
	if [ "$install_only" == "y" ]; then
		exit 0
	fi
}

function print_iteration {
	# printing a iteration assumes this must be a new row, so include \n first
	printf "\n%28s" "$1" >>$benchmark_summary_txt_file
	printf "\n%s" "$1" >>$benchmark_summary_csv_file
	if [ $1 == "iteration" ]; then
		# this is just a label, so no links here
		printf "\n%28s %s %s" "iteration" "summary" "tools">>$benchmark_summary_html_file
	else
		printf "\n%28s <a href=./$iteration/reference-result/summary-result.html>%s</a> <a href=./$iteration/reference-result/tools-$tool_group>%s</a>" "$1" "summary" "tools">>$benchmark_summary_html_file
	fi
}

function print_value {
	if [ -z "$2" ]; then
		printf "%${spacing}s" "$1" >>$benchmark_summary_txt_file
		printf "%s" ",$1,stddevpct" >>$benchmark_summary_csv_file
		printf "%${spacing}s" "$1" >>$benchmark_summary_html_file
	else
		printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_txt_file
		printf "%s" ",$1,$2" >>$benchmark_summary_csv_file
		printf "%${spacing}s" "$1[+/-$2]" >>$benchmark_summary_html_file
	fi
}

function print_newline {
	printf "\n" >>$benchmark_summary_txt_file
	printf "\n" >>$benchmark_summary_csv_file
	printf "\n" >>$benchmark_summary_html_file
}

# Make sure this devices exists
function fio_device_check() {
	local devs=$1
	local clients=$2
	local dev=""
	local client=""
	local rc=0
	if [ "$postprocess_only" == "n" ]; then
		debug_log "fio_device_check() $devs $clients"
		for dev in `echo $devs | sed -e s/,/" "/g`; do
			if echo $dev | grep -q "^/dev/"; then
				if [ ! -z "$clients" ]; then
					for client in `echo $clients | sed -e s/,/" "/g`; do
						debug_log "checking to see if $dev exists on client $client"
						ssh $ssh_opts $client "if [ -L $dev ]; then dev=`dirname $dev`/`readlink $dev`; fi; test -b $dev" || rc=1
					done
					wait
				else
					if [ -L $dev ]; then dev=`dirname $dev`/`readlink $dev`; fi; test -b $dev || rc=1
				fi
				if [ $rc -eq 1 ]; then
					debug_log "At least one client did not have block device $dev, exiting"
					exit 1
				fi
			fi
		done
	fi
}

function fio_create_jobfile() {
	local test_type="$1"
	local ioengine="$2"
	local block_size="$3"
	local iodepth="$4"
	local direct="$5"
	local sync="$6"
	local runtime="$7"
	local ramptime="$8"
	local size="$9"
	local rate_iops="${10}"
	local targets="${11}"
	local fio_job_file="${12}"
	local job_num=1
	local job_dir=`dirname "$fio_job_file"`
	local target

	mkdir -p $job_dir
	printf "[global]\n"		 >$fio_job_file
	printf "bs=%sk\n" $block_size	>>$fio_job_file
	printf "ioengine=$ioengine\n"	>>$fio_job_file
	printf "iodepth=$iodepth\n"	>>$fio_job_file
	printf "direct=$direct\n"	>>$fio_job_file
	printf "sync=$sync\n"		>>$fio_job_file
	printf "time_based=1\n"		>>$fio_job_file
	printf "runtime=$runtime\n"	>>$fio_job_file
	printf "clocksource=gettimeofday\n" >>$fio_job_file 
	printf "ramp_time=$ramptime\n" >>$fio_job_file 
	for target in `echo $targets | sed -e s/,/" "/g`; do
		printf "[job%d]\n" $job_num	>>$fio_job_file
		printf "rw=%s\n" $test_type	>>$fio_job_file
		printf "size=%s\n" "$size"	>>$fio_job_file # only used if actual file is used
		printf "write_bw_log=fio\n"	>>$fio_job_file
		printf "write_iops_log=fio\n"	>>$fio_job_file
		printf "write_lat_log=fio\n"	>>$fio_job_file
		printf "log_avg_msec=1000\n"	>>$fio_job_file
		if [ ! -z "$rate_iops" ]; then
			printf "rate_iops=$rate_iops\n"	>>$fio_job_file
		fi
		if [ ! -z "$directory" ]; then 
			printf "directory=$directory\n" >>$fio_job_file
		else
			printf "filename=%s\n" $target  >>$fio_job_file
		fi 
		if [ ! -z "$numjobs" ]; then 
			printf "numjobs=$numjobs\n"	>>$fio_job_file
		fi 
		let job_num=$job_num+1
	done

	echo "The following jobfile was created: $fio_job_file"
	cat $fio_job_file
}

function fio_run_job() {
	local iteration="$1"
	local benchmark_results_dir="$2"
	local fio_job_file="$3"
	local clients="$4"
	local bench_cmd="$benchmark_bin"
	local bench_opts="--output-format=json $fio_job_file"

	if [ ! -e $fio_job_file ]; then
		debug_log "fio jobfile could not be found: $fio_job_file"
		return
	fi
	echo "running fio job: $fio_job_file"

	mkdir -p $benchmark_results_dir
	mkdir -p $benchmark_results_dir/clients
	if [ ! -z "$clients" ]; then
		debug_log "creating directories on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client mkdir -p $benchmark_results_dir &
		done
		wait
		debug_log "opening port 8765 on firewall on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "firewall-cmd --add-port=8765/tcp >/dev/null" &
		done
		wait
		debug_log "killing any old fio process on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "killall fio >/dev/null 2>&1" &
		done
		wait
		debug_log "starting new fio process on the clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			ssh $ssh_opts $client "pushd $benchmark_results_dir >/dev/null; screen -dmS fio-server bash -c ''$bench_cmd' --server 2>&1 >client-result.txt'"
		done
		wait
	else
		mkdir -p $benchmark_results_dir/clients/localhost
	fi
	start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	local client_opts=""
	if [ ! -z "$clients" ]; then
		local max_jobs=0
		for client in `echo $clients | sed -e s/,/" "/g`; do
			client_opts="$client_opts --client=$client"
			if [ ! -z $numjobs ]; then 
				let max_jobs=$numjobs
			else
				let max_jobs=$max_jobs+1
			fi 
		done
		client_opts="$client_opts --max-jobs=$max_jobs $bench_opts"
	fi
	
	# create a command file and keep it with the results for debugging later, or user can run outside of pbench
	echo "$bench_cmd $bench_opts $client_opts" >$benchmark_results_dir/fio.cmd
	chmod +x $benchmark_results_dir/fio.cmd
	debug_log "$benchmark: Going to run [$bench_cmd $bench_opts $client_opts]"
	pushd $benchmark_results_dir >/dev/null
	$benchmark_results_dir/fio.cmd >$benchmark_results_dir/fio-result.txt
	popd >/dev/null
	stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	if [ ! -z "$clients" ]; then
		debug_log "getting log files from clients"
		for client in `echo $clients | sed -e s/,/" "/g`; do
			mkdir -p $benchmark_results_dir/clients/$client
			scp $ssh_opts $client:"$benchmark_results_dir/*.log" $benchmark_results_dir/clients/$client/ >/dev/null &
		done
		wait
		for client in `echo $clients | sed -e s/,/" "/g`; do
			mkdir -p $benchmark_results_dir/clients/$client
			ssh $ssh_opts $client /bin/rm "$benchmark_results_dir/*.log"  &
		done
		wait
	else
		mv $benchmark_results_dir/fio*.log $benchmark_results_dir/clients/localhost/
	fi
	postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
	echo "fio job complete"
}


# Run the benchmark and start/stop perf analysis tools
function fio_run_benchmark() {
	fio_device_check "$targets" "$clients"
	benchmark_summary_txt_file="$benchmark_run_dir/summary-result.txt"
	rm -f $benchmark_summary_txt_file
	benchmark_summary_csv_file="$benchmark_run_dir/summary-result.csv"
	rm -f $benchmark_summary_csv_file
	benchmark_summary_html_file="$benchmark_run_dir/summary-result.html"
	rm -f $benchmark_summary_html_file

	printf "# these results generated with:\n# $script_name %s\n\n" "$orig_cmd" >$benchmark_summary_txt_file
	printf "<pre>\n# these results generated with:\n# $script_name %s\n\n" "$orig_cmd" >$benchmark_summary_html_file
	printf "\n" >>$benchmark_summary_txt_file
	printf "\n" >>$benchmark_summary_html_file

	mkdir -p $benchmark_run_dir/.running
	local count=1
	if [ "$job_mode" = "serial" ]; then
		# if each target is separated by a space, there will be one job for each in next for loop
		targets=`echo $targets | sed -e s/,/" "/g`
	fi
	for dev in $targets; do
		for test_type in `echo $test_types | sed -e s/,/" "/g`; do
			for block_size in `echo $block_sizes | sed -e s/,/" "/g`; do
				job_num=1
				iteration="${count}-${test_type}-${block_size}KiB"
				iteration_dir=$benchmark_run_dir/$iteration
				result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
				failures=0
				while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
					if [[ $failures -gt 0 ]]; then
						echo "Restarting iteration $iteration ($count of $total_iterations)"
						log "Restarting iteration $iteration ($count of $total_iterations)"
					fi
					if [ "$postprocess_only" == "n" ]; then
						mkdir -p $iteration_dir
					else
						if [ ! -e $iteration_dir ]; then
							# if the iteration dir does not exist, look for a failed result directory or archive
							fail_nr=$failures
							((fail_nr++))
							fail_tag="-fail$fail_nr"
							failed_iteration_dir="$iteration_dir$fail_tag"
							if [ -e $failed_iteration_dir ]; then
								mv $failed_iteration_dir $iteration_dir || exit 1
							else
								failed_iteration_archive="$iteration_dir$fail_tag.tar.xz"
								if [ -e $failed_iteration_archive ]; then
									echo "using $failed_iteration_archive as $iteration_dir"
									tar -C $benchmark_run_dir -J -x -f $failed_iteration_archive || exit 1
									mv $failed_iteration_dir $iteration_dir || exit 1
								else
									echo "Could not find $iteration_dir, $failed_iteration_dir, or $failed_iteration_archive"
								fi
							fi
						fi
					fi
					if [ -e $iteration_dir ]; then
						iteration_failed=0
						# each attempt at a test config requires multiple samples to get stddev
						sample_failed=0
						for sample in `seq 1 $nr_samples`; do
							if [ "$job_mode" = "serial" ]; then
								dev_short_name="`basename $dev`"
								# easier to identify what job used what device when having 1 job per device
								iteration="$iteration-${dev_short_name}"
							fi
							benchmark_results_dir="$iteration_dir/sample$sample"
							benchmark_tools_dir="$benchmark_results_dir/tools-$tool_group"
							if [ "$postprocess_only" == "n" ]; then
								mkdir -p $benchmark_results_dir
								fio_job_file="$benchmark_results_dir/fio.job"
								fio_create_jobfile "$test_type" "$ioengine" "$block_size" "$iodepth" "$direct" "$sync" "$runtime" "$ramptime" "$file_size" "$rate_iops" "$dev" "$fio_job_file"
								fio_run_job "$iteration" "$benchmark_results_dir" "$fio_job_file" "$clients"
							else
								# if we are only postprocessing, then we might have to untar an existing result
								pushd $iteration_dir >/dev/null
								if [ ! -e sample$sample ]; then
									if [ -e sample$sample.tar.xz ]; then
										tar Jxf sample$sample.tar.xz && /bin/rm sample$sample.tar.xz
									else
										echo "sample $sample missing.  There should be $nr_samples samples"
									fi
								fi
								popd >/dev/null
							fi
							debug_log "post-processing fio result"
							$script_path/postprocess/$benchmark-postprocess $benchmark_results_dir $iteration $tool_group
							rc=$?
							# if for any reason the benchmark postprocessing script fails, consider this a failure to get a sample
							if [ $rc -ne 0 ]; then
								debug_log "failed: $script_path/postprocess/$benchmark-postprocess $benchmark_results_dir $iteration $tool_group" 
								sample_failed=1
							fi
							if [ $sample_failed -eq 1 ]; then
								# we need all samples to be good, so bust out of testing samples now
								break
							fi
						done
						if [ $sample_failed -eq 0 ]; then
							# find the keys that we will compute avg & stddev
							# NOTE: we always choose "sample1" since it is
							# always present and shares the same keys with
							# every other sample
							keys=`cat $iteration_dir/sample1/result.txt  | awk -F= '{print $1}'`
							key_nr=0
							# for each key, get the average & stddev
							for key in $keys; do
								# the s_key is used in the summary reports to save space, it is just an abbreviated key
								s_key=`echo $key | cut  -d- -f2-`
								# remove the label pattern from the s_key
								s_key=`echo $s_key | sed -e s/"$tool_label_pattern"//`
								s_key=`echo $s_key | sed -e s/"transactions"/"trans"/`
								# store these in reverse order as the keys and be sure to print values in reverse order later
								s_keys[$key_nr]="$s_key"
								s_key_length=`echo $s_key | wc -m`
								if [ $s_key_length -gt $max_key_length ]; then
									max_key_length=$s_key_length
								fi
								iteration_samples=""
								for sample in `seq 1 $nr_samples`; do
									value=`grep -- "^$key" $iteration_dir/sample$sample/result.txt | awk -F= '{print $2}'`
									iteration_samples="$iteration_samples $value"
								done
								avg_stddev_result=`avg-stddev $iteration_samples`
								samples[$key_nr]="$iteration_samples"
								avg[$key_nr]=`echo $avg_stddev_result | awk '{print $1}'`
								avg[$key_nr]=`printf "%.2f" ${avg[$key_nr]}`
								stddev[$key_nr]=`echo $avg_stddev_result | awk '{print $2}'`
								stddevpct[$key_nr]=`echo $avg_stddev_result | awk '{print $3}'`
								stddevpct[$key_nr]=`printf "%.1f" ${stddevpct[$key_nr]}`
								closest[$key_nr]=`echo $avg_stddev_result | awk '{print $4}'`
								if echo $key | grep -q "$primary_metric"; then
									tput_index=$key_nr
									tput_metric=$key
								fi
								((key_nr++))
							done
			
							# create a symlink to the result dir which most accurately represents the average result
							for sample in `seq 1 $nr_samples`; do
								sample_dir="sample$sample"
								if [ $sample -eq ${closest[$tput_index]} ]; then
									msg="$tput_metric: ${samples[$tput_index]}  average: ${avg[$tput_index]} stddev: ${stddevpct[$tput_index]}%  closest-sample: $sample"
									rm -f $iteration_dir/sample-runs-summary.txt
									echo $msg | tee $iteration_dir/sample-runs-summary.txt
									log $msg
									pushd "$iteration_dir" >/dev/null; /bin/rm -rf reference-result; ln -sf $sample_dir reference-result; popd >/dev/null
								else
									# delete the tool data [and respose time log for rr tests] from the other samples to save space
									# this option is off by default
									if [ "$keep_failed_tool_data" == "n" ]; then
										/bin/rm -rf $iteration_dir/$sample_dir/tools-* $iteration_dir/$sample_dir/response-times.txt
									fi
									# since non reference-result sample data is rarely referenced, tar it up to reduce the number of files used
									if [ "$tar_nonref_data" == "y" ]; then
										pushd "$iteration_dir" >/dev/null; tar --create --xz --force-local --file=$sample_dir.tar.xz $sample_dir && /bin/rm -rf $sample_dir; popd >/dev/null
									fi
								fi
							done
	
							# if result is not within stddevpct, consider it a failed attempt
							if [[ $(echo "if (${stddevpct[$tput_index]} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; then
								iteration_failed=1
								msg="$iteration: the percent standard deviation (${stddevpct[$tput_index]}%) was not within maximum allowed (${maxstddevpct}%)"
								echo $msg
								log $msg
								msg="This iteration will be repeated until either standard deviation is below the maximum allowed, or $max_failures failed attempts."
								echo $msg
								log $msg
								msg="Changing the standard deviation percent can be done with --max-stddev= and the maximum failures with --max-failures="
								echo $msg
								log $msg
							fi
						else
							# failed at getting sample, so this iteration attempt is a failure
							iteration_failed=1
						fi

						# failure cleanup
						if [ $iteration_failed -eq 1 ]; then
							let failures=$failures+1
							# tar up the failed iteration.  We may need to look at it later, but don't waste space by keeping it uncompressed
							# if all attempts failed, leaving no good result, leave the last attempt uncompressed
							if [ $failures -le $max_failures ]; then
								pushd $benchmark_run_dir >/dev/null
								mv $iteration $iteration-fail$failures
								tar --create --xz --force-local --file=$iteration-fail$failures.tar.xz $iteration-fail$failures &&\
								/bin/rm -rf $iteration-fail$failures
								popd >/dev/null
							fi
						fi

						# break out of this multi-attempt iteration loop if passed or too many failures
						if [ $iteration_failed -eq 0 -o $failures -ge $max_failures ]; then
							break
						fi
					else # $iteration_dir missing
						echo "$iteration_dir missing, so this iteration is being skipped (what happened?)"
						iteration_failed=1
						break
					fi
				done
				spacing=`echo "$max_key_length + 1" | bc`
				
				((key_nr--))
				# print the labels for this group
				if [ "$last_test_type" != "$test_type" ]; then
					print_newline
					print_iteration "iteration"
					for i in `seq 0 $key_nr`; do
						print_value "${s_keys[$i]}"
					done
				fi
				# print the correspnding values
				print_iteration $iteration
				if [ $iteration_failed -eq 0 ]; then
					for i in `seq 0 $key_nr`; do
						print_value "${avg[$i]}" "${stddevpct[$i]}%"
					done
				fi

				echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
	printf "</pre>\n" >>$benchmark_summary_html_file
	printf "\n" >>$benchmark_summary_txt_file
}

function fio_print_summary() {
	cat $benchmark_summary_txt_file
}

fio_process_options "$@"
fio_install

export benchmark config
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
fio_run_benchmark
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end
fio_print_summary

rmdir $benchmark_run_dir/.running
