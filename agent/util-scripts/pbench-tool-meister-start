#!/usr/bin/env python3
# -*- mode: python -*-

"""pbench-tool-meister-start

Responsible for:

   1. Starting a redis server
   2. Loading up the tool group data for the target group into the redis server
   3. Starting the tool-data-sink process
   4. Starting all the local and remote tool meisters

When complete we leave running locally, a redis server and a tool data sink
process, and any local or remote tool meisters.

The pbench-tool-meister-stop will take care of (gracefully) stopping all of
these process, locally or remotely.
"""

import sys
import json
import os
import time
import errno
import redis
import logging

# Port number is "One Tool" in hex 0x17001
# FIXME: move to common area
redis_port = 17001

# Redis server configuration template for pbench's use
redis_conf_tmpl = """bind {hostnames}
daemonize yes
dir {tm_dir}
dbfilename pbench-redis.rdb
logfile {tm_dir}/redis.log
pidfile {tm_dir}/redis_{redis_port:d}.pid
port {redis_port:d}
"""

# FIXME: this should be moved to a shared area
channel = "tool-meister-chan"


class ToolGroup(object):
    tg_prefix = "tools-v1"

    def __init__(self, group):
        self.group = group
        _pbench_run = os.environ["pbench_run"]
        _tg_dir = os.path.join(_pbench_run, "{}-{}".format(self.tg_prefix, self.group))
        if not os.path.isdir(_tg_dir):
            raise Exception(
                "bad tool group, {}: directory {} does not exist".format(group, _tg_dir)
            )
        self.tg_dir = os.path.realpath(_tg_dir)

        # __trigger__
        try:
            with open(os.path.join(self.tg_dir, "__trigger__"), "r") as fp:
                _trigger = fp.read()
        except OSError as ex:
            if ex.errno != errno.ENOENT:
                raise
            # Ignore missing trigger file
            self.trigger = None
        else:
            if len(_trigger) == 0:
                # Ignore empty trigger file contents
                self.trigger = None
            else:
                self.trigger = _trigger

        # toolnames - Dict with tool name as the key, dictionary with host
        # names and parameters for each host
        self.toolnames = {}
        # hostnames - Dict with host name as the key, dictionary with tool
        # names and parameters for each tool
        self.hostnames = {}
        self.labels = {}
        for hdirent in os.listdir(self.tg_dir):
            if hdirent == "__trigger__":
                # Ignore handled above
                continue
            if not os.path.isdir(os.path.join(self.tg_dir, hdirent)):
                # Ignore wayward non-directory files
                continue
            # We assume this directory is a hostname.
            host = hdirent
            if host not in self.hostnames:
                self.hostnames[host] = {}
            for tdirent in os.listdir(os.path.join(self.tg_dir, host)):
                if tdirent == "__label__":
                    with open(os.path.join(self.tg_dir, host, tdirent)) as fp:
                        self.labels[host] = fp.read()
                    continue
                if tdirent.endswith("__noinstall__"):
                    # FIXME: ignore "noinstall" for now, tools are going to be
                    # in containers so this does not make sense going forward.
                    continue
                tool = tdirent
                with open(os.path.join(self.tg_dir, host, tool)) as fp:
                    tool_opts = fp.read()
                if tool not in self.toolnames:
                    self.toolnames[tool] = {}
                self.toolnames[tool][host] = tool_opts

    def get_tools(self, host):
        """Given a target host, return a dictionary with the list of tool names
        as keys, and the values being their options for that host.
        """
        tools = dict()
        for tool, opts in self.toolnames.items():
            tools[tool] = opts[host]
        return tools


def wait_for_subs(redis_server, chan, expected_tms, logger):
    """Wait for the data sink and the proper number of TMs to register, and
    when they are all registered, then record them in the "tm-pids" key.
    """
    pids = dict()
    have_ds = False
    num_tms = 0
    for payload in chan:
        try:
            json_str = payload["data"].decode("utf-8")
        except Exception:
            logger.warning("data payload in message not UTF-8, '%r'", json_str)
            continue
        logger.debug('channel payload, "%r"', json_str)
        try:
            data = json.loads(json_str)
        except json.JSONDecodeError:
            logger.warning("data payload in message not JSON, '%s'", json_str)
            continue
        # We expect the payload to look like:
        #   { "kind": "<ds|tm>",
        #     "hostname": "<hostname>",
        #     "pid": "<pid>"
        #   }
        # Where 'kind' is either 'ds' (data-sink) or 'tm' (tool-meister),
        # 'hostname' is the host name on which that entity is running, and
        # 'pid' is that entity's PID on that host.
        try:
            new_data = dict(
                kind=data["kind"], hostname=data["hostname"], pid=data["pid"]
            )
        except KeyError:
            logger.warning("unrecognized data payload in message," " '%r'", data)
            continue
        else:
            if new_data["kind"] == "ds":
                pids["ds"] = new_data
                have_ds = True
            elif new_data["kind"] == "tm":
                if "tm" not in pids:
                    pids["tm"] = []
                pids["tm"].append(new_data)
                num_tms += 1
            else:
                logger.warning("unrecognized 'kind', in data payload '%r'", data)
                continue
        if have_ds and num_tms == expected_tms:
            break
    # Record our collected pids.
    redis_server.set("tm-pids", json.dumps(pids))


def main(argv):
    """Main program for the tool meister start.
    """
    PROG = os.path.basename(argv[0])
    logger = logging.getLogger(PROG)
    logger.setLevel(logging.DEBUG)
    sh = logging.StreamHandler()
    sh.setLevel(logging.DEBUG)
    shf = logging.Formatter("%(message)s")
    sh.setFormatter(shf)
    logger.addHandler(sh)

    try:
        group = argv[1]
    except IndexError:
        group = "default"

    # 1. Load the tool group data given the tool group argument
    try:
        tool_group = ToolGroup(group)
    except Exception:
        logger.exception("failed to load tool group data")
        return 1

    try:
        benchmark_run_dir = os.environ["benchmark_run_dir"]
        hostname = os.environ["hostname"]
        full_hostname = os.environ["full_hostname"]
    except Exception:
        logger.exception("failed to fetch parameters from the environment")
        return 1
    else:
        tm_dir = os.path.join(benchmark_run_dir, "tm")
        try:
            os.mkdir(tm_dir)
            os.chdir(tm_dir)
        except Exception:
            logger.exception("failed to create the local tool meister" " directory")
            return 1
    if not full_hostname or not hostname:
        logger.error(
            "ERROR - hostname ('%s') and full_hostname ('%s') environment"
            " variables are required",
            hostname,
            full_hostname,
        )
    if os.environ.get("_PBENCH_UNIT_TESTS"):
        hostnames = "localhost"
    else:
        hostnames = f"localhost {full_hostname}"
    params = {"hostnames": hostnames, "tm_dir": tm_dir, "redis_port": redis_port}

    # 2. Start the Redis Server (config of port from agent config)
    #   - the Redis Server is requested to create the PID file

    # Create the Redis Server pbench-specific configuration file
    redis_conf = os.path.join(tm_dir, "redis.conf")
    try:
        with open(redis_conf, "w") as fp:
            fp.write(redis_conf_tmpl.format(**params))
    except Exception:
        logger.exception("failed to create redis server configuration")
        return 1
    # Start the Redis Server itself
    #   - FIXME: use podman to start a redis server container
    redis_srvr = "redis-server"
    redis_srvr_path = os.path.join(os.path.sep, "usr", "bin", redis_srvr)
    logger.debug("starting redis server")
    try:
        retcode = os.spawnl(os.P_WAIT, redis_srvr_path, redis_srvr, redis_conf)
    except Exception:
        logger.exception("failed to create redis server, daemonized")
        return 1
    else:
        if retcode != 0:
            logger.error(
                "failed to create redis server, daemonized; return" " code: %d", retcode
            )
            return 1

    try:
        timeout = time.time() + 60
        started_channel = "{}-start".format(channel)
        redis_connection_state = "connecting"
        while redis_connection_state == "connecting":
            try:
                redis_server = redis.Redis(host="localhost", port=redis_port, db=0)
                pubsub = redis_server.pubsub()
                pubsub.subscribe(started_channel)
                chan = pubsub.listen()
                # Pull off first message which is an acknowledgement we have
                # successfully subscribed.
                resp = next(chan)
            except redis.exceptions.ConnectionError:
                if time.time() > timeout:
                    raise
            else:
                redis_connection_state = "connected"
    except Exception as exc:
        logger.error(
            "Unable to connect to redis server, %s:%d: %r", "localhost", redis_port, exc
        )
        return 1
    else:
        assert resp["type"] == "subscribe", f"bad type: f{resp!r}"
        assert resp["pattern"] is None, f"bad pattern: {resp!r}"
        assert (
            resp["channel"].decode("utf-8") == started_channel
        ), f"bad channel: {resp!r}"
        assert resp["data"] == 1, f"bad data: {resp!r}"

    # 3. Start the tool-data-sink process
    #   - leave a PID file for the tool data sink process
    #   - FIXME: use podman to start a tool-data-sink container
    tds_param_key = "tds-{}".format(group)
    tds = dict(channel=channel, benchmark_run_dir=benchmark_run_dir)
    try:
        redis_server.set(tds_param_key, json.dumps(tds))
    except Exception:
        logger.exception(
            "failed to create tool-data-sink parameter key in" " redis server"
        )
        # FIXME: fetch redis-server PID file and kill it.
        return 1
    data_sink = "pbench-tool-data-sink"
    data_sink_path = os.path.join(os.path.dirname(argv[0]), data_sink)
    logger.debug("starting tool data sink")
    try:
        retcode = os.spawnl(
            os.P_WAIT,
            data_sink_path,
            data_sink,
            "localhost",
            str(redis_port),
            tds_param_key,
        )
    except Exception:
        logger.exception("failed to create pbench data sink, daemonized")
        return 1
    else:
        if retcode != 0:
            logger.error(
                "failed to create pbench data sink, daemonized;" " return code: %d",
                retcode,
            )
            return 1

    # 4. Start all the local and remote tool meister processes
    #   - leave a PID file on each local/remote host
    #   - FIXME: use podman on the remote hosts to start a tool meister
    #            container
    failures = 0
    successes = 0
    tool_meister_cmd = "pbench-tool-meister"
    tool_meister_cmd_path = os.path.join(os.path.dirname(argv[0]), tool_meister_cmd)
    ssh_cmd = "ssh"
    args = [
        ssh_cmd,
        "<host replace me>",
        tool_meister_cmd,
        full_hostname,
        redis_port,
        "<tm param key>",
    ]
    ssh_pids = []
    for host in tool_group.hostnames.keys():
        tools = tool_group.get_tools(host)
        tm = dict(
            benchmark_run_dir=benchmark_run_dir,
            channel=channel,
            controller=full_hostname,
            group=group,
            hostname=host,
            tools=tools,
        )
        tm_param_key = "tm-{}-{}".format(group, host)
        try:
            redis_server.set(tm_param_key, json.dumps(tm))
        except Exception:
            logger.exception(
                "failed to create tool-data-sink parameter key" " in redis server"
            )
            # FIXME: fetch redis-server PID file and kill it.
            return 1
        if host == full_hostname:
            logger.debug("starting locolhost tool meister")
            try:
                retcode = os.spawnl(
                    os.P_WAIT,
                    tool_meister_cmd_path,
                    tool_meister_cmd,
                    "localhost",
                    str(redis_port),
                    tm_param_key,
                )
            except Exception:
                logger.exception("failed to create pbench tool meister," " daemonized")
                failures += 1
            else:
                if retcode == 0:
                    successes += 1
                else:
                    logger.error(
                        "failed to create pbench tool meister,"
                        " daemonized; return code: %d",
                        retcode,
                    )
                    failures += 1

            continue
        args[1] = host
        args[5] = tm_param_key
        logger.debug("starting remote tool meister")
        # FIXME: here we should consider using Ansible instead
        try:
            pid = os.spawnv(os.P_NOWAIT, ssh_cmd, args)
        except Exception:
            logger.exception(
                "failed to create a tool meister instance for" " host %s", host
            )
            failures += 1
        else:
            ssh_pids.append((pid, host))
            successes += 1
    # Wait for all the SSH pids to complete.
    for pid, host in ssh_pids:
        try:
            os.waitpid(pid, 0)
        except OSError:
            logger.exception(
                "failed to create a tool meister instance for" " host %s", host
            )
            failures += 1

    # If any successes, then we need to wait for them to show up as
    # subscribers.
    logger.debug("waiting for subs")
    wait_for_subs(redis_server, chan, successes, logger)

    # For any failures, just terminate early.
    if failures > 0:
        logger.info("terminating localhost:%d due to failures", redis_port)
        terminate_msg = dict(state="terminate", group=group, directory=None)
        try:
            ret = redis_server.publish(channel, json.dumps(terminate_msg))
        except Exception:
            logger.exception("Failed to publish terminate message")
        else:
            logger.debug("publish() = %r", ret)
        # FIXME: kill redis server PID

    return 1 if failures > 0 else 0


if __name__ == "__main__":
    status = main(sys.argv)
    sys.exit(status)
