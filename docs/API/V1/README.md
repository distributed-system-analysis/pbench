# Pbench server V1 API documentation

The documents in this set describe the APIs supported by the Pbench 1.0 server.

## Discovering the Pbench server API

Once you know the hostname of a Pbench server, you can ask for the API configuration using the [endpoints](endpoints.md) API. This will report the server's version and a list of all API end points (URI roots) supported by the server.

## Login and registration

You can register a new user (depending on the administration policy of the server) using the [register](register.md) API. If this succeeds, you can log in

You can log in as a known username by calling the [login](login.md) API, which returns a bearer schema authentication token supplied to subsequent API calls using the `authorization` header.

You can log out an active authentication token by passing it as the `authorization` header to the [logout](logout.md) API.

While logged in, you can retrieve (`GET`) and modify (`PUT`) your user profile through the [user](user.md) API.

## Dataset metadata

You can read a more complete specification of Pbench server metadata at [metadata](../metadata.md).

When a dataset is first processed, the Pbench server will populate basic metadata, including the creation timestamp, the owner of the dataset (the user authorization token used to authenticate the `PUT` from the agent via `pbench-results-move`), and the full contents of the dataset's `metadata.log` file inside the dataset tarball. The Pbench server will also calculate a default deletion date for the dataset based on the owner's retention policy and the server administrator's retention policy.

Clients can also set arbitrary metadata through the "dashboard" and "user" metadata namespaces. The "dashboard" namespace can only be modified by the owner of the dataset, and is visible to anyone with read access to the dataset. The "user" namespace is private to each authenticated user, and even if you don't own a dataset you can set your own private "user" metadata to help you categorize that dataset and to find it again.

The primary dataset `resource_id` is the computed MD5 of the dataset tarball. This is generated by the agent, and checked on the server side to ensure data consistency.

## Discovering accessible datasets

The datasets accessible by a client are limited by the dataset access controls and the client's authorization. Any client can read all "public" datasets, but only the owning user can access "private" datasets.

You can determine the datasets you're allowed to view using the [list](list.md) API. This will return the resource name, the formal resource ID, and selected metadata for each dataset the authenticated user is allowed to read. You can filter the datasets you want listed by date range, owning user, or access policy.

It may sometimes be convenient to know the date range of those datasets, without retrieving the entire list and processing the `dataset.created` timestamp for each. You can do this by calling the [daterange](daterange.md) API; for example, to establish a date picker control.

## Discovering dataset details

To get the details of the dataset's run configuration, in addition to specified server metadata, you can use the [detail](detail.md) API. This returns the resource ID of each selected dataset, along with selectable metadata providing information about the system configuration and tool configuration for the dataset.

## Managing a dataset

To make a dataset you own accessible to other users (or to make a published dataset you own private again), use the [publish](publish.md) API.

To delete a dataset you own, use [delete](delete.md).

## Accessing dataset inventory

The Pbench agent code uploads a dataset to the Pbench server in the form of a tarball with a carefully designed format. That tarball contains a hierarchical file system which the Pbench server uses to index and expose details collected by the agent during the course of a pbench benchmark run.

While there are many ways for a client to access the metrics and metadata associated with a dataset, a client can also directly access the dataset tarball inventory.

The [contents](contents.md) API exposes the directory hierarchy within the tarball, from the root directory `/` through the leaf files.

The [inventory](inventory.md) API returns the full byte stream of any regular file within the directory hierarchy, including log files, postprocessed JSON files, and benchmark result text files.

Starting from the root directory of the dataset tarball, the "contents" API allows the client to discover the entire file system structure, including all directories, regular files, and symbolic links.

The "inventory" API allows the client to retrieve any regular file for further analysis.

### Example

```
    def directory(request, level: int, name: str = "/", url: str):
        ls = request.get(url).get_json()
        print(f"{'  '*level}{name}")
        for d in ls.directories:
            directory(request, level + 1, d.name, d.url)
        for f in ls.files:
            print(f"{'  '*(level+1)}{f.name})
            bytes = request.get(f.url)
            # display byte stream:
            # inline on terminal doesn't really make sense

    directory(request, 0, "/", "http://host.example.com/api/v1/contents/<dataset>/")
```

__What do we do with a symlink?__

> It could, in theory, be a link to either a file or a directory within the tarball. In which case, the URL for that item would differ, and the client can't blindly `GET` that URL without understanding the context. This breaks the simple recursive descent algorithm I tried to model.
>
> One potential solution would be to put a symlink to a regular file in the `files` list and a symlink to a directory in the `directories` list. Note that the symlink "file object" includes the `"linkpath"` of a symlink; however the API will have to figure out whether that's a directory or a regular file, either with another Elasticsearch query or via a `FileTree` check.
>
>__NOTE__: that the most common symlink in current archives seems to be a `reference-result` link to the `sample1` directory in the same parent directory, which would be easy to check.
>
>__ALTERNATIVE__: symlinks to directories would duplicate a directory tree in a straightforward contents traversal; e.g., both `reference-result` and `sample1` trees, which are identical. Another option is to treat a symlink specially, and not produce a URL; the target will either appear elsewhere in the tree and be handled normally, or it's a broken symlink that can't be followed anyway.