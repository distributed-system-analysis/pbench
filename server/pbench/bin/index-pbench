#! /usr/bin/env python3
# -*- mode: python -*-

# prereqs:
# config file: specified in ES_CONFIG_PATH
# mapping file:
# vos.analysis.lib.tstos:

from __future__ import print_function

import sys, os, time, re, stat, copy
import hashlib, json, glob, csv, tarfile

try:
    from configparser import SafeConfigParser, Error as ConfigParserError, NoSectionError, NoOptionError
except ImportError:
    from ConfigParser import SafeConfigParser, Error as ConfigParserError, NoSectionError, NoOptionError
from optparse import OptionParser, make_option
from elasticsearch import VERSION, Elasticsearch, helpers, exceptions as es_excs
from urllib3 import exceptions as ul_excs, Timeout
from datetime import datetime, timedelta
from vos.analysis.lib import tstos

_VERSION_ = "0.1.0.0"
_NAME_    = "index-pbench"
_DEBUG    = 9

# global - defaults to normal dict, ordered dict for unittests
_dict_const = dict

# global - passed in through environment by pbench-index
_tmpdir = None


class MappingFileError(Exception):
    pass


class BadDate(Exception):
    pass


class ConfigFileNotSpecified(Exception):
    pass


class ConfigFileError(Exception):
    pass


class BadMDLogFormat(Exception):
    pass


class TemplateError(Exception):
    pass


class UnsupportedTarballFormat(Exception):
    pass


class SosreportHostname(Exception):
    pass


_ts_start = None
def ts(msg, newline=False):
    """Debugging helper for emitting a timestamp aiding timing.
    """
    global _ts_start

    now = datetime.now()
    if _ts_start:
        print(now - _ts_start, file=sys.stderr)
    _ts_start = now
    if newline:
        print(msg, file=sys.stderr)
        _ts_start = None
    else:
        print(msg, end=' ', file=sys.stderr)
    sys.stderr.flush()

def load_json_mapping(mapping_fn):
    with open(mapping_fn, "r") as mappingfp:
        try:
            mapping = json.load(mappingfp)
        except ValueError as err:
            raise MappingFileError("%s: %s" % (mapping_fn, err))
    return mapping

def fetch_mapping(mapping_fn):
    """Fetch the mapping JSON data from the given file.

    Returns a tuple consisting of the mapping name pulled from the file, and
    the python dictionary loaded from the JSON file.

    Raises MappingFileError if it encounters any problem loading the file.
    """
    mapping = load_json_mapping(mapping_fn)
    keys = list(mapping.keys())
    if len(keys) != 1:
        raise MappingFileError("Invalid mapping file: %s" % mapping_fn)
    return keys[0], mapping

def es_template(es, options, INDEX_PREFIX, INDEX_VERSION, config):
    """Load the various Elasticsearch index templates required by pbench.

    We first load the pbench-run index templates, and then we construct and
    load the templates for each tool data index.
    """
    if not es:
        # Don't do anything if we don't have an Elasticsearch client object.
        return

    index_settings = dict(
        gateway=dict(
            local=dict(
                sync='1m')),
        merge=dict(
            scheduler=dict(
                max_thread_count=1)),
        translog=dict(
            flush_threshold_size='1g'),
    )

    try:
        NUMBER_OF_SHARDS = config.get('Settings', 'number_of_shards')
    except Exception:
        pass
    else:
        index_settings['number_of_shards'] = NUMBER_OF_SHARDS

    try:
        NUMBER_OF_REPLICAS = config.get('Settings', 'number_of_replicas')
    except Exception:
        pass
    else:
        index_settings['number_of_replicas'] = NUMBER_OF_REPLICAS

    # where to find the mappings
    MAPPING_DIR = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0]))),
        'lib', 'mappings')

    run_mappings = {}
    for mapping_fn in glob.iglob(os.path.join(MAPPING_DIR, "run*.json")):
        key, mapping = fetch_mapping(mapping_fn)
        run_mappings[key] = mapping

    # The API body for the create() contains a dictionary with the settings and
    # the mappings.
    run_template_name = '%s.run' % (INDEX_PREFIX,)
    run_template_body = dict(
        template='%s.run.*' % (INDEX_PREFIX,),
        settings=index_settings,
        mappings=run_mappings)

    # Now for the tool data mappings. First we fetch the base skeleton they
    # all share.
    skel = load_json_mapping(os.path.join(MAPPING_DIR, "tool-data-skel.json"))

    # Next we load all the tool fragments
    fpat = re.compile(r'tool-data-frag-(?P<toolname>.+)\.json')
    tool_mapping_frags = {}
    for mapping_fn in glob.iglob(os.path.join(MAPPING_DIR, "tool-data-frag-*.json")):
        m = fpat.match(os.path.basename(mapping_fn))
        toolname = m.group('toolname')
        tool_mapping_frags[toolname] = load_json_mapping(mapping_fn)

    tool_templates = []
    for toolname,frag in tool_mapping_frags.items():
        tool_skel = copy.deepcopy(skel)
        tool_skel['properties'][toolname] = frag
        tool_mapping = { "pbench-tool-data-%s" % toolname: tool_skel }
        tool_template_body = dict(
            template='%s.tool-data-%s.*' % (INDEX_PREFIX, toolname),
            settings=index_settings,
            mappings=tool_mapping)
        tool_templates.append((toolname, tool_template_body))

    # Next we load all the results
    results_mappings = {}
    for mapping_fn in glob.iglob(os.path.join(MAPPING_DIR, "result-*.json")):
        key, mapping = fetch_mapping(mapping_fn)
        if options.debug_unittest:
            print("fetch_mapping: {}".format(key))
            from pprint import pprint;
            pprint(mapping)
        results_mappings[key] = mapping

    results_template_name = '%s.result-data' % (INDEX_PREFIX,)
    results_template_body = dict(
        template='%s.result-data.*' % (INDEX_PREFIX,),
        settings=index_settings,
        mappings=results_mappings)

    if not es:
        # Don't do anything if we don't have an Elasticsearch client object.
        print(run_template_name, json.dumps(run_template_body, indent=4, sort_keys=True))
        for name, body in tool_templates:
            print(name, json.dumps(body, indent=4, sort_keys=True))
        return

    # FIXME: We should query to see if a template exists, and only
    # create it if it does not, or if the version of it is older than
    # expected.
    try:
        # Pbench Run template
        res = es.indices.put_template(name=run_template_name, body=run_template_body)
        # Pbench Tool Data templates
        for name, body in tool_templates:
            tool_template_name = '%s.tool-data-%s' % (INDEX_PREFIX,name)
            res = es.indices.put_template(name=tool_template_name, body=body)
    except Exception as e:
        raise TemplateError(e)

###########################################################################
# FIXME: this is cribbed from the vos sosreport index stuff and should be
# made generic and imported.
_read_timeout = 30
_op_type = "create"

def es_index(es, actions, dbg=0):
    """
    Now do the indexing specified by the actions.
    """
    if not es:
        # If we don't have an actual ES client object, just emit a sample of
        # the actions we have been given.
        global options
        if options.debug_time_operations: ts("dumps")
        actions_l = []
        tocs = 3
        tods = 15
        for action in actions:
            if action['_type'] == 'pbench-run-toc-entry':
                tocs -= 1
                if tocs >= 0:
                    actions_l.append(action)
            elif action['_type'].startswith('pbench-tool-data'):
                tods -= 1
                if tods >= 0:
                    actions_l.append(action)
            else:
                actions_l.append(action)
        print("len(actions) = {}".format(len(actions_l)))
        print(json.dumps(actions_l, indent=4, sort_keys=True))
        if options.debug_time_operations: ts("Done", newline=True)
        return 0

    delay = _read_timeout
    tries = 20

    beg, end = time.time(), None
    start = beg
    if dbg > 0:
        print("\tbulk index (beg ts: %s) ..." % tstos(beg))
        sys.stdout.flush()
    # FIXME: Switch below to streaming_bulk() to not do this!
    all_actions = list(actions)
    while True:
        try:
            # FIXME: if action is a generator, then we are not going to work
            # well in the face of retrying on errors.  Switch to using
            # streaming_bulk() and retry using the full action in the error
            # payload.
            res = helpers.bulk(es, all_actions)
        except es_excs.ConnectionError as err:
            end = time.time()
            if isinstance(err.info, ul_excs.ReadTimeoutError):
                tries -= 1
                if tries > 0:
                    print("\t\tWARNING (end ts: %s, duration: %.2fs):"
                          " read timeout, delaying %d seconds before"
                          " retrying (%d attempts remaining)..." % (
                              tstos(end), end - beg, delay, tries),
                          file=sys.stderr)
                    time.sleep(delay)
                    delay *= 2
                    beg, end = time.time(), None
                    print("\t\tWARNING (beg ts: %s): retrying..." % (
                        tstos(beg)), file=sys.stderr)
                    if _DEBUG > 8:
                        import pdb; pdb.set_trace()
                    continue
                print("\tERROR(%s) (end ts: %s, duration: %.2fs): %s" % (
                    _NAME_, tstos(end), end - start, err), file=sys.stderr)
                return 1
        except helpers.BulkIndexError as err:
            end = time.time()
            len_errors = len(err.errors)
            error_idx = 0
            lcl_successes = 0
            lcl_duplicates = 0
            lcl_errors = 0
            for e in err.errors:
                sts = e[_op_type]['status']
                if sts not in (200, 201):
                    if _op_type == 'create' and sts == 409:
                        lcl_duplicates += 1
                    else:
                        if _DEBUG > 8:
                            import pdb; pdb.set_trace()
                        print("\t\tERRORS (%d of %d): %r" % \
                              (error_idx, len_errors, e[_op_type]['error']),
                              file=sys.stderr)
                        lcl_errors += 1
                else:
                    lcl_successes += 1
            if _DEBUG > 0 or lcl_errors > 0:
                print("\tdone (end ts: %s, duration: %.2fs,"
                      " success: %d, duplicates: %d, errors: %d)" % (
                          tstos(end), end - start, lcl_successes,
                          lcl_duplicates, lcl_errors))
                sys.stdout.flush()
            break
        except Exception as err:
            end = time.time()
            print("\tERROR(%s) (end ts: %s, duration: %.2fs): %s" % (
                _NAME_, tstos(end), end - start, err), file=sys.stderr)
            return 1
        else:
            end = time.time()
            successes = res[0]
            duplicates = 0
            errors = 0
            len_res1 = len(res[1])
            for idx, ires in enumerate(res[1]):
                sts = ires[_op_type]['status']
                if sts not in (200, 201):
                    if _op_type == 'create' and sts == 409:
                        duplicates += 1
                    else:
                        print("\t\tERROR (%d of %d): %r" % (
                            idx, len_res1, ires[_op_type]['error']),
                            file=sys.stderr)
                        errors += 1
                else:
                    successes += 1
            if dbg > 0 or errors > 0:
                print("\tdone (end ts: %s, duration: %.2fs,"
                    " success: %d, duplicates: %d, errors: %d)" % (
                        tstos(end), end - start, successes, duplicates,
                        errors))
                sys.stdout.flush()
            if errors > 0:
                if successes > 0:
                    modifier = "some "
                else:
                    modifier = ""
                    print("\tERROR(%s) %serrors encountered during indexing" % (
                        _NAME_, modifier), file=sys.stderr)
                    return 1
            break
    return 0

###########################################################################
# Benchmark result data: json files only
#

# convert a 0 value in a timeseries array to a float, else ES barfs.
def convert_to_float(d):
    if type(d) == type({}) and len(d) != 0:
        for k1 in d:
            convert_to_float(d[k1])
    elif type(d) == type([]) and len(d) != 0:
        # for {date, value} timeseries, convert the value but leave the date alone
        if type(d[0]) == type({}) and set(d[0].keys()).issuperset(set(['date', 'value'])):
            for i in range(0, len(d)):
                d[i]['value'] = float(d[i]['value'])
        else:
            for i in range(0, len(d)):
                convert_to_float(d[i])
    else:
        pass

class ResultData(object):
    def __init__(self, tstamp, md5sum, experiment, mdlog, tb):
        self.run_metadata = {
            "runtstamp": tstamp,
            "runid": md5sum,
            "experiment": experiment,
        }
        self.json_files = None
        try:
            self.json_files = ResultData.get_json_files(mdlog, tb)
        except KeyError:
            self.json_files = None

    def _make_source_json(self):
        for df in self.json_files:
            item = {}
            results = json.loads(open(os.path.join(_tmpdir, df['path'])).read())
            convert_to_float(results)
            item['results'] = results
            item['@metadata'] = self.run_metadata
            ts_str = self.run_metadata['runtstamp']
            item['@timestamp'] = ts_str
            yield item
        return

    def make_source(self):
        """Simple jump method to pick the write source generator based on the
        handler's prospectus."""
        if not self.json_files:
            # If we do not have any json files for this experiment, ignore it.
            return None
        gen = self._make_source_json()
        return gen

    @staticmethod
    def get_json_files(mdlog, tb):
        """
        Fetch the list of result.json files for this experiment;
        return a list of dicts containing their metadata.
        """
        paths = [x for x in tb.getnames() if x.find("result.json") >= 0 and tb.getmember(x).isfile()]
        datafiles = []
        for p in paths:
            fname = os.path.basename(p)
            datafile = { "path": p, "basename": fname }
            datafiles.append(datafile)

        return datafiles


###########################################################################
# Tool data routines

# Handlers data table / dictionary describing how to process a given tool's
# .csv files.  The outer dictionary holds one dictionary for each tool. Each
# tool entry has one '@prospectus' entry, which records behavior and handling
# controls, and then one entry for each .csv file name emitted by the tool.
#
# For each .csv file, we record the metric "class", and its name, "metric".
# E.g. The metadata path hierarchy for iostat disk_IOPS.csv data would be,
# 'iostat.disk.iops', for disk_Queue_Size.csv it would be 'iostat.disk.qsize',
# etc.
#
# We record the metrics "display" name (human readable, or really what is
# currently used by pbench today in generated html charts), and the "units"
# for the metric, but neither field is currently indexed (FIXME!).
#
# The "colpat" field, or column pattern, describes as a regular expression the
# constituent components of the .csv file column header. Currently, the "id"
# and "subfield" match groups are the only two expected groups, with
# "subfield" considered optional.
#
# The "subfields" field contents in the handler data structure itself is
# meant to be a programmatic way to ensure any subfield derived from a column
# header can be checked, but that is not currently used (FIXME!).
_known_csv_handlers = {
    'iostat': {
        '@prospectus': {
            # For iostat .csv files, we want to merge all columns into one
            # JSON document
            'handling': 'unify'
        },
        'disk_IOPS.csv': {
            'class': 'disk',
            'metric': 'iops',
            'display': 'IOPS',
            'units': 'count_per_sec',
            'subfields': [ 'read', 'write' ],
            'colpat': re.compile(r'(?P<id>.+)-(?P<subfield>read|write)')
        },
        'disk_Queue_Size.csv': {
            'class': 'disk',
            'metric': 'qsize',
            'display': 'Queue_Size',
            'units': 'count',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)')
        },
        'disk_Request_Merges_per_sec.csv': {
            'class': 'disk',
            'metric': 'reqmerges',
            'display': 'Request_Merges',
            'units': 'count_per_sec',
            'subfields': [ 'read', 'write' ],
            'colpat': re.compile(r'(?P<id>.+)-(?P<subfield>read|write)')
        },
        'disk_Request_Size_in_512_byte_sectors.csv': {
            'class': 'disk',
            'metric': 'reqsize',
            'display': 'Request_Size',
            'units': 'count_512b_sectors',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)')
        },
        'disk_Throughput_MB_per_sec.csv': {
            'class': 'disk',
            'metric': 'tput',
            'display': 'Throughput',
            'units': 'MB_per_sec',
            'subfields': [ 'read', 'write' ],
            'colpat': re.compile(r'(?P<id>.+)-(?P<subfield>read|write)')
        },
        'disk_Utilization_percent.csv': {
            'class': 'disk',
            'metric': 'util',
            'display': 'Utilization',
            'units': 'percent',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)')
        },
        'disk_Wait_Time_msec.csv': {
            'class': 'disk',
            'metric': 'wtime',
            'display': 'Wait_Time',
            'units': 'msec',
            'subfields': [ 'read', 'write' ],
            'colpat': re.compile(r'(?P<id>.+)-(?P<subfield>read|write)')
        }
    },
    'proc-interrupts': {
        '@prospectus': {
            # For proc-interrupts .csv files, we want to individually index
            # each column entry as its own JSON document
            'handling': 'individual'
        },
    },
    'pidstat': {
        '@prospectus': {
            # For pidstat .csv files, we want to individually index
            # each column entry as its own JSON document
            'handling': 'unify'
        },
        'context_switches_nonvoluntary_switches_sec.csv': {
            'class': 'pidstat',
            'metric': 'context_switches_nonvoluntary_switches',
            'display': 'Context_Switches_Nonvoluntary',
            'units': 'count_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'context_switches_voluntary_switches_sec.csv': {
            'class': 'pidstat',
            'metric': 'context_switches_voluntary_switches',
            'display': 'Context_Switches_Voluntary',
            'units': 'count_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'cpu_usage_percent_cpu.csv': {
            'class': 'pidstat',
            'metric': 'cpu_usage',
            'display': 'CPU_Usage',
            'units': 'percent_cpu',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'file_io_io_reads_KB_sec.csv': {
            'class': 'pidstat',
            'metric': 'io_reads',
            'display': 'IO_Reads',
            'units': 'KB_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'file_io_io_writes_KB_sec.csv': {
            'class': 'pidstat',
            'metric': 'io_writes',
            'display': 'IO_Writes',
            'units': 'KB_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'memory_faults_major_faults_sec.csv': {
            'class': 'pidstat',
            'metric': 'memory_faults_major',
            'display': 'Memory_Faults_Major',
            'units': 'count_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'memory_faults_minor_faults_sec.csv': {
            'class': 'pidstat',
            'metric': 'memory_faults_minor',
            'display': 'Memory_Faults_Minor',
            'units': 'count_per_sec',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'memory_usage_resident_set_size.csv': {
            'class': 'pidstat',
            'metric': 'rss',
            'display': 'RSS',
            'units': 'KB',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        },
        'memory_usage_virtual_size.csv': {
            'class': 'pidstat',
            'metric': 'vsz',
            'display': 'VSZ',
            'units': 'KB',
            'subfields': [],
            'colpat': re.compile(r'(?P<id>.+)'),
            'metadata': [ 'pid', 'command' ],
            'metadata_pat': re.compile(r'(?P<pid>.+?)-(?P<command>.+)')
        }
    },
    'sar': {},
    'turbostat': {},
}

_known_json_handlers = {
    'prometheus-metrics': {
        '@prospectus': {
            'handling': 'json',
         },
     },
}
# If we need to deal with old .csv file names, place an alias here to map to
# an existing handler above.
_aliases = {
    'disk_Request_Merges.csv': 'disk_Request_Merges_per_sec.csv',
    'disk_Request_Size.csv': 'disk_Request_Size_in_512_byte_sectors.csv',
    'disk_Throughput.csv': 'disk_Throughput_MB_per_sec.csv',
    'disk_Utilization.csv': 'disk_Utilization_percent.csv',
    'disk_Wait_Time.csv': 'disk_Wait_Time_msec.csv',
}


class ToolData(object):
    def __init__(self, tstamp, md5sum, experiment, iteration, sample, host, tool, toolgroup,
                 mdlog, tb):
        self.toolname = tool
        try:
            (iterseqno, itername) = iteration.split('-', 1)
        except Exception:
            iterseqno = itername = iteration
        self.run_metadata = {
            "runtstamp": tstamp,
            "runid": md5sum,
            "experiment": experiment,
            # FIXME: What are the constituent parts of an iteration name?
            "iteration": itername,
            "iterseqno": iterseqno,
            "sample": sample,
            "host": host,
            "toolgroup": toolgroup,
        }

        # Impedance match between host names used when
        # registering tools and <label>:<hostname_s>
        # convention used when collecting the results.

        # import pdb; pdb.set_trace()
        label = get_tool_label(host, mdlog)
        hostname_s = get_tool_hostname_s(host, mdlog)
        if label and hostname_s:
            hostpath = "{}:{}".format(label, hostname_s)
        elif hostname_s:
            hostpath = "{}".format(hostname_s)
        else:
            hostpath = host

        self.csv_files = None
        self.json_files = None
        try:
            self.handler = _known_json_handlers[tool]
        except KeyError:
            try:
                self.handler = _known_csv_handlers[tool]
            except KeyError:
                self.handler = None
            else:
                # Fetch all the .csv files as a dictionary containing metadata
                # about them, a csv reader object, and the header line from each
                # file.
                self.csv_files = ToolData.get_csv_files(
                    self.handler, iteration, sample, hostpath, tool, toolgroup, mdlog, tb)
        else:
            self.json_files = ToolData.get_json_files(self.handler,
                                                          iteration, sample,
                                                          hostpath, tool, toolgroup,
                                                          mdlog, tb)

    def _make_source_unified(self):
        """Process each .csv file at the same time, reading one row from each,
        unifying them into one record per column identifier when the
        timestamps match."""
        # Class list is generated from the handler data
        class_list = _dict_const()
        # The metric mapping provides (klass, metric) tuples for a given
        # .csv file.
        metric_mapping = _dict_const()
        for k,v in self.handler.items():
            if k.startswith('@'):
                continue
            class_list[v['class']] = True
            metric_mapping[k] = (v['class'], v['metric'])
        # The list of identifiers is generated from the combined headers,
        # parsing out the IDs
        identifiers = _dict_const()
        # The field mapping provides (identifier, subfield, metadata)
        # tuples for all columns of all files.
        field_mapping = _dict_const()
        # Metadata extracted from column header
        metadata = _dict_const()
        for csv in self.csv_files:
            assert csv['header'][0] == 'timestamp_ms'
            header = csv['header']
            handler = self.handler[csv['basename']]
            colpat = handler['colpat']
            if csv['basename'] not in field_mapping:
                field_mapping[csv['basename']] = _dict_const()
            for idx,col in enumerate(header):
                if idx == 0:
                    field_mapping[csv['basename']][idx] = None
                    continue
                m = colpat.match(col)
                identifier = m.group('id')
                identifiers[identifier] = True
                try:
                    subfield = m.group('subfield')
                except IndexError:
                    subfield = None
                else:
                    if subfield not in handler['subfields']:
                        raise Exception(
                            "Handler subfields, %r, do not match header"
                            " subfield name %s" % (
                                handler['subfields'], subfield))
                field_mapping[csv['basename']][idx] = (identifier, subfield)
                try:
                    metadata_pat = handler['metadata_pat']
                except KeyError:
                    pass
                else:
                    m = metadata_pat.match(col)
                    if m:
                        colmd = {}
                        for md in handler['metadata']:
                            try:
                                val = m.group(md)
                            except IndexError:
                                raise Exception(
                                    "Handler metadata, %r, not found in column"
                                    " %r using pattern %r" % (
                                        handler['metadata'], col,
                                        handler['metadata_pat']))
                            else:
                                colmd[md] = val
                        metadata[identifier] = colmd
        while True:
            # Read a row from each .csv file
            rows = _dict_const()
            for csv in self.csv_files:
                try:
                    rows[csv['basename']] = next(csv['reader'])
                except StopIteration:
                    # This should handle the case of mismatched number of
                    # rows across all .csv files. All readers which have
                    # finished will emit a StopIteration.
                    pass
            if not rows:
                # None of the csv file readers returned any rows to process,
                # so we're done.
                break
            # Verify timestamps are all the same
            tstamp = None
            first = None
            for fname in rows.keys():
                tstamp = rows[fname][0]
                if not first:
                    first = tstamp
                else:
                    assert(first == tstamp)
            # Create the base document per identifier; the timestamp is taken
            # from the "first" timestamp, converted to a floating point
            # seconds value, and then formatted as a string.
            first = int(first)/1000
            ts = datetime.utcfromtimestamp(first)
            ts_str = ts.strftime("%Y-%m-%dT%H:%M:%S.%f%z")
            datum = _dict_const()
            for identifier in identifiers.keys():
                datum[identifier] = _dict_const([
                    # Since they are all the same, we use the first to generate the
                    # real timestamp.
                    ('@timestamp', ts_str),
                    ('@metadata', self.run_metadata)
                ])
                datum[identifier][self.toolname] = _dict_const()
                for klass in class_list.keys():
                    datum[identifier][self.toolname][klass] = _dict_const([('id',identifier)])
                try:
                    md = metadata[identifier]
                except KeyError:
                    pass
                else:
                    datum[identifier][self.toolname][klass].update(md)
            # Now we can perform the mapping from multiple .csv files to JSON
            # documents using a known field hierarchy (no identifiers in field
            # names) with the identifiers as additional metadata. Note that we
            # are constructing this document just from the current row of data
            # taken from all .csv files (assumes timestamps are the same).
            for fname,row in rows.items():
                klass, metric = metric_mapping[fname]
                for idx,val in enumerate(row):
                    if idx == 0:
                        continue
                    # Given an fname and a column offset, return the
                    # identifier from the header
                    identifier, subfield = field_mapping[fname][idx]
                    if subfield:
                        if metric not in datum[identifier][self.toolname][klass]:
                            datum[identifier][self.toolname][klass][metric] = _dict_const()
                        datum[identifier][self.toolname][klass][metric][subfield] = val
                    else:
                        datum[identifier][self.toolname][klass][metric] = val
            # At this point we have fully mapped all data from all .csv files
            # to their proper fields for each identifier. Now we can yield
            # record for the identifiers.
            for key,source in datum.items():
                yield source
        return

    def _make_source_individual(self):
        """Read .csv files individually, emitting records for each row and
        column coordinate."""
        pass

    def _make_source_json(self):
        for df in self.json_files:
            payload = json.loads(open(os.path.join(_tmpdir, df['path'])).read())
            for item in payload:
                item['@metadata'] = self.run_metadata
                # any transformations needed should be done here

                # timestamp handling.
                try:
                    # unix seconds since epoch timestamp
                    ts = datetime.utcfromtimestamp(item['@timestamp'])
                    ts_str = ts.strftime("%Y-%m-%dT%H:%M:%S.%f%z")
                    item['@timestamp'] = ts_str
                except Exception:
                    # assume that item[@timestamp] is already in %Y-%m-%dT%H:%M:%S.%f%z format
                    pass

                yield item
        return

    def make_source(self):
        """Simple jump method to pick the write source generator based on the
        handler's prospectus."""
        if not self.csv_files and not self.json_files:
            # If we do not have any csv or json files for this tool, ignore it.
            return
        if self.handler['@prospectus']['handling'] == 'unify':
            gen = self._make_source_unified()
        elif self.handler['@prospectus']['handling'] == 'individual':
            gen = self._make_source_individual()
        elif self.handler['@prospectus']['handling'] == 'json':
            gen = self._make_source_json()
        else:
            raise Exception("Logic bomb!")
        return gen

    @staticmethod
    def get_csv_files(handler, iteration, sample, host, tool, toolgroup, mdlog, tb):
        """
        Fetch the list of .csv files for this tool, fetch their headers, and
        return a dictionary mapping their column headers to their field names.
        """
        path = os.path.join(iteration, sample, "tools-%s" % (toolgroup,), host, tool, "csv")
        paths = [x for x in tb.getnames() if x.find(path) >= 0 and tb.getmember(x).isfile()]
        datafiles = []
        for p in paths:
            fname = os.path.basename(p)
            try:
                handler_rec = handler[fname]
            except KeyError:
                # Try an alias
                alias_name = fname
                try:
                    fname = _aliases[alias_name]
                    handler_rec = handler[fname]
                except KeyError:
                    # Ignore .csv files for which we don't have a handler
                    #print("WARNING: no .csv handler for %s\n" % (p,), file=sys.stdout)
                    continue
            datafile = { "path": p, "basename": fname, 'handler': handler_rec }
            # FIXME: we might have to deal with UTF-8 decoding issues
            # reader = csv.reader(codecs.iterdecode(tb.extractfile(dfilepath), 'utf-8'))
            datafile['reader'] = reader = csv.reader(open(os.path.join(_tmpdir, p)))
            # FIXME: how should we handle .csv files that are empty?
            datafile['header'] = next(reader)
            datafiles.append(datafile)
        return datafiles

    @staticmethod
    def get_json_files(handler, iteration, sample, host, tool, toolgroup, mdlog, tb):
        """
        Fetch the list of json files for this tool, and
        return a list of dicts containing their metadata.
        """
        # import pdb; pdb.set_trace()
        path = os.path.join(iteration, sample, "tools-%s" % (toolgroup,), host, tool, "json")
        paths = [x for x in tb.getnames() if x.find(path) >= 0 and tb.getmember(x).isfile()]
        datafiles = []
        for p in paths:
            fname = os.path.basename(p)
            datafile = { "path": p, "basename": fname, 'handler': handler }
            datafiles.append(datafile)

        return datafiles

# tool data are stored in csv files in the tarball.
# the structure is
#      <iterN> -> sampleN -> tools-$group -> <host> -> <tool> -> csv -> files
# we have access routines for getting the iterations, samples, hosts, tools and files
# because although we prefer to get as many of these things out of the metadata log,
# that may not be possible; in the latter case, we fall back to trawling through the
# tarball and using heuristics.

def get_iterations(mdlog, tb):
    try:
        return mdlog.get("pbench", "iterations").split(', ')
    except Exception:
        # TBD - trawl through tb with some heuristics
        iterations = []
        for x in tb.getnames():
            l = x.split('/')
            if len(l) != 2:
                continue
            iter = l[1]
            if re.search('^[1-9][0-9]*-', iter):
                iterations.append(iter)
        return iterations

def get_samples(iteration, mdlog, tb):
    samples = []
    for x in tb.getnames():
        if x.find("{}/".format(iteration)) < 0:
            continue
        l = x.split('/')
        if len(l) !=  3:
            continue
        sample = l[2]
        if sample.startswith('sample'):
            samples.append(sample)
    if len(samples) == 0:
        samples.append('reference-result')
    return samples

def get_pbench_hosts(iteration, sample, mdlog, tb):
    try:
        # N.B. Space-separated list
        return mdlog.get("tools", "hosts").split()
    except ConfigParserError:
        print("ConfigParser error in get_pbench_hosts: tool data will *not* be indexed.")
        print("This is most probably a bug: please open an issue.")
        return []
    except NoSectionError:
        print("No [tools] section in metadata.log: tool data will *not* be indexed.")
        return []
    except NoOptionError:
        print("No hosts in [tools] section in metadata log: tool data will *NOT* be indexed.")
        return []

def get_tools(iteration, sample, host, mdlog, tb):
    try:
        return mdlog.options("tools/{}".format(host))
    except ConfigParserError:
        print("ConfigParser error in get_tools: tool data will *not* be indexed.")
        print("This is most probably a bug: please open an issue.")
        return []
    except NoSectionError:
        print("No [tools/{}] section in metadata.log: tool data will *not* be indexed.".format(host))
        return []
    except NoOptionError:
        print("No tools in [tools/{}] section in metadata log: tool data will *NOT* be indexed.".format(host))
        return []

def get_tool_label(host, mdlog):
    try:
        return mdlog.get("tools/" + host, "label")
    except:
        return ""

def get_tool_hostname_s(host, mdlog):
    try:
        return mdlog.get("tools/" + host, "hostname-s")
    except:
        return ""

def mk_tool_data(mdlog, md5sum, tb, tstamp):
    experiment = mdlog.get("pbench", "name")
    try:
        toolsgroup = mdlog.get("tools", "group")
    except Exception:
        toolsgroup = "default"
    iterations = get_iterations(mdlog, tb)
    for iteration in iterations:
        samples = get_samples(iteration, mdlog, tb)
        for sample in samples:
            hosts = get_pbench_hosts(iteration, sample, mdlog, tb)
            for host in hosts:
                tools = get_tools(iteration, sample, host, mdlog, tb)
                for tool in tools:
                    yield ToolData(
                        tstamp, md5sum, experiment, iteration, sample,
                        host, tool, toolsgroup, mdlog, tb)
    return

def mk_tool_data_actions(ptb, options, INDEX_PREFIX):
    index_name_template = "%s.tool-data-%%s" % INDEX_PREFIX
    for td in mk_tool_data(ptb.mdconf, ptb.md5sum, ptb.tb, ptb.start_run):
        # Each ToolData object, td, that is returned here represents how
        # data collected for that tool across all hosts is to be returned.
        # The make_source method returns a generator that will emit each
        # source document for the appropriate unit of tool data.  Each has
        # the option of constructing that data as best fits its tool data.
        # The tool data for each tool is kept in its own index to allow
        # for different curation policies for each tool.
        index_name_template_for_tool = (index_name_template % td.toolname) + '.'
        asource = td.make_source()
        if not asource:
            continue
        for source in asource:
            action = {
                "_op_type": _op_type,
                "_index":   index_name_template_for_tool + source['@timestamp'].split('T', 1)[0],
                "_type":    "pbench-tool-data-%s" % td.toolname,
                # FIXME: we should generate our own JSON doc ID
                #"_id":      source_id,
                "_source":  source
            }
            yield action
    return

def mk_result_data_actions(ptb, options, INDEX_PREFIX):
    index_name_template = "%s.result-data." % INDEX_PREFIX
    mdlog = ptb.mdconf
    experiment = mdlog.get("pbench", "name")
    rd = ResultData(ptb.start_run, ptb.md5sum, experiment, mdlog, ptb.tb)
    if not rd:
        return
    sources = rd.make_source()
    if not sources:
        return
    for source in sources:
            action = {
                "_op_type": _op_type,
                "_index":   index_name_template + source['@timestamp'].split('T', 1)[0],
                "_type":    "pbench-result-data",
                # FIXME: we should generate our own JSON doc ID
                #"_id":      source_id,
                "_source":  source
            }
            yield action

    return

###########################################################################
# Build tar ball table-of-contents (toc) source documents.

def mk_toc(tb, md5sum, options):
    members = tb.getmembers()
    prefix = members[0].name.split(os.path.sep)[0]
    # create a list of dicts where each directory that contains files is represented
    # by a dictionary with a 'name' key, whose value is the pathname of the dictionary
    # and a 'files' key, which is a list of the files contained in the directory.
    d = _dict_const()
    for m in members:
        if m.isdir():
            dname = os.path.dirname(m.name)[len(prefix):] + os.path.sep
            if dname not in d:
                d[dname] = _dict_const(directory=dname)
        elif m.isfile() or m.issym():
            dname = os.path.dirname(m.name)[len(prefix):] + os.path.sep
            fentry = _dict_const(
                    name=os.path.basename(m.name),
                    size=m.size,
                    mode=oct(m.mode)
            )
            if m.issym():
                fentry['linkpath'] = m.linkpath
            if dname in d:
                # we may have created an empty dir entry already
                # without a 'files' entry, so we now make sure that
                # there is an empty 'files' entry that we can populate.
                if 'files' not in d[dname]:
                    d[dname]['files'] = []
                d[dname]['files'].append(fentry)
            else:
                # presumably we'll never fall through here any more:
                # the directory entry always preceded the file entry
                # in the tarball.
                d[dname] = _dict_const(directory=dname, files=[fentry])
        else:
            # if debug: print(repr(m))
            continue
    return (prefix, d)

def get_md5sum_of_dir(dir, parentid):
    """Calculate the md5 sum of all the names in the toc"""
    h = hashlib.md5()
    h.update(parentid.encode('utf-8'))
    h.update(dir['directory'].encode('utf-8'))
    if 'files' in dir:
        for f in dir['files']:
            for k in sorted(f.keys()):
                h.update(repr(f[k]).encode('utf-8'))
    return h.hexdigest()

def mk_toc_actions(ptb, options, INDEX_PREFIX):
    """Construct Table-of-Contents actions.

    These are indexed into the run index along side the runs."""
    index_name_template = "%s.run.%%d-%%02d" % INDEX_PREFIX
    tstamp = ptb.start_run.replace('_', 'T')
    prefix, dirs = mk_toc(ptb.tb, ptb.md5sum, options)
    assert(prefix == ptb.dirprefix)
    for dname,d in dirs.items():
        d["@timestamp"] = tstamp
        action = {
            "_id":      get_md5sum_of_dir(d, ptb.md5sum),
            "_op_type": _op_type,
            "_index":   index_name_template % (int(ptb.date[0]), int(ptb.date[1])),
            "_type":    "pbench-run-toc-entry",
            "_source":  d,
            "_parent":  ptb.md5sum
        }
        yield action
    return

###########################################################################
# Build run source document

# routines for handling sosreports, hostnames, and tools
def valid_ip(address):
    import socket
    try:
        socket.inet_aton(address)
        return True
    except Exception:
        return False

def search_by_host(sos_d_list, host):
    for sos_d in sos_d_list:
        if host in sos_d.values():
            return sos_d['hostname-f']
    return None

def search_by_ip(sos_d_list, ip):
    # import pdb; pdb.set_trace()
    for sos_d in sos_d_list:
        for l in sos_d.values():
            if type(l) != type([]):
                continue
            for d in l:
                if type(d) != type({}):
                    continue
                if ip in d.values():
                    return sos_d['hostname-f']
    return None

def get_hostname_f_from_sos_d(sos_d, host=None, ip=None):
    if not host and not ip:
        return None

    if host:
        return search_by_host(sos_d, host)
    else:
        return search_by_ip(sos_d, ip)

def mk_tool_info(sos_d, mdconf):
    """Return a dict containing tool info (local and remote)"""
    try:
        tools_array = []
        # from pprint import pprint; pprint(mdconf.get("tools", "hosts"))

        labels = {}
        for host in mdconf.get("tools", "hosts").split():
            # from pprint import pprint; pprint(host)
            tools_info = {}
            # XXX - we should have an FQDN for the host but
            # sometimes we have only an IP address.
            tools_info['hostname'] = host
            # import pdb; pdb.set_trace()
            if valid_ip(host):
                tools_info['hostname-f'] = get_hostname_f_from_sos_d(sos_d, ip=host)
            else:
                tools_info['hostname-f'] = get_hostname_f_from_sos_d(sos_d, host=host)
            section = "tools/%s" % (host)
            items = mdconf.items(section)
            options = mdconf.options(section)
            if 'label' in options:
                tools_info['label'] = mdconf.get(section, 'label')

            # process remote entries for a label and remember them in the labels dict
            remoteitems = [x for x in items if x[0].startswith('remote@') and x[1]]
            for (k,v) in remoteitems:
                host = k.replace('remote@', '', 1)
                labels[host] = v

            # now, forget about any label or remote entries - they have been dealt with.
            items = [x for x in items if x[0] != 'label' and not x[0].startswith('remote')]

            tools = {}
            tools.update(items)
            tools_info['tools'] = tools
            tools_array.append(tools_info)

        # now process remote labels
        for item in tools_array:
            host = item['hostname']
            if host in labels:
                item['label'] = labels[host]

        return tools_array

    except Exception as err:
        print("mk_tool_info" , err)
        return []

def ip_address_to_ip_o_addr(s):
    # This routine deals with the contents of either the ip_-o_addr
    # (preferred) or the ip_address file in the sosreport.
    # If each line starts with a number followed by a colon,
    # leave it alone and return it as is - that's the preferred case.
    # If not, grovel through the ip_address file, collect the juicy pieces
    # and fake a string that is similar in format to the preferred case -
    # at least similar enough to satisfy the caller of this function.
    as_is = True
    pat = re.compile(r'^[0-9]+:')

    # reduce is not available in python3 :-(
    # as_is = reduce(lambda x, y: x and y,
    #               map(lambda x: re.match(pat, x), s.split('\n')[:-1]))
    for l in s.split('\n')[:-1]:
        if not re.match(pat, l):
            as_is = False
            break
    if as_is:
        return s

    # rats - we've got to do real work
    # state machine
    # start: 0
    # seen <N:>: 1
    # seen inet* : 2
    # EOF: 3
    # if we see anything else, we stay put in the current state
    # transitions: 2 --> 1  action: output a line
    #              2 --> 2  action: output a line
    #
    state = 0
    ret = ""
    # import pdb; pdb.set_trace()
    for l in s.split('\n'):
        if re.match(pat, l):
            if state == 0 or state == 1:
                state = 1
            elif state == 2:
                ret += "%s: %s %s %s\n" % (serial, ifname, proto, addr)
                state = 1
            serial, ifname = l.split(':')[0:2]
        elif l.lstrip().startswith('inet'):
            assert(state != 0)
            if state == 1:
                state = 2
            elif state == 2:
                ret += "%s: %s %s %s\n" % (serial, ifname, proto, addr)
                state = 2
            proto, addr = l.lstrip().split()[0:2]
    if state == 2:
        ret += "%s: %s %s %s\n" % (serial, ifname, proto, addr)
        state = 3
    return ret

def if_ip_from_sosreport(ip_addr_f):
    """Parse the ip_-o_addr file or ip_address file from the sosreport and
    get a dict associating the if name with the ip - separate entries
    for inet and inet6.
    """

    s = str(ip_addr_f.read(), 'iso8859-1')
    d = {}
    # if it's an ip_address file, convert it to ip_-o_addr format
    s = ip_address_to_ip_o_addr(s)
    for l in s.split('\n'):
        fields = l.split()
        if not fields:
            continue
        ifname = fields[1]
        ifproto = fields[2]
        ifip = fields[3].split('/')[0]
        if ifproto not in d:
            d[ifproto] = []
        d[ifproto].append({"ifname": ifname, "ipaddr" : ifip})

    return d

def hostnames_if_ip_from_sosreport(sos):
    """Return a dict with hostname info (both short and fqdn) and
    ip addresses of all the network interfaces we find at sosreport time."""

    sostb = tarfile.open(fileobj=sos)
    hostname_files = [x for x in sostb.getnames() if x.find('sos_commands/general/hostname') >= 0]

    # Fetch the hostname -f and hostname file contents
    hostname_f_file = [x for x in hostname_files if x.endswith('hostname_-f')]
    if hostname_f_file:
        try:
            hostname_f = str(sostb.extractfile(hostname_f_file[0]).read(), 'iso8859-1')[:-1]
        except IOError as e:
            raise SosreportHostname("Failure to fetch a hostname-f from the sosreport")
        if hostname_f == 'hostname: Name or service not known':
            hostname_f = ""
    else:
        hostname_f = ""
    hostname_s_file = [x for x in hostname_files if x.endswith('hostname')]
    if hostname_s_file:
        try:
            hostname_s = str(sostb.extractfile(hostname_s_file[0]).read(), 'iso8859-1')[:-1]
        except IOError as e:
            raise SosreportHostname("Failure to fetch a hostname from the sosreport")
    else:
        hostname_s = ""

    if not hostname_f and not hostname_s:
        raise SosreportHostname("We do not have a hostname recorded in the sosreport")

    # import pdb; pdb.set_trace()
    if hostname_f and hostname_s:
        if hostname_f == hostname_s:
            # Shorten the hostname if possible
            hostname_s = hostname_f.split('.')[0]
        elif hostname_f.startswith(hostname_s):
            # Already have a shortened hostname
            pass
        elif hostname_s.startswith(hostname_f):
            # switch them
            x = hostname_s
            hostname_s = hostname_f
            hostname_f = x
        elif hostname_f != "localhost":
            hostname_s = hostname_f.split('.')[0]
        elif hostname_s != "localhost":
            hostname_f = hostname_s
        else:
            assert(1 == 0)

    elif not hostname_f and hostname_s:
        # The sosreport did not include, or failed to properly collect, the
        # output from "hostname -f", so we'll just keep them the same
        hostname_f = hostname_s
        # Shorten the hostname if possible
        hostname_s = hostname_f.split('.')[0]
    elif hostname_f and not hostname_s:
        # Shorten the hostname if possible
        hostname_s = hostname_f.split('.')[0]
    else:
        # both undefined
        raise SosreportHostname("Hostname undefined (both short and long)")

    if hostname_f == "localhost" and hostname_s != "localhost":
        hostname_f = hostname_s
        hostname_s = hostname_f.split('.')[0]
    elif hostname_f != "localhost" and hostname_s == "localhost":
        hostname_s = hostname_f.split('.')[0]
    elif hostname_f == "localhost" and hostname_s == "localhost":
        raise SosreportHostname("The sosreport did not collect a hostname other than 'localhost'")
    else:
        pass

    d = {
        'hostname-f': hostname_f,
        'hostname-s': hostname_s
    }

    # get the ip addresses for all interfaces
    ipfiles = [x for x in sostb.getnames() if x.find('sos_commands/networking/ip_') >= 0]
    ip_files = [x for x in ipfiles if x.find('sos_commands/networking/ip_-o_addr') >= 0]
    if ip_files:
        d.update(if_ip_from_sosreport(sostb.extractfile(ip_files[0])))
    else:
        # try the ip_address file instead
        ip_files = [x for x in ipfiles if x.find('sos_commands/networking/ip_address') >= 0]
        if ip_files:
            d.update(if_ip_from_sosreport(sostb.extractfile(ip_files[0])))
    return d

def mk_sosreports(tb):
    sosreports = [ x for x in tb.getnames() if x.find("sosreport") >= 0 ]
    sosreports.sort()

    sosreportlist = []
    for x in sosreports:
        if x.endswith('.md5'):
            # x is the *sosreport*.tar.xz.md5 filename
            d = {}
            sos = x[:x.rfind('.md5')]
            d['name'] = sos
            d['md5'] = tb.extractfile(x).read().decode('ascii')[:-1]
            # get hostname (short and FQDN) from sosreport
            d.update(hostnames_if_ip_from_sosreport(tb.extractfile(sos)))
            sosreportlist.append(d)

    return sosreportlist

def mk_pbench_metadata(mdconf):
    d = {}
    try:
        d['pbench-rpm-version'] = mdconf.get("pbench", "rpm-version")
    except Exception:
        d['pbench-rpm-version'] = "Unknown"
    return d

def _to_utc(l, u):
    """l is a timestamp in local time and u is one in UTC. These are
    supposed to be close, so we calculate an offset rounded to a
    half-hour and add it to the local date. We then convert back to an
    ISO format date.
    """
    lts = datetime.strptime(l, "%Y-%m-%dT%H:%M:%S")
    uts = datetime.strptime(u[:u.rfind('.')] , "%Y-%m-%dT%H:%M:%S")

    offset = lts - uts
    res = round(float(offset.seconds) / 60 / 60 + offset.days * 24, 1)
    return (lts - timedelta(0, int(res*60*60), 0)).isoformat()

def fix_date_format(ts):
    """ts is a string representing a timestamp with possible formatting
    problems:

        - it might have an underscore instead of a T separating the
          date from the time - ES does not like that.

        - the date part might also be of the form YYYYMMDD instead of
          the approved format YYYY-MM-DD.

    This function fixes up those problems and returns the possibly
    modified string.
    """
    rts = ts.replace('_', 'T')
    ns = rts[rts.rfind('.'):]
    rts = rts[:rts.rfind('.')]
    try:
        val = datetime.strptime(rts, "%Y-%m-%dT%H:%M:%S")
        return rts + ns
    except ValueError as e:
        val = datetime.strptime(rts, "%Y%m%dT%H:%M:%S")
        return datetime.strftime(val, "%Y-%m-%dT%H:%M:%S") + ns
    except Exception as e:
        # on any other exception, we give up
        return "1900-01-01T00:00:00"

def mk_run_metadata(mdconf, md5sum, dirname, dirprefix):
    try:
        d = {}
        d.update(mdconf.items('run'))
        d.update(mdconf.items('pbench'))
        # Fix up old format dates so ES won't barf on them.
        d['start_run'] = fix_date_format(d['start_run'])
        d['end_run'] = fix_date_format(d['end_run'])
        # Convert the date to UTC: date and start-run are supposed to be close
        # (except that date may be in local time and start-run is UTC).
        d['date'] = _to_utc(fix_date_format(d['date']), d['start_run'])
        # the run id here is the md5sum that's the _id of the main document.
        # it's what ties all of the relevant documents together.
        d['id'] = md5sum
        d['tarball-dirname'] = dirname
        d['tarball-toc-prefix'] = dirprefix
        del d['rpm-version']
        return d
    except KeyError as e:
        print("Tarball: %s - %s is missing in metadata.log\n" % (dirname, e), file=sys.stdout)
        return {}

def mk_user_specified_metadata(options):
    # parse the JSON string into a dict and return it
    try:
        return json.loads(options.metadata_string)
    except Exception:
        print("Cannot parse JSON string: {}\n".format(options.metadata_string))
        return {}

def mk_metadata(fname, tb, mdconf, md5sum):
    """Return a dict with metadata about a tarball"""
    mtime = datetime.utcfromtimestamp(os.stat(fname)[stat.ST_MTIME])

    mddict = {'generated-by': _NAME_,
              'generated-by-version': _VERSION_,
              'pbench-agent-version': mdconf.get("pbench", "rpm-version"),
              'file-date': mtime.strftime("%Y-%m-%d"),
              'file-name': fname,
              'md5': md5sum
    }
    return mddict

def mk_run_action(ptb, options, INDEX_PREFIX):
    #mk_run_action(hostname, tbname, dirname, dirprefix, tb, mdconf, idxname, md5sum, ts, options):
    """Extract metadata from the named tarball and create an indexing
       action out of them.

       There are two kinds of metadata: what goes into _source[@metadata]
       is metadata about the tarball itself - not things that are *part* of
       the tarball: its name, size, md5, mtime.
       Metadata about the run are *data* to be indexed.
    """
    source = { '@timestamp': ptb.start_run.replace('_', 'T') }

    # debug: -T options will cause each call below to be timed
    # and the elapsed interval printed.
    debug_time_operations = options.debug_time_operations
    if debug_time_operations: ts("mk_metadata")
    source['@metadata'] = mk_metadata(ptb.tbname, ptb.tb, ptb.mdconf, ptb.md5sum)
    if options.metadata_string:
        if debug_time_operations: ts("mk_user_specified_metadata")
        source['user_specified_metadata'] = mk_user_specified_metadata(options)
    #if debug_time_operations: ts("mk_pbench_metadata")
    #source['pbench'] = mk_pbench_metadata(mdconf)
    if debug_time_operations: ts("mk_run_metadata")
    source['run'] = mk_run_metadata(ptb.mdconf, ptb.md5sum, ptb.dirname, ptb.dirprefix)
    if debug_time_operations: ts("mk_sosreports")
    source['sosreports'] = sos_d = mk_sosreports(ptb.tb)
    if debug_time_operations: ts("mk_tool_info")
    source['host_tools_info'] = mk_tool_info(sos_d, ptb.mdconf)

    # make a simple action for indexing
    index_name_template = "%s.run.%%d-%%02d" % INDEX_PREFIX
    action = {
        "_op_type": _op_type,
        "_index":   index_name_template % (int(ptb.date[0]), int(ptb.date[1])),
        "_type":    "pbench-run",
        "_id":      ptb.md5sum,
        "_source":  source,
    }
    if debug_time_operations: ts("Done!", newline=True)
    return action

def make_all_actions(ptb, options, INDEX_PREFIX, INDEX_VERSION):
    """Driver for generating all actions on source documents for indexing
    into Elasticsearch. This generator drives the generation of the run
    source document, the table-of-contents tar ball documents, and finally
    all the tool data.
    """
    debug_time_operations = options.debug_time_operations
    if debug_time_operations: ts("mk_run_action")
    yield mk_run_action(ptb, options, INDEX_PREFIX)
    if debug_time_operations: ts("mk_toc_actions")
    for action in mk_toc_actions(ptb, options, INDEX_PREFIX):
        yield action
    if debug_time_operations: ts("mk_result_data_actions")
    for action in mk_result_data_actions(ptb, options, INDEX_PREFIX):
        yield action
    if debug_time_operations: ts("mk_tool_data_actions")
    for action in mk_tool_data_actions(ptb, options, INDEX_PREFIX):
        yield action
    if debug_time_operations: ts("", newline=True)
    return


class PbenchTarBall(object):
    def __init__(self, tbarg):
        global _tmpdir

        self.tbname = tbarg
        self.hostname = os.path.basename(os.path.dirname(self.tbname))
        self.tb = tarfile.open(self.tbname)

        # This is the top-level name of the run - it should be the common
        # first component of every member of the tarball.
        dirname = os.path.basename(self.tbname)
        # FIXME: should we handle result tarballs that are uncompressed, or
        # compressed with a different compression mechanism from xz?
        self.dirname = dirname[:dirname.rfind('.tar.xz')]
        # ... but let's make sure.
        # FIXME: how are we making sure it is the first component of every
        # member of the tar ball?  Here we seem to be only looking at the
        # first, and not comparing it to self.dirname.
        self.dirprefix = self.tb.getmembers()[0].name.split(os.path.sep)[0]

        # Verify we have a metadata.log file in the tar ball before we
        # start extracting.
        member_name = "%s/metadata.log" % (self.dirname)
        try:
            self.mdlog = self.tb.getmember(member_name)
        except KeyError:
            raise UnsupportedTarballFormat(self.tbname)

        # FIXME: Should we have the option of using an already extracted
        # version of the tar ball?
        # FIXME: Is this a temporary extraction or permanent? If temporary,
        # where does it get cleaned up?
        self.tb.extractall(path=_tmpdir)
        try:
            self.mdconf = PbenchTarBall.get_mdconfig(os.path.join(_tmpdir, member_name))
        except Exception:
            raise BadMDLogFormat(self.tbname)
        # We get the start date out of the metadata log.
        try:
            # N.B. the start_run timestamp is in UTC.
            self.start_run = fix_date_format(self.mdconf.get('run', 'start_run'))
            split_date_str = self.start_run.split('T')
            # List of constituent date components, year [0], month [1], day [2]
            self.date = [ int(x) for x in split_date_str[0].split("-") ]
        except Exception:
            raise BadDate(self.tbname)
        # Open the MD5 file of the tarball and read the MD5 sum from it.
        self.md5sum = open("%s.md5" % (self.tbname)).read().split()[0]
        # FIXME: at this point, do we assume the MD5 sum of the tar ball is good?

    @staticmethod
    def get_mdconfig(mdf):
        """Get metadata from a metadata.log file.

        metadata.log is in the format expected by configparser,
        so we just get and return a configparser object."""
        mdfconfig = SafeConfigParser()
        mdfconfig.read(mdf)

        # FIXME: verify start_run and end_run dictionary items are present
        d = {}
        d.update(mdfconfig.items('run'))
        if "start_run" not in d or "end_run" not in d:
            raise Exception('Missing start_run or end_run elements')
        return mdfconfig


def get_hosts(config):
    """
    Return list of dicts (a single dict for now) -
    that's what ES is expecting.
    """
    try:
        URL = config.get('Server', 'server')
    except NoSectionError:
        print("Need a [Server] section with host and port defined in %s"
              " configuration file" % (" ".join(config.__files__)),
                file=sys.stderr)
        return None
    except NoOptionError:
        host = config.get('Server', 'host')
        port = config.get('Server', 'port')
    else:
        host, port = URL.rsplit(':', 1)
    timeoutobj = Timeout(total=1200, connect=10, read=_read_timeout)
    return [dict(host=host, port=port, timeout=timeoutobj),]

###########################################################################
# main: create index from template if necessary, prepare the action and ship
# it to ES for indexing.

def main(options, args):
    global _tmpdir

    # ^$!@!#%# compatibility
    # FileNotFoundError is python 3.3 and the travis-ci hosts still (2015-10-01) run
    # python 3.2
    filenotfounderror = getattr(__builtins__, 'FileNotFoundError', IOError)

    try:
        config=SafeConfigParser()
        try:
            cfg_name = options.cfg_name
        except Exception:
            cfg_name = os.environ.get("ES_CONFIG_PATH")

        if not cfg_name:
            raise ConfigFileNotSpecified("No config file specified: set ES_CONFIG_PATH env variable or use --config <file> on the command line")

        try:
            config.read(cfg_name)
            INDEX_PREFIX = config.get('Settings', 'index_prefix')
            INDEX_VERSION = config.get('Settings', 'index_version')
        except Exception as e:
            raise ConfigFileError(e)

        _tmpdir = os.environ['TMPDIR']

        if options.debug_unittest:
            es = None
            import collections
            global _dict_const
            _dict_const = collections.OrderedDict
        else:
            hosts = get_hosts(config)
            es = Elasticsearch(hosts, max_retries=0)

        # prepare the actions - the tarball name is the only argument.
        ptb = PbenchTarBall(args[0])
        actions = make_all_actions(ptb, options, INDEX_PREFIX, INDEX_VERSION)

        # Create the various index templates
        if options.debug_time_operations: ts("es_template")
        es_template(es, options, INDEX_PREFIX, INDEX_VERSION, config)

        # returns 0 or 1
        if options.debug_time_operations: ts("es_index")
        res = es_index(es, actions)

    except ConfigFileNotSpecified as e:
        print(e, file=sys.stderr)
        res = 2

    except ConfigFileError as e:
        print(e, file=sys.stderr)
        res = 3

    except UnsupportedTarballFormat as e:
        print("Unsupported Tarball Format - no metadata.log: ", e, file=sys.stderr)
        res = 4

    except BadDate as e:
        print("Bad Date: ", e, file=sys.stderr)
        res = 5

    except filenotfounderror as e:
        print("No such file: ", e, file=sys.stderr)
        res = 6

    except BadMDLogFormat as e:
        print("The metadata.log file is curdled in tarball: ", e, file=sys.stderr)
        res = 7

    except MappingFileError as e:
        print(e, file=sys.stderr)
        res = 8

    except TemplateError as e:
        print(repr(e), file=sys.stderr)
        res = 9

    except SosreportHostname as e:
        print("Bad hostname in sosreport: ", e, file=sys.stderr)
        res = 10

    except tarfile.TarError as e:
        print("Can't unpack tarball into {}: {}".format(_tmpdir, e), file=sys.stderr)
        res = 11

    # this is Spinal Tap
    except Exception as e:
        print("Other error", e, file=sys.stderr)
        import traceback
        print(traceback.format_exc())
        res = 12

    finally:
        if options.debug_time_operations: ts("Done", newline=True)

    # status codes: these are used by pbench-index to segregate tarballs into
    #               classes: should we retry (perhaps after fixing bugs in the
    #               indexing) or should we just forget about them?
    #       0 - normal, successful exit, no errors
    #       1 - Operational error while indexing
    #       2 - Configuration file not specified
    #       3 - Bad configuration file
    #       4 - Tar ball does not contain a metadata.log file
    #       5 - Bad start run date value encountered
    #       6 - File Not Found error
    #       7 - Bad metadata.log file encountered
    #       8 - Error reading a mapping file for Elasticsearch templates
    #       9 - Error creating one of the Elasticsearch templates
    #      10 - Bad hostname in a sosreport
    #      11 - Failure unpacking the tar ball
    #      12 - generic error, needs to be investigated and can be retried after
    #           any indexing bugs are fixed.
    return res

###########################################################################
# Options handling

prog_options = [
    make_option("-C", "--config", dest="cfg_name", help="Specify config file"),
    make_option("-M", "--metadata", dest="metadata_string", help="Specify additional metadata (e.g. for browbeat) as a JSON document"),
    # options for debuggging and unit testing
    make_option("-U", "--unittest", action="store_true", dest="debug_unittest", help="Run in unittest mode"),
    make_option("-T", "--time-ops", action="store_true", dest="debug_time_operations", help="Time action making routines"),
]

if __name__ == '__main__':
    parser = OptionParser("Usage: index-pbench [--config <path-to-config-file>] <path-to-tarball>")
    for o in prog_options:
        parser.add_option(o)

    (options, args) = parser.parse_args()

    status = main(options, args)
    sys.exit(status)
