#!/bin/bash

# Main unit/functional test driver for server legacy tests.  All the actual
# test running code is located in `utils/run-unittest`.

# We force the umask to a common value so that unit test output matches on all
# platforms (well, mostly matches on all platforms).
umask 0002

# Don't allow an external _PBENCH_SERVER_CONFIG environment variable to affect the operation
# of the unit tests since the unit tests themselves use pbench-config which looks
# for that envinronment variable as input.  Also, remove the _PBENCH_AGENT_CONFIG environment
# variable as well, because the pthread-config command will use that if it is defined.
unset _PBENCH_SERVER_CONFIG
unset _PBENCH_AGENT_CONFIG

# Ensure we always use the same locale for the unit tests.
export LANG=C.UTF-8
export LC_ALL=C.UTF-8

export _tdir=$(dirname $(readlink -f ${0}))
export _testbase=/var/tmp/pbench-test-server
rm -rf ${_testbase}
mkdir ${_testbase}
if [[ ! -d ${_testbase} ]]; then
    echo "ERROR: failed to create test base directory, \"${_testbase}\"" >&2
    exit 1
fi

function remove_path {
    # REMOVE (${1}) => /opt/a dir/bin
    REMOVE=${1}
    # PATH (${2}) => /bin:/opt/a dir/bin:/sbin
    WORK=:${2}:
    # WORK => :/bin:/opt/a dir/bin:/sbin:
    WORK=${WORK/:${REMOVE}:/:}
    # WORK => :/bin:/sbin:
    WORK=${WORK%:}
    WORK=${WORK#:}
    # PATH => /bin:/sbin
    echo ${WORK}
}
# Just in case somebody had installed the pbench-server or pbench-agent code
# (either via RPM or otherwise), such that the installed server and/or agent
# code has been added to `PATH`, we remove those path elements to avoid
# disturbing the tests.
PATH=$(remove_path /opt/pbench-server/bin ${PATH})
PATH=$(remove_path /opt/pbench-agent/bench-scripts ${PATH})
export PATH=$(remove_path /opt/pbench-agent/util-scripts ${PATH})

declare -A cmds=(
    # check for no TOP directory
    [test-0]="_run_allscripts"
    # check for no LOGSDIR directory
    [test-0.1]="_run_allscripts"
    # check for no TMP directory
    [test-0.2]="_run_allscripts"
    # check for no ARCHIVE directory
    [test-1]="_run_allscripts"
    # check for no INCOMING directory
    [test-2]="_run_allscripts"
    # check for no RESULTS directory
    [test-3]="_run_allscripts"
    # check for no USERS directory
    [test-4]="_run_allscripts"
    # check that all scripts run normally if everything is there
    [test-5]="_run_allscripts"
    # check that all scripts run normally if everything is there, using a
    # separate unpack directory from the incoming directory
    [test-5.1]="_run_allscripts"

    # backup tests
    # check for normal operation
    [test-6]="_run pbench-backup-tarballs"
    # check for no ARCHIVE directory
    [test-6.1]="_run pbench-backup-tarballs"
    # check for no ARCHIVE link resolution
    [test-6.2]="_run pbench-backup-tarballs"
    # checks that the dest directory is not bogus
    [test-6.3]="_run pbench-backup-tarballs"
    # real dest directory, no files needing backup - should succeed
    [test-6.4]="_run pbench-backup-tarballs"
    # real dest directory, all files needing backup - should succeed
    [test-6.5]="_run pbench-backup-tarballs"
    # real dest directory, no files needing backup - corrupted archive should not be copied
    [test-6.6]="_run pbench-backup-tarballs"
    # all files needing backup - md5 file missing should fail
    [test-6.7]="_run pbench-backup-tarballs"
    # check for no bucket
    [test-6.8]="_run pbench-backup-tarballs"
    # Upload fail since s3 already contains the result with different md5sum
    [test-6.9]="_run pbench-backup-tarballs"
    # Upload successful since s3 already contains the result with same md5sum
    [test-6.10]="_run pbench-backup-tarballs"
    # Upload successful
    [test-6.11]="_run pbench-backup-tarballs"
    # Upload successful of large tar ball, using multi-upload API
    [test-6.12]="_run pbench-backup-tarballs"

    # Upload fail since md5 corrupt
    # TBD: right now, it is quite complicated to test this from the
    # unit test, because we check whether the md5sum of a tarball is
    # matching with its md5 file or not. To test this case, we need to
    # inject the change in the md5 sum of the tarball after it passes
    # the initial check: only then can we check for a failure because
    # of a mismatched md5 sum when we try to upload to the (mock) s3
    # service.

    # check for successful local backup if S3 is disabled
    [test-6.13]="_run pbench-backup-tarballs"
    [test-6.14]="_run pbench-backup-tarballs"

    # Upload successful of already backed-up large tar ball, using
    # multi-upload API. Variation of 6.12.
    [test-6.15]="_run pbench-backup-tarballs"

    # Check for unexpected errors from get_object().
    # We just enhance 6.12 by adding another tar ball
    # in each case that is named in such a way as to
    # trigger the error.

    # ClientError Other - tar ball added: client_error_other.tar.xz
    [test-6.16]="_run pbench-backup-tarballs"
    # Generic exception - tar ball added: get_object_exception.tar.xz
    [test-6.17]="_run pbench-backup-tarballs"

    # indexing tests are all test-7.*

    # Special case of running pbench-index with -I to get a dump
    # of the index patterns in use.
    [test-7.0.0]="_run pbench-index --dump-index-pattern"
    [test-7.0.1]="_run pbench-index --dump-templates"

    [test-7.1]="_run_indexing"

    # 7.2.X: UnsupportedTarballFormat exceptions:
    # missing metadata.log
    [test-7.2.0]="_run_indexing"
    # badly formed tarball: ./<resultdir> prefix instead of <resultdir>
    [test-7.2.1]="_run_indexing"

    [test-7.3]="_run_indexing"
    [test-7.4]="_run_indexing"
    [test-7.5]="_run_indexing"
    [test-7.6]="_run_indexing"
    [test-7.7]="_run_indexing"
    # uperf tarball
    [test-7.8]="_run_indexing"
    # pbench-user-benchmark tarball
    [test-7.9]="_run_indexing"
    # uperf, fio results data and prometheus data
    # with float conversion
    [test-7.10]="_run_indexing"
    [test-7.11]="_run_indexing"
    [test-7.12]="_run_indexing"
    # The proc-vmstat results data is 7.13.
    [test-7.13]="_run_indexing"
    # The mpstat results data is in 7.14.
    [test-7.14]="_run_indexing"

    # truncated uperf results data so we can do
    # a complete comparison of input to output.
    # uperf throughput (Gb_sec) results
    [test-7.15]="_run_indexing"

    # truncated uperf results data so we can do
    # a complete comparison of input to output.
    # uperf latency and throughput (usec and trans_sec) results
    [test-7.16]="_run_indexing"

    # Test vmstat tool data indexing.
    [test-7.17]="_run_indexing"

    # Test bad controller.
    [test-7.18]="_run_indexing"

    # Test trafficgen data.
    [test-7.19]="_run_indexing"

    # Test run-benchmark fio data.
    [test-7.20]="_run_indexing"
    # Test run-benchmark trafficgen data.
    [test-7.21]="_run_indexing"
    # Test run-benchmark linpack data.
    [test-7.22]="_run_indexing"

    # Test handling of non-uniform timestamps (strings and ints)
    [test-7.23]="_run_indexing"

    # Test user-benchmark indexing
    [test-7.24]="_run_indexing"

    # Debug uperf indexing
    [test-7.25]="_run_indexing"

    # Verify re-indexing
    [test-7.26]="_run_re_indexing"

    # Verify 0.69 agent metadata.log file handling
    [test-7.27]="_run_indexing"

    # activation test
    [test-8]="_run_activate"

    # pbench-verify-backup-tarballs
    # normal case
    [test-9.1]="_run pbench-verify-backup-tarballs"
    # more tarballs in archive than in backup and S3
    [test-9.2]="_run pbench-verify-backup-tarballs"
    # more tarballs in local backup
    [test-9.3]="_run pbench-verify-backup-tarballs"
    # bad md5 in archive - nothing in S3
    [test-9.4]="_run pbench-verify-backup-tarballs"
    # bad md5 in backup - nothing in S3
    [test-9.5]="_run pbench-verify-backup-tarballs"
    # more tarballs in s3
    [test-9.6]="_run pbench-verify-backup-tarballs"
    # check for Continuation Token
    [test-9.7]="_run pbench-verify-backup-tarballs"
    # large tar ball - reuses the 6.15 tar ball.
    [test-9.8]="_run pbench-verify-backup-tarballs"

    # trivial results: no mail
    [test-10]="_run pbench-sync-satellite satellite-one"
    # non-trivial results: mail
    [test-11]="_run pbench-sync-satellite satellite-one"

    # pbench-unpack-tarballs unpacking to the incoming directory
    [test-16]="_run pbench-unpack-tarballs small"
    [test-17]="_run pbench-unpack-tarballs small"

    [test-20]="_run echo audit archive hierarchy"

    [test-21]="_run pbench-dispatch"

    # Actually exercises satellite code
    [test-22]="_run pbench-satellite-cleanup"

    # Uses pbench-dispatch to test report status via syslog
    [test-23]="_run pbench-dispatch"

    # Tests config file that specifies two shims versions
    [test-24]="_run_activate"

    # Tests that the behavior of the find command works as we expect to
    # ensure that buckets for pbench-unpack-tarballs work properly.
    [test-25]="_run test-find-behavior"

    # Test culling unpacked tar balls
    [test-27]="_run pbench-cull-unpacked-tarballs"

    # Test ssh-error
    [test-28]="_run pbench-sync-satellite satellite-one"

    # Test pbench-reindex
    [test-29.0]="_run pbench-reindex 1970-01-01"
    [test-29.1]="_run pbench-reindex 1970-02-01 1970-02-XX"
    [test-29.2]="_run pbench-reindex 1970-02-01 1970-02-28"
    [test-29.3]="_run pbench-reindex --dry-run 1970-02-01 1970-02-28"
)
all_tests_sorted=$(for x in ${!cmds[@]}; do echo ${x}; done | sed 's/\./-/' | sort -n -t '-' -k 3 | sort -n -t '-' -k 2 --stable | sed 's/\(.*-[0-9]\+\)-\([0-9]\+\)/\1.\2/')

mode="${PBENCH_UNITTEST_PARALLEL:-auto}"
case ${1} in
    --serial)
        shift
        mode="serial"
        ;;
    --parallel)
        shift
        mode="auto"
        ;;
    --*)
        printf -- "Bad argument %s\n" "${1}" >&2
        exit 1
        ;;
esac
if [[ "${mode}" != "serial" && "${mode}" != "auto" ]]; then
    printf "Bad server unit test mode \"%s\", choose either 'serial' or 'auto'\n" "${mode}" >&2
    exit 1
fi
jobs_arg=""
if [[ "${mode}" == "serial" ]]; then
    jobs_arg="-j 1"
fi

test_args=${*}
if [[ -z "${test_args}" ]]; then
    # No tests given, run them all in sorted order
    tests=${all_tests_sorted}
else
    tests=""
    for test_arg in ${test_args}; do
        let found=0
        for test_name in ${all_tests_sorted}; do
            case ${test_arg} in
            test-*)
                ;;
            *)
                test_arg="test-${test_arg}"
                ;;
            esac
            case ${test_arg} in
            *\.)
                # if ${test_arg} ends in "." use as a prefix
                if [[ "${test_name}" =~ "${test_arg}" ]]; then
                    tests="${tests} ${test_name}"
                    let found=found+1
                fi
                ;;
            *)
                # if ${test_arg} does not end in "." use as a full test name
                if [[ "${test_name}" = "${test_arg}" ]]; then
                    tests="${tests} ${test_name}"
                    let found=found+1
                    break
                fi
                ;;
            esac
        done
        if [[ ${found} -eq 0 ]]; then
            printf -- "Unknown test %s, skipping ...\n" "${test_arg}" >&2
        fi
    done
fi

# The parallel program is really cool.  The usage of `parallel` is internal and
# automated; only test code depends on this tool, and we, as developers, have
# viewed the citation and are justified in suppressing future displays of it in
# our development processes (use of --will-cite below).

let count=0
let failures=0
for testname in ${tests}; do
    cmd=${cmds[${testname}]}
    if [[ -z "${cmd}" ]]; then
        printf -- "Unknown test - Logic bomb!: \"%s\"\n" "${testname}" >&2
        continue
    fi
    let count=count+1
done
if [[ ${count} -eq 0 ]]; then
    printf "No tests run!\n" >&2
    failures=1
else
    for testname in ${tests}; do
        cmd=${cmds[${testname}]}
        if [[ -n "${cmd}" ]]; then
            echo "${testname} ${cmd}"
        fi
    done | parallel --will-cite -k --lb ${jobs_arg} ${_tdir}/utils/run-unittest
    if [[ ${?} -ne 0 ]]; then
        failures=1
    fi
fi

# Attempt to remove test directory entirely; if we fail, ignore it as it
# just means there are output files present for failed tests.
rmdir ${_testbase} > /dev/null 2>&1

exit ${failures}
