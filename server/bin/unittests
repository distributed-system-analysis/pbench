#!/bin/bash

# We force the umask to a common value so that unit test output matches on all
# platforms (well, mostly matches on all platforms).
umask 0002

# Don't allow an external _PBENCH_SERVER_CONFIG environment variable to affect the operation
# of the unit tests since the unit tests themselves use getconf.py which looks
# for that envinronment variable as input.
unset _PBENCH_SERVER_CONFIG

# Ensure we always use the same locale for the unit tests.
export LANG=C
export LC_ALL=C

export _tdir=$(dirname $(readlink -f $0))
export _gitdir=$(dirname $(dirname $_tdir))
export _tlib=${_tdir}/../lib
export _testbase=/var/tmp/pbench-test-server
rm -rf ${_testbase}
mkdir ${_testbase}
if [[ ! -d $_testbase ]]; then
    echo "ERROR: failed to create test base directory, \"$_testbase\"" >&2
    exit 1
fi

function remove_path {
    # PATH ($2) => /bin:/opt/a dir/bin:/sbin
    WORK=:$2:
    # WORK => :/bin:/opt/a dir/bin:/sbin:
    REMOVE=$1
    WORK=${WORK/:$REMOVE:/:}
    # WORK => :/bin:/sbin:
    WORK=${WORK%:}
    WORK=${WORK#:}
    #PATH=$WORK
    # PATH => /bin:/sbin
    echo $WORK
}
PATH=$(remove_path /opt/pbench-server/bin $PATH)
PATH=$(remove_path /opt/pbench-agent/bench-scripts $PATH)
export PATH=$(remove_path /opt/pbench-agent/util-scripts $PATH)

function _run {
    tname=${1}
    shift
    if [[ -z "${@}" ]]; then
        echo "+++ Running ${tname}" >> ${_testout}
    else
        echo "+++ Running ${tname} ${@}" >> ${_testout}
    fi
    ${tname} ${@} >> ${_testout} 2>&1
    echo "--- Finished ${tname} (status=${?})" >> ${_testout}
}

function _run_indexing {
    _run pbench-unpack-tarballs
    _run pbench-index
    _run pbench-index --tool-data
}

function _run_activate {
    # Testing the server activate script, which is used for every other
    # unit test as well is just a matter of emitting the resulting
    # crontab along side the rest of the environment state provided by
    # the unit test framework.
    echo "+++ Verifying server activation" >> $_testout
    sed -i 's;'${_gitdir}'/server/;/home/user/repo/pbench/server/;' $_testactout
    cat $_testactout >> $_testout
    echo "++++ ${_testopt}/lib/crontab/crontab" >> $_testout
    cat ${_testopt}/lib/crontab/crontab >> $_testout
    rc=$?
    echo "---- ${_testopt}/lib/crontab/crontab" >> $_testout
    echo "++++ ${_testopt_sat}/lib/crontab/crontab" >> $_testout
    cat ${_testopt_sat}/lib/crontab/crontab >> $_testout
    rc=$?
    echo "---- ${_testopt_sat}/lib/crontab/crontab" >> $_testout
    if [ -s $_testactlog ]; then
        echo "++++ $(basename $_testactlog) file contents" >> $_testout
        cat $_testactlog >> $_testout 2>&1
        echo "---- $(basename $_testactlog) file contents" >> $_testout
    fi
    echo "--- Finished verifying server activation (status=$rc)" >> $_testout
}

function _run_allscripts {
    # These three pull in new tar balls from move-results and remote
    # satellite pbench servers feeding them into the dispatch loop.
    _run pbench-server-prep-shim-002
    # NOTE WELL: the "satellite-one" argument refers a configuration
    # section in the unit test's pbench-server.cfg file - keep them
    # synchronized.
    _run pbench-sync-satellite satellite-one
    # These next five are related and would flow in this order
    _run pbench-dispatch
    _run pbench-unpack-tarballs small
    _run pbench-copy-sosreports
    _run pbench-index
    _run pbench-index --tool-data
    # These four are independent, running periodically to accomplish
    # their specific tasks.
    _run pbench-clean-up-dangling-results-links
    _run pbench-cull-unpacked-tarballs
    _run pbench-backup-tarballs
    _run pbench-verify-backup-tarballs
    # Invoke pbench-satellite-cleanup in the context of the satellite
    PATH=${_testopt_sat}/unittest-scripts:${_testopt_sat}/bin:${_orig_PATH} _PBENCH_SERVER_CONFIG=${SATCONFIG} _run pbench-satellite-cleanup
}

function _local_find {
    # We create our own local find command so that we don't emit the size
    # information for directories.  This is due to the fact that on different
    # file systems empty directories, or directories with small numbers of
    # files, can be handled differently.  E.g. on Ext4 directories have a
    # minimum size of 4096, while on XFS only after a certain size do they
    # grow to multiples of 4096 [1].  We only care about the sizes of files and
    # links in our tests.
    #
    # [1] https://superuser.com/questions/585844/why-directories-size-are-different-in-ls-l-output-on-xfs-file-system
    find ${1} ! -name $(basename ${1}) -type d -printf '%M          - %P\n' , \( ! -type d ! -type l -printf '%M %10s %P\n' \) , -type l -printf '%M %10s %P -> %l\n' | sort -k 3
}

function _normalize_output {
    # Fix up tmp directory references
    sed -E -e 's;tmp/pbench-([-a-zA-Z0-9]+)\.[0-9][0-9]*/;tmp/pbench-\1.NNNN/;' $*
}

function _save_tree {
    # Save state of the tree
    if [ -d ${_testhtml} ] ;then
        echo "+++ var/www/html tree state (${_testhtml})" >> $_testout
        _local_find ${_testhtml} >> $_testout
        echo "--- var/www/html tree state" >> $_testout
        echo "+++ results host info (${_testhtml}/pbench-results-host-info.versioned)" >> $_testout
        grep -HvF "\-\-should-n0t-ex1st--" ${_testhtml}/pbench-results-host-info.versioned/pbench-results-host-info.URL00?.* >> $_testout 2>&1
        echo "--- results host info" >> $_testout
    fi
    if [ -d ${_testhtml_sat} ] ;then
        echo "+++ var/www/html-satellite tree state (${_testhtml_sat})" >> $_testout
        _local_find ${_testhtml_sat} >> $_testout
        echo "--- var/www/html-satellite tree state" >> $_testout
        echo "+++ results host info (${_testhtml_sat}/pbench-results-host-info.versioned)" >> $_testout
        grep -HvF "\-\-should-n0t-ex1st--" ${_testhtml_sat}/pbench-results-host-info.versioned/pbench-results-host-info.URL00?.* >> $_testout 2>&1
        echo "--- results host info" >> $_testout
    fi
    echo "+++ pbench tree state (${_testdir})" >> $_testout
    if [ -d ${_testdir} ] ;then
        _local_find ${_testdir} | _normalize_output >> $_testout
    fi
    echo "--- pbench tree state" >> $_testout
    if [ -d ${_testdir_local} ] ;then
        echo "+++ pbench-local tree state (${_testdir_local})" >> $_testout
        _local_find ${_testdir_local} | _normalize_output >> $_testout
        echo "--- pbench-local tree state" >> $_testout
    fi
    if [ -d ${_testdir_sat} ] ;then
        echo "+++ pbench-satellite tree state (${_testdir_sat})" >> $_testout
        _local_find ${_testdir_sat} | _normalize_output >> $_testout
        echo "--- pbench-satellite tree state" >> $_testout
    fi
    if [ -d ${_testdir_sat_local} ] ;then
        echo "+++ pbench-satellite-local tree state (${_testdir_sat_local})" >> $_testout
        _local_find ${_testdir_sat_local} | _normalize_output >> $_testout
        echo "--- pbench-satellite-local tree state" >> $_testout
    fi
    if [ -d ${_testtmp} ]; then
        echo "+++ ${_testtmp}" >> $_testout
        find ${_testtmp} -ls >> $_testout 2>&1
        echo "--- ${_testtmp}" >> $_testout
    fi
}

function _audit_server {
    echo "+++ Running unit test audit" >> $_testout
    pbench-audit-server >> $_testout 2>&1
    echo "--- Finished unit test audit (status=$?)" >> $_testout
}

function _dump_logs {
    # Dump the state of any generated script logs
    echo "+++ pbench log file contents" >> $_testout
    for dir in ${_testdir} ${_testdir_local} ${_testdir_sat} ${_testdir_sat_local} ;do
        if [ -d ${dir}/logs ] ;then
            echo "++++ $(basename ${dir})/logs" >> $_testout
            find ${dir}/logs -type f -printf "%P\n" | sort | \
                while read fname; do
                    echo "+++++ ${fname}" >> $_testout
                    _normalize_output ${dir}/logs/${fname} >> $_testout 2>&1
                    echo "----- ${fname}" >> $_testout
                done
            echo "---- $(basename ${dir})/logs" >> $_testout
        fi
    done
    echo "--- pbench log file contents" >> $_testout

    if [ -s $_testlog ]; then
        echo "+++ $(basename $_testlog) file contents" >> $_testout
        _normalize_output $_testlog >> $_testout 2>&1
        echo "--- $(basename $_testlog) file contents" >> $_testout
    fi

    if [ -s $_testcurlpayload ]; then
        echo "+++ $(basename $_testcurlpayload) file contents" >> $_testout
        _normalize_output $_testcurlpayload >> $_testout 2>&1
        echo "--- $(basename $_testcurlpayload) file contents" >> $_testout
    fi

    if [ -s $_testloggerpayload ]; then
        echo "+++ $(basename $_testloggerpayload) file contents" >> $_testout
        _normalize_output $_testloggerpayload >> $_testout 2>&1
        echo "--- $(basename $_testloggerpayload) file contents" >> $_testout
    fi
}

function _verify_output {
    tname=${1}
    # Fix up "_id", "_parent", and "@generated-by" IDs using:
    #   * 5ca1ab1e70015f100dedfab1ed0ff1ce
    #    "scalable tools flooded fabled office"
    #   * babb1e70015c01055a15ca1ab1eb0a75
    #    "babble tools colossal scalable boats"
    #   * 70015f01dedbadbeefc105edcafedead
    #    "tools folded bad beef closed cafe dead"
    sed -i -E -e 's/"_id": "[0-9a-f]+",/"_id": "5ca1ab1e70015f100dedfab1ed0ff1ce",/' \
              -e 's/"_parent": "[0-9a-f]+",/"_parent": "babb1e70015c01055a15ca1ab1eb0a75",/' \
              -e 's/"@generated-by": "[0-9a-f]+",/"@generated-by": "70015f01dedbadbeefc105edcafedead",/' ${_testout}
    diff -c ${_tdir}/gold/${tname}.txt ${_testout} > ${_testdiff} 2>&1
    if [[ ${?} -gt 0 ]]; then
        let res=1
        echo "FAIL" > ${_testres}
    else
        let res=0
        echo "PASS" > ${_testres}
        rm ${_testout} ${_testdiff}
    fi
    return ${res}
}

function _setup_state {
    res=0
    mkdir -p $_testopt/unittest-scripts/
    let res=res+$?
    mkdir -p $_testopt_sat/unittest-scripts/
    let res=res+$?
    cp -a $_tdir/test-bin/* $_testopt/unittest-scripts/
    let res=res+$?
    cp $_tdir/test-bin/* $_testopt_sat/unittest-scripts/
    let res=res+$?
    mkdir -p $_testopt/bin
    let res=res+$?
    mkdir -p $_testopt_sat/bin
    let res=res+$?
    mkdir -p $_testopt/common/lib
    let res=res+$?
    cp -a $_tdir/{unittests,getconf.py,pbench*} $_testopt/bin
    let res=res+$?
    mkdir -p $_testopt/html/static/{js,css}/v0.{2,3}
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.2/js/ $_testopt/html/static/js/v0.2
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.3/js/ $_testopt/html/static/js/v0.3
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.2/css/ $_testopt/html/static/css/v0.2
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.3/css/ $_testopt/html/static/css/v0.3
    let res=res+$?
    cp -a $_tdir/{unittests,getconf.py,pbench*} $_testopt_sat/bin
    let res=res+$?
    cp -a $_tdir/../lib $_testopt
    let res=res+$?
    cp -a $_tdir/../lib $_testopt_sat
    let res=res+$?
    mkdir -p $_testopt_sat/html/static/{js,css}/v0.{2,3}
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.2/js/ $_testopt_sat/html/static/js/v0.2
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.3/js/ $_testopt_sat/html/static/js/v0.3
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.2/css/ $_testopt_sat/html/static/css/v0.2
    let res=res+$?
    cp -a $_tdir/../../web-server/v0.3/css/ $_testopt_sat/html/static/css/v0.3
    let res=res+$?
    mkdir -p $_testhtml
    let res=res+$?
    mkdir -p $_testhtml_sat
    let res=res+$?
    if [ $res -ne 0 ]; then
        echo "ERROR: failed to properly setup the test environment root, \"$_testroot\"" >&2
        exit $res
    fi

    mkdir $_testdir $_testdir_sat $_testtmp
    if [[ $? -gt 0 ]]; then
        echo "ERROR: failed to create test pbench, pbench-satellite, and tmp directories, \"$_testdir\", \"$_testdir_sat\", and/or \"$_testtmp\"" >&2
        exit 1
    fi
    if [[ ! -d $_testdir ]]; then
        echo "ERROR: test pbench directory does not exist, \"$_testdir\"" >&2
        exit 1
    fi
    if [[ ! -d $_testdir_sat ]]; then
        echo "ERROR: test pbench-satellite directory does not exist, \"$_testdir_sat\"" >&2
        exit 1
    fi
    if [[ ! -d $_testtmp ]]; then
        echo "ERROR: test tmp directory does not exist, \"$_testtmp\"" >&2
        exit 1
    fi

    # All the "real" scripts are found at $_testopt/bin, the mock scripts
    # are found in $_testopt/unittest-scripts.
    _orig_PATH=$PATH
    export PATH=$_testopt/unittest-scripts:$_testopt/bin:$PATH
    export PYTHONPATH="${_testopt}/lib:${_testopt}/common/lib:${PYTHONPATH}"

    # Expected location of the final configuration files
    export _PBENCH_SERVER_CONFIG=$_testopt/lib/config/pbench-server.cfg
    export LOGSDIR=$_testdir_local/logs
    export SATCONFIG=$_testopt_sat/lib/config/pbench-server.cfg

    # The activate invocations are supposed to work without _PBENCH_SERVER_CONFIG being set,
    # so they do *not* use the global _PBENCH_SERVER_CONFIG file that the rest of the tests
    # use.  We copy the server and index configuration files to a special
    # directory outside of the source tree to isolate any possible changes to
    # the original source.  The activate script copies it to its "final"
    # resting place.

    _state_config="${_tdir}/state/${1}.config/pbench-server.cfg"
    if [ ! -e ${_state_config} ]; then
        _state_config="${_tdir}/state/config/pbench-server.cfg"
    else
        # the testcase-specific config file includes the global state config file
        # but under the name "state-pbench-server.cfg" to prevent conflicts. It also
        # gets the default section below, so we have to include the install-dir line in it
        # and delete it from the global state config file.
        sed 1d "${_tdir}/state/config/pbench-server.cfg" > ${_testopt}/lib/config/state-pbench-server.cfg
    fi
    > ${_testtmp}/pbench-server.cfg
    printf "[DEFAULT]\n"                   >> ${_testtmp}/pbench-server.cfg
    printf "unittest-dir = ${_testroot}\n" >> ${_testtmp}/pbench-server.cfg
    cat ${_state_config}                   >> ${_testtmp}/pbench-server.cfg

    # First we activate the main pbench server (not a satellite).
    echo "$_testopt/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg" >> $_testactout
    $_testopt/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg >> $_testactout
    rc=$?
    rm ${_testtmp}/pbench-server.cfg
    if [ $rc == 0 ] ;then
        # This script uses the copied config file to do the rest.
        echo "$_testopt/bin/pbench-server-activate ${_PBENCH_SERVER_CONFIG}" >> $_testactout
        $_testopt/bin/pbench-server-activate ${_PBENCH_SERVER_CONFIG} >> $_testactout
        rc=$?
    fi
    if [ $rc -ne 0 ]; then
        echo "ERROR: failed to properly activate the main server test environment root, \"$_testroot\"" >&2
        cat $_testactout >&2
        exit $rc
    fi

    _state_config="${_tdir}/state/${1}.config-satellite/pbench-server.cfg"
    if [ ! -e ${_state_config} ]; then
        _state_config="${_tdir}/state/config-satellite/pbench-server.cfg"
    else
        # the testcase-specific config file includes the global state config file
        # but under the name "state-pbench-server.cfg" to prevent conflicts. It also
        # gets the default section below, so we have to include the install-dir line in it
        # and delete it from the global state config file.
        sed 1d "${_tdir}/state/config/pbench-server.cfg" > ${_testopt}/lib/config/state-pbench-server.cfg
    fi
    > ${_testtmp}/pbench-server.cfg
    printf "[DEFAULT]\n"                   >> ${_testtmp}/pbench-server.cfg
    printf "unittest-dir = ${_testroot}\n" >> ${_testtmp}/pbench-server.cfg
    cat ${_state_config}                   >> ${_testtmp}/pbench-server.cfg

    # Next we activate the satellite pbench server.
    echo "$_testopt_sat/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg" >> $_testactout
    $_testopt_sat/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg >> $_testactout
    rc=$?
    rm ${_testtmp}/pbench-server.cfg
    if [ $rc == 0 ] ;then
        # This script uses the copied config file to do the rest.
        echo "$_testopt_sat/bin/pbench-server-activate ${SATCONFIG}" >> $_testactout
        $_testopt_sat/bin/pbench-server-activate ${SATCONFIG} >> $_testactout
        rc=$?
    fi
    if [ $rc -ne 0 ]; then
        echo "ERROR: failed to properly activate the satellite server test environment root, \"$_testroot\"" >&2
        cat $_testactout >&2
        exit $rc
    fi

    # Up until this point, the activate scripts have been running using the
    # mock scripts, which record their output and execution in $_testlog.
    # But we don't want to have every unit test inherit activation log
    # output unconditionally.  So we move the logs to a special activation
    # log file to make sure we keep it around if we need it when debugging
    # or if a unit test might require it.
    mv $_testlog $_testactlog
    rc=$?
    if [ $rc -ne 0 ]; then
        echo "ERROR: failed to rename $_testlog to $_testactlog: code $rc" >&2
        exit $rc
    fi

    # Add files for a given test
    _state_tb=${_tdir}/state/${1}.tar.xz
    if [ -e ${_state_tb} ]; then
        (cd $_testroot; tar xpf $_state_tb)
        if [[ $? -gt 0 ]]; then
            echo "ERROR: unable to create pbench hierarchy for state $1" >&2
            exit 1
        fi
    fi

    # Run per-test state setup
    _state_setup=${_tdir}/state/${1}.setup
    if [ -f ${_state_setup} ]; then
        (cd $_testroot; $_state_setup)
        if [[ $? -gt 0 ]]; then
            echo "ERROR: unable to run per-test state setup for $1" >&2
            exit 1
        fi
    fi
}

function _run_test {
    # What it takes to run one test
    testname=$1
    cmd=$2
    shift 2

    export _testroot="${_testbase}/${testname}"
    mkdir -p $_testroot
    if [[ ! -d $_testroot ]]; then
        echo "ERROR: failed to create test root directory, \"$_testroot\"" >&2
        exit 1
    fi
    rm -rf $_testroot/*
    if [[ $? -gt 0 ]]; then
        echo "ERROR: failed to empty test root directory, \"$_testroot\"" >&2
        exit 1
    fi

    export _testdur=$_testroot/result.duration
    export _testres=$_testroot/result.txt
    export _testout=$_testroot/output.txt
    export _testdiff=$_testroot/output.diff
    export _testactout=$_testroot/actoutput.txt
    export _testactlog=$_testroot/test-activation-execution.log
    export _testlog=$_testroot/test-execution.log
    export _testcurlpayload=$_testroot/test-curl-payload.log
    export _testloggerpayload=$_testroot/test-logger-payload.log
    export _testdir=$_testroot/pbench
    export _testdir_local=$_testroot/pbench-local
    export _testdir_sat=$_testroot/pbench-satellite
    export _testdir_sat_local=$_testroot/pbench-satellite-local
    export _testtmp=$_testroot/tmp
    export TMPDIR=$_testtmp
    export _testhtml=$_testroot/var-www-html
    export _testhtml_sat=$_testroot/var-www-html-satellite
    export _testopt=$_testroot/opt/pbench-server
    export _testopt_sat=$_testroot/opt/pbench-server-satellite

    _setup_state ${testname}
    # echo ${testname}: ${cmd}
    SECONDS=0
    ${cmd} $*
    echo "${SECONDS} secs" > ${_testdur}
    _audit_server
    rmdir ${_testtmp} > /dev/null 2>&1
    _save_tree
    _dump_logs
    _verify_output ${testname}
    _reset_state ${testname}
    return 0
}

function _reset_state {
    # Run per-test state reset
    _state_reset=$_tdir/state/${1}.reset
    if [ -f ${_state_reset} ]; then
        (cd $_testroot; $_state_reset)
        if [[ $? -gt 0 ]]; then
            echo "ERROR: unable to run per-test state reset for $1" >&2
            exit 1
        fi
    fi

    export PATH=${_orig_PATH}
    unset _PBENCH_SERVER_CONFIG
    unset LOGSDIR
    unset SATCONFIG

    rm -f ${_testopt}/lib/config/state-pbench-server.cfg
    rm -f $_testactout
    rm -f $_testlog
    rm -f $_testactlog
    rm -f $_testcurlpayload
    rm -f $_testloggerpayload
    rm -rf $_testdir
    if [[ -d $_testdir ]]; then
        echo "ERROR: unable to remove pbench hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testdir_local}
    if [[ -d ${_testdir_local} ]]; then
        echo "ERROR: unable to remove pbench-local hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testdir_sat}
    if [[ -d ${_testdir_sat} ]]; then
        echo "ERROR: unable to remove pbench-satellite hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testdir_sat_local}
    if [[ -d ${_testdir_sat_local} ]]; then
        echo "ERROR: unable to remove pbench-satellite-local hierarchy" >&2
        exit 1
    fi
    rm -rf $_testtmp
    if [[ -d $_testtmp ]]; then
        echo "ERROR: unable to remove tmp hierarchy" >&2
        exit 1
    fi
    rm -rf $_testhtml
    if [[ -d $_testhtml ]]; then
        echo "ERROR: unable to remove var-www-html hierarchy" >&2
        exit 1
    fi
    rm -rf $_testhtml_sat
    if [[ -d $_testhtml_sat ]]; then
        echo "ERROR: unable to remove var-www-html hierarchy" >&2
        exit 1
    fi
    rm -rf $_testroot/opt
    if [[ -d $_testroot/opt ]]; then
        echo "ERROR: unable to remove opt hierarchy" >&2
        exit 1
    fi
}

declare -A cmds=(
    # check for no TOP directory
    [test-0]="_run_allscripts"
    # check for no LOGSDIR directory
    [test-0.1]="_run_allscripts"
    # check for no TMP directory
    [test-0.2]="_run_allscripts"
    # check for no ARCHIVE directory
    [test-1]="_run_allscripts"
    # check for no INCOMING directory
    [test-2]="_run_allscripts"
    # check for no RESULTS directory
    [test-3]="_run_allscripts"
    # check for no USERS directory
    [test-4]="_run_allscripts"
    # check that all scripts run normally if everything is there
    [test-5]="_run_allscripts"
    # check that all scripts run normally if everything is there, using a
    # separate unpack directory from the incoming directory
    [test-5.1]="_run_allscripts"
    # check that all scripts process tar balls, using a separate unpack
    # directory from the incoming directory
    [test-5.2]="_run_allscripts"

    # backup tests
    # check for normal operation
    [test-6]="_run pbench-backup-tarballs"
    # check for no ARCHIVE directory
    [test-6.1]="_run pbench-backup-tarballs"
    # check for no ARCHIVE link resolution
    [test-6.2]="_run pbench-backup-tarballs"
    # checks that the dest directory is not bogus
    [test-6.3]="_run pbench-backup-tarballs"
    # real dest directory, no files needing backup - should succeed
    [test-6.4]="_run pbench-backup-tarballs"
    # real dest directory, all files needing backup - should succeed
    [test-6.5]="_run pbench-backup-tarballs"
    # real dest directory, no files needing backup - corrupted archive should not be copied
    [test-6.6]="_run pbench-backup-tarballs"
    # all files needing backup - md5 file missing should fail
    [test-6.7]="_run pbench-backup-tarballs"
    # check for no bucket
    [test-6.8]="_run pbench-backup-tarballs"
    # Upload fail since s3 already contains the result with different md5sum
    [test-6.9]="_run pbench-backup-tarballs"
    # Upload successful since s3 already contains the result with same md5sum
    [test-6.10]="_run pbench-backup-tarballs"
    # Upload successful
    [test-6.11]="_run pbench-backup-tarballs"
    # Upload successful of large tar ball, using multi-upload API
    [test-6.12]="_run pbench-backup-tarballs"

    # Upload fail since md5 corrupt
    # TBD: right now, it is quite complicated to test this from the
    # unit test, because we check whether the md5sum of a tarball is
    # matching with its md5 file or not. To test this case, we need to
    # inject the change in the md5 sum of the tarball after it passes
    # the initial check: only then can we check for a failure because
    # of a mismatched md5 sum when we try to upload to the (mock) s3
    # service.

    # check for successful local backup if S3 is disabled
    [test-6.13]="_run pbench-backup-tarballs"
    [test-6.14]="_run pbench-backup-tarballs"

    # Upload successful of already backed-up large tar ball, using
    # multi-upload API. Variation of 6.12.
    [test-6.15]="_run pbench-backup-tarballs"

    # Check for unexpected errors from get_object().
    # We just enhance 6.12 by adding another tar ball
    # in each case that is named in such a way as to
    # trigger the error.

    # ClientError Other - tar ball added: client_error_other.tar.xz
    [test-6.16]="_run pbench-backup-tarballs"
    # Generic exception - tar ball added: get_object_exception.tar.xz
    [test-6.17]="_run pbench-backup-tarballs"

    # indexing tests are all test-7.*

    # Special case of running pbench-index with -I to get a dump
    # of the index patterns in use.
    [test-7.0.0]="_run pbench-index --dump-index-pattern"
    [test-7.0.1]="_run pbench-index --dump-templates"

    [test-7.1]="_run_indexing"

    # 7.2.X: UnsupportedTarballFormat exceptions:
    # missing metadata.log
    [test-7.2.0]="_run_indexing"
    # badly formed tarball: ./<resultdir> prefix instead of <resultdir>
    [test-7.2.1]="_run_indexing"

    [test-7.3]="_run_indexing"
    [test-7.4]="_run_indexing"
    [test-7.5]="_run_indexing"
    [test-7.6]="_run_indexing"
    [test-7.7]="_run_indexing"
    # uperf tarball
    [test-7.8]="_run_indexing"
    # pbench-user-benchmark tarball
    [test-7.9]="_run_indexing"
    # uperf, fio results data and prometheus data
    # with float conversion
    [test-7.10]="_run_indexing"
    [test-7.11]="_run_indexing"
    [test-7.12]="_run_indexing"
    # The proc-vmstat results data is 7.13.
    [test-7.13]="_run_indexing"
    # The mpstat results data is in 7.14.
    [test-7.14]="_run_indexing"

    # truncated uperf results data so we can do
    # a complete comparison of input to output.
    # uperf throughput (Gb_sec) results
    [test-7.15]="_run_indexing"

    # truncated uperf results data so we can do
    # a complete comparison of input to output.
    # uperf latency and throughput (usec and trans_sec) results
    [test-7.16]="_run_indexing"

    # Test vmstat tool data indexing.
    [test-7.17]="_run_indexing"

    # Test bad controller.
    [test-7.18]="_run_indexing"

    # Test trafficgen data.
    [test-7.19]="_run_indexing"

    # Test run-benchmark fio data.
    [test-7.20]="_run_indexing"
    # Test run-benchmark trafficgen data.
    [test-7.21]="_run_indexing"
    # Test run-benchmark linpack data.
    [test-7.22]="_run_indexing"

    # Test handling of non-uniform timestamps (strings and ints)
    [test-7.23]="_run_indexing"

    # Test user-benchmark indexing
    [test-7.24]="_run_indexing"

    # Debug uperf indexing
    [test-7.25]="_run_indexing"

    # activation test
    [test-8]="_run_activate"

    # pbench-verify-backup-tarballs
    # normal case
    [test-9.1]="_run pbench-verify-backup-tarballs"
    # more tarballs in archive than in backup and S3
    [test-9.2]="_run pbench-verify-backup-tarballs"
    # more tarballs in local backup
    [test-9.3]="_run pbench-verify-backup-tarballs"
    # bad md5 in archive - nothing in S3
    [test-9.4]="_run pbench-verify-backup-tarballs"
    # bad md5 in backup - nothing in S3
    [test-9.5]="_run pbench-verify-backup-tarballs"
    # more tarballs in s3
    [test-9.6]="_run pbench-verify-backup-tarballs"
    # check for Continuation Token
    [test-9.7]="_run pbench-verify-backup-tarballs"
    # large tar ball - reuses the 6.15 tar ball.
    [test-9.8]="_run pbench-verify-backup-tarballs"

    # trivial results: no mail
    [test-10]="_run pbench-sync-satellite satellite-one"
    # non-trivial results: mail
    [test-11]="_run pbench-sync-satellite satellite-one"

    # shim - failure, no quarantine directory
    [test-13]="_run pbench-server-prep-shim-002"
    # shim - failure, no reception area
    [test-15]="_run pbench-server-prep-shim-002"

    # pbench-unpack-tarballs unpacking to the incoming directory
    [test-16]="_run pbench-unpack-tarballs small"
    [test-17]="_run pbench-unpack-tarballs small"

    [test-20]="_run echo audit archive hierarchy"

    [test-21]="_run pbench-dispatch"

    # Actually exercises satellite code
    [test-22]="_run pbench-satellite-cleanup"

    # Uses pbench-dispatch to test report status via syslog
    [test-23]="_run pbench-dispatch"

    # Tests config file that specifies two shims versions
    [test-24]="_run_activate"

    # Tests that the behavior of the find command works as we expect to
    # ensure that buckets for pbench-unpack-tarballs work properly.
    [test-25]="_run test-find-behavior"

    # Test to Log messages on File
    [test-26.1]="_run test_logger_type.py"

    # Test to Log messages on devlog
    [test-26.2]="_run test_logger_type.py"

    # Test to check error when logger_port and logger_host are not provided with hostport
    [test-26.3]="_run test_logger_type.py"

    # Test to Log messages on hostport with logger-host and logger_port
    [test-26.4]="_run test_logger_type.py"
    [test-26.5]="_run_test_logger_level.py"

    # Test culling unpacked tar balls
    [test-27]="_run pbench-cull-unpacked-tarballs"
)
all_tests_sorted=$(for x in ${!cmds[@]} ;do echo $x ;done | sed 's/\./-/' | sort -n -t '-' -k 3 | sort -n -t '-' -k 2 --stable | sed 's/\(.*-[0-9]\+\)-\([0-9]\+\)/\1.\2/')

mode="serial"
if [[ -n "$PBENCH_UNITTEST_SERVER_MODE" ]]; then
    mode="$PBENCH_UNITTEST_SERVER_MODE"
fi
case $1 in
    --serial)
        shift
        mode="serial"
        ;;
    --parallel)
        shift
        mode="parallel"
        ;;
    --*)
        printf "Bad argument $1\n" >&2
        exit 1
        ;;
esac
if [[ "$mode" != "serial" && "$mode" != "parallel" ]]; then
    printf "Bad server unit test mode \"$mode\", choose either 'serial' or 'parallel'\n" >&2
    exit 1
fi

test_args=$*
if [ -z "$test_args" ] ;then
    # No tests given, run them all in sorted order
    tests=$all_tests_sorted
else
    tests=""
    for test_arg in $test_args ;do
        let found=0
        for test_name in $all_tests_sorted; do
            case $test_arg in
            test-*)
                ;;
            *)
                test_arg="test-$test_arg"
                ;;
            esac
            case $test_arg in
            *\.)
                # if $test_arg ends in "." use as a prefix
                if [[ "$test_name" =~ "$test_arg" ]]; then
                    tests="$tests $test_name"
                    let found=found+1
                fi
                ;;
            *)
                # if $test_arg does not end in "." use as a full test name
                if [[ "$test_name" = "$test_arg" ]]; then
                    tests="$tests $test_name"
                    let found=found+1
                    break
                fi
                ;;
            esac
        done
        if [ $found -eq 0 ]; then
            printf "Unknown test $test_arg, skipping ...\n" >&2
        fi
    done
fi

function report_test_result {
    testname=$1
    pass_or_fail=$(cat ${_testbase}/${testname}/result.txt)
    if [[ ${pass_or_fail} == "PASS" ]]; then
        echo "${pass_or_fail} - ${testname} ($(cat ${_testbase}/${testname}/result.duration))"
        rm ${_testbase}/${testname}/result.txt
        rm -f ${_testbase}/${testname}/result.duration
        rmdir ${_testbase}/${testname}
        res=0
    else
        if [[ -e ${_testbase}/${testname}/output.diff ]]; then
            cat ${_testbase}/${testname}/output.diff
        else
            echo "+++"
            echo "*** ${testname} failed with no output"
            echo "---"
        fi
        echo "FAIL - ${testname} ($(cat ${_testbase}/${testname}/result.duration))"
        res=1
        rm -f ${_testbase}/${testname}/result.duration
    fi
    return ${res}
}

declare -A pids

trap "kill -KILL -$$" INT TERM QUIT

let count=0
let failures=0
for testname in $tests ;do
    cmd=${cmds[$testname]}
    if [ -z "$cmd" ]; then
        printf "Unknown test - Logic bomb!: \"${testname}\"\n" >&2
        continue
    fi
    let count=count+1
    if [[ $mode == "parallel" ]]; then
        _run_test ${testname} ${cmd} &
        pids[${testname}]=${!}
    else
        _run_test ${testname} ${cmd}
        report_test_result ${testname}
        if [[ $? -ne 0 ]]; then
            let failures=${failures}+1
        fi
    fi
done
if [ $count -eq 0 ]; then
    printf "No tests run!\n" >&2
    let failures=1
elif [[ $mode == "parallel" ]]; then
    for testname in $tests ;do
        wait ${pids[${testname}]} > /dev/null 2>&1
        report_test_result ${testname}
        if [[ $? -ne 0 ]]; then
            let failures=${failures}+1
        fi
    done
fi

# Attempt to remove test directory entirely; if we fail, ignore it as it
# just means there are output files present for failed tests.
rmdir $_testbase > /dev/null 2>&1

exit $failures
