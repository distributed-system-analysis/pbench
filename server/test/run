#!/bin/bash

# We force the umask to a common value so that unit test output matches on all
# platforms (well, mostly matches on all platforms).
umask 0002

# Don't allow an external _PBENCH_SERVER_CONFIG environment variable to affect the operation
# of the unit tests since the unit tests themselves use getconf.py which looks
# for that envinronment variable as input.
unset _PBENCH_SERVER_CONFIG

# Ensure we always use the same locale for the unit tests.
export LANG=C
export LC_ALL=C

function _log_exit {
    echo "${1}" >&2
    exit 1
}

export _server=$(dirname $(dirname $(realpath "${0}")))
export _bdir=${_server}/bin
test -d "${_bdir}" || _log_exit "ERROR: \"${_bdir}\" not a directory"
export _ldir=${_server}/lib
test -d "${_ldir}" || _log_exit "ERROR: \"${_ldir}\" not a directory"
export _tdir=${_server}/test
test -d "${_tdir}" || _log_exit "ERROR: \"${_tdir}\" not a directory"
export _webs=$(dirname "${_server}")/web-server
test -d "${_webs}" || _log_exit "ERROR: \"${_webs}\" not a directory"
export _testbase=/var/tmp/pbench-test-server
rm -rf ${_testbase}
mkdir ${_testbase}
test -d ${_testbase} || _log_exit "ERROR: Failed to create test base directory, \"${_testbase}\""

alias logger="logger --no-act --stderr"

function remove_path {
    # PATH (${2}) => /bin:/opt/a dir/bin:/sbin
    local WORK=:${2}:
    # WORK => :/bin:/opt/a dir/bin:/sbin:
    local REMOVE=${1}
    WORK=${WORK/:${REMOVE}:/:}
    # WORK => :/bin:/sbin:
    WORK=${WORK%:}
    WORK=${WORK#:}
    #PATH=${WORK}
    # PATH => /bin:/sbin
    echo ${WORK}
}
# Remove potential conflicts with installed RPMs.
PATH=$(remove_path /opt/pbench-server/bin ${PATH})
PATH=$(remove_path /opt/pbench-agent/bench-scripts ${PATH})
export PATH=$(remove_path /opt/pbench-agent/util-scripts ${PATH})

function _run {
    local tname=${1}
    shift
    if [[ -z "${@}" ]]; then
        echo "+++ Running ${tname}" >> ${_testout}
    else
        echo "+++ Running ${tname} ${@}" >> ${_testout}
    fi
    ${tname} ${@} >> ${_testout} 2>&1
    echo "--- Finished ${tname} (status=${?})" >> ${_testout}
}

function _run_re_unpack {
    _run pbench-restore-and-unpack-tarballs ${1}
    _run pbench-unpack-tarballs none re-unpack
}

function _run_activate {
    # Testing the server activate scripts.
    #
    # The server activate scripts are used to setup the server environment for
    # every test run.  We don't actually invoke the activation scripts in this
    # test, but just emit the resulting crontab and activation log file contents
    # generated by the test framework itself.
    echo "+++ Verifying server activation" >> ${_testout}
    sed -i 's;'${_server}'/;/home/user/repo/pbench/server/;' ${_testactout}
    cat ${_testactout} >> ${_testout}
    echo "++++ ${_testopt}/lib/crontab/crontab" >> ${_testout}
    cat ${_testopt}/lib/crontab/crontab >> ${_testout}
    local rc=${?}
    echo "---- ${_testopt}/lib/crontab/crontab" >> ${_testout}
    if [[ -s ${_testactlog} ]]; then
        echo "++++ $(basename ${_testactlog}) file contents" >> ${_testout}
        cat ${_testactlog} >> ${_testout} 2>&1
        echo "---- $(basename ${_testactlog}) file contents" >> ${_testout}
    fi
    echo "--- Finished verifying server activation (status=${rc})" >> ${_testout}
}

function _run_allscripts {
    # Pull in new tar balls from the remote satellite pbench servers and feed
    # them into the dispatch loop.
    #
    # NOTE WELL: the "satellite-one" argument refers a configuration section in
    # the unit test's pbench-server.cfg file - keep them synchronized.
    _run pbench-sync-satellite satellite-one
    # These next five are related and would flow in this order.
    _run pbench-dispatch
    _run pbench-unpack-tarballs small
    # These next five run periodically to accomplish specific tasks outside the
    # normal transition pipeline.
    _run pbench-unpack-tarballs none re-unpack
    _run pbench-clean-up-dangling-results-links
    _run pbench-cull-unpacked-tarballs
    _run pbench-tarball-stats
}

function _local_find {
    # We create our own local find command so that we don't emit the size
    # information for directories.  This is due to the fact that on different
    # file systems empty directories, or directories with small numbers of
    # files, can be handled differently.  E.g. on Ext4 directories have a
    # minimum size of 4096, while on XFS only after a certain size do they
    # grow to multiples of 4096 [1].  We only care about the sizes of files and
    # links in our tests.
    #
    # [1] https://superuser.com/questions/585844/why-directories-size-are-different-in-ls-l-output-on-xfs-file-system
    find ${1} ! -name $(basename ${1}) -type d -printf '%M          - %P\n' , \( ! -type d ! -type l -printf '%M %10s %P\n' \) , -type l -printf '%M %10s %P -> %l\n' | sort -k 3
}

function _normalize_output {
    # Fix up tmp directory references
    sed -E -e 's;tmp/pbench-([-a-zA-Z0-9]+)\.[0-9][0-9]*/;tmp/pbench-\1.NNNN/;' ${*}
}

function _save_tree {
    # Save state of the tree
    if [[ -d ${_testhtml} ]]; then
        echo "+++ var/www/html tree state (${_testhtml})" >> ${_testout}
        _local_find ${_testhtml} >> ${_testout}
        echo "--- var/www/html tree state" >> ${_testout}
        echo "+++ results host info (${_testhtml}/pbench-results-host-info.versioned)" >> ${_testout}
        grep -HvF "\-\-should-n0t-ex1st--" ${_testhtml}/pbench-results-host-info.versioned/pbench-results-host-info.URL00?.* >> ${_testout} 2>&1
        echo "--- results host info" >> ${_testout}
    fi
    echo "+++ pbench tree state (${_testdir})" >> ${_testout}
    if [[ -d ${_testdir} ]]; then
        _local_find ${_testdir} | _normalize_output >> ${_testout}
    fi
    echo "--- pbench tree state" >> ${_testout}
    if [[ -d ${_testdir_local} ]]; then
        echo "+++ pbench-local tree state (${_testdir_local})" >> ${_testout}
        _local_find ${_testdir_local} | _normalize_output >> ${_testout}
        echo "--- pbench-local tree state" >> ${_testout}
    fi
    if [[ -d ${_testtmp} ]]; then
        echo "+++ ${_testtmp}" >> ${_testout}
        find ${_testtmp} -ls >> ${_testout} 2>&1
        echo "--- ${_testtmp}" >> ${_testout}
    fi
}

function _audit_server {
    # Now that the test is complete, replace the test's potentially crazy
    # configuration file with a known-good one for the audit server.
    printf "[DEFAULT]\n"                            > ${_PBENCH_SERVER_CONFIG}
    printf "unittest-dir = ${_testroot}\n"         >> ${_PBENCH_SERVER_CONFIG}
    cat "${_tdir}/state/config/pbench-server.cfg"  >> ${_PBENCH_SERVER_CONFIG}

    echo "+++ Running unit test audit" >> ${_testout}
    pbench-audit-server >> ${_testout} 2>&1
    echo "--- Finished unit test audit (status=${?})" >> ${_testout}
}

function _dump_logs {
    local dir
    local fname
    # Dump the state of any generated script logs
    echo "+++ pbench log file contents" >> ${_testout}
    for dir in ${_testdir} ${_testdir_local}; do
        if [[ -d ${dir}/logs ]]; then
            echo "++++ $(basename ${dir})/logs" >> ${_testout}
            find ${dir}/logs -type f -printf "%P\n" | sort | \
                while read fname; do
                    echo "+++++ ${fname}" >> ${_testout}
                    _normalize_output ${dir}/logs/${fname} >> ${_testout} 2>&1
                    ${_tdir}/bin/verify_cee ${dir}/logs/${fname} >> ${_testout} 2>&1 || echo "Bad CEE JSON in ${dir}/logs/${fname}" >> ${_testout} 2>&1
                    echo "----- ${fname}" >> ${_testout}
                done
            echo "---- $(basename ${dir})/logs" >> ${_testout}
        fi
    done
    echo "--- pbench log file contents" >> ${_testout}

    if [[ -s ${_testlog} ]]; then
        echo "+++ $(basename ${_testlog}) file contents" >> ${_testout}
        _normalize_output ${_testlog} >> ${_testout} 2>&1
        echo "--- $(basename ${_testlog}) file contents" >> ${_testout}
    fi
}

function _verify_output {
    local tname=${1}
    local res
    # Fix up "_id", "_parent", and "@generated-by" IDs using:
    #   * 5ca1ab1e70015f100dedfab1ed0ff1ce
    #    "scalable tools flooded fabled office"
    #   * babb1e70015c01055a15ca1ab1eb0a75
    #    "babble tools colossal scalable boats"
    #   * 70015f01dedbadbeefc105edcafedead
    #    "tools folded bad beef closed cafe dead"
    sed -i -E -e 's/"_id": "[0-9a-f]+",/"_id": "5ca1ab1e70015f100dedfab1ed0ff1ce",/' \
              -e 's/"_parent": "[0-9a-f]+",/"_parent": "babb1e70015c01055a15ca1ab1eb0a75",/' \
              -e 's/"@generated-by": "[0-9a-f]+",/"@generated-by": "70015f01dedbadbeefc105edcafedead",/' ${_testout}
    diff -c ${_tdir}/gold/${tname}.txt ${_testout} > ${_testdiff} 2>&1
    res=${?}
    if [[ ${res} -gt 0 ]]; then
        echo "FAIL" > ${_testres}
    else
        echo "PASS" > ${_testres}
        rm ${_testout} ${_testdiff}
    fi
    return ${res}
}

function move_out_of_maint {
    # Move the Pbench Server out of maintenance mode for the target environment.
    local tgthtml=${1}
    local prefix=${tgthtml}/pbench-results-host-info.versioned
    local tgtname=pbench-results-host-info.URL002
    ln -sf ${tgtname}.active ${prefix}/${tgtname}
}

function _setup_state {
    local res=0
    mkdir -p ${_testopt}/unittest-scripts/
    let res=res+${?}
    cp -a ${_tdir}/bin/* ${_testopt}/unittest-scripts/
    let res=res+${?}
    mkdir -p ${_testopt}/bin
    let res=res+${?}
    cp -a ${_bdir}/{getconf.py,pbench*} ${_testopt}/bin
    let res=res+${?}
    mkdir -p ${_testopt}/html/static/{js,css}/v0.{2,3}
    let res=res+${?}
    cp -a ${_webs}/v0.2/js/ ${_testopt}/html/static/js/v0.2
    let res=res+${?}
    cp -a ${_webs}/v0.3/js/ ${_testopt}/html/static/js/v0.3
    let res=res+${?}
    cp -a ${_webs}/v0.2/css/ ${_testopt}/html/static/css/v0.2
    let res=res+${?}
    cp -a ${_webs}/v0.3/css/ ${_testopt}/html/static/css/v0.3
    let res=res+${?}
    cp -a ${_ldir} ${_testopt}/
    let res=res+${?}
    cp -a ${_server}/../lib/* ${_testopt}/lib
    let res=res+${?}
    mkdir -p ${_testhtml}
    let res=res+${?}
    if [[ $res -ne 0 ]]; then
        echo "ERROR: failed to properly setup the test environment root, \"${_testroot}\"" >&2
        exit $res
    fi

    mkdir ${_testdir} ${_testtmp}
    if [[ ${?} -gt 0 ]]; then
        echo "ERROR: failed to create test pbench and tmp directories, \"${_testdir}\" and/or \"${_testtmp}\"" >&2
        exit 1
    fi
    if [[ ! -d ${_testdir} ]]; then
        echo "ERROR: test pbench directory does not exist, \"${_testdir}\"" >&2
        exit 1
    fi
    if [[ ! -d ${_testtmp} ]]; then
        echo "ERROR: test tmp directory does not exist, \"${_testtmp}\"" >&2
        exit 1
    fi

    echo "X.YY.ZZ" > ${_testopt}/VERSION
    echo "abcdefg" > ${_testopt}/SHA1

    cat > ${_testroot}/agent-profile <<-EOF
	export _PBENCH_AGENT_CONFIG="fake"
	EOF

    # All the "real" scripts are found at ${_testopt}/bin, the mock scripts
    # are found in ${_testopt}/unittest-scripts.
    _orig_PATH=${PATH}
    export PATH=${_testopt}/unittest-scripts:${_testopt}/bin:${PATH}
    export PYTHONPATH="${_testopt}/lib:${PYTHONPATH}"

    # Expected location of the final configuration files
    export _PBENCH_SERVER_CONFIG=${_testopt}/lib/config/pbench-server.cfg
    export LOGSDIR=${_testdir_local}/logs

    # The activate invocations are supposed to work without
    # _PBENCH_SERVER_CONFIG being set, so they do *not* use the global
    # _PBENCH_SERVER_CONFIG file that the rest of the tests use.  We copy the
    # server configuration files to a special directory outside of the source
    # tree to isolate any possible changes to the original source.  The activate
    # script copies it to its "final" resting place.

    > ${_testtmp}/pbench-server.cfg
    printf "[DEFAULT]\n"                   >> ${_testtmp}/pbench-server.cfg
    printf "unittest-dir = ${_testroot}\n" >> ${_testtmp}/pbench-server.cfg

    _state_config="${_tdir}/state/${1}.config/pbench-server.cfg"
    if [[ -e ${_state_config} ]]; then
        # The individual test configs reference the global test config as
        # `state-pbench-server.cfg`.  Copy the global test config into place,
        # and then add the particular test config to final pbench-server.cfg.
        cp ${_tdir}/state/config/pbench-server.cfg ${_testopt}/lib/config/state-pbench-server.cfg
        cat ${_state_config} >> ${_testtmp}/pbench-server.cfg
    else
        # Since there is no particular test configuration, add the global test
        # configuration to the final pbench-server.cfg without the duplicate
        # `[DEFAULT]` header.
        sed 1d "${_tdir}/state/config/pbench-server.cfg" >> ${_testtmp}/pbench-server.cfg
    fi

    # Activate the main pbench server.
    echo "${_testopt}/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg" >> ${_testactout}
    ${_testopt}/bin/pbench-server-config-activate ${_testtmp}/pbench-server.cfg >> ${_testactout}
    local rc=${?}
    rm ${_testtmp}/pbench-server.cfg
    if [[ ${rc} == 0 ]]; then
        # This script uses the copied config file to do the rest.
        echo "${_testopt}/bin/pbench-server-activate ${_PBENCH_SERVER_CONFIG}" >> ${_testactout}
        ${_testopt}/bin/pbench-server-activate ${_PBENCH_SERVER_CONFIG} >> ${_testactout}
        rc=${?}
        if [[ ${rc} == 0 ]]; then
            move_out_of_maint ${_testhtml}
            rc=${?}
        fi
    fi
    if [[ ${rc} -ne 0 ]]; then
        echo "ERROR: failed to properly activate the main server test environment root, \"${_testroot}\"" >&2
        cat ${_testactout} >&2
        exit ${rc}
    fi

    # Up until this point, the activate scripts have been running using the
    # mock scripts, which record their output and execution in ${_testlog}.
    # But we don't want to have every unit test inherit activation log
    # output unconditionally.  So we move the logs to a special activation
    # log file to make sure we keep it around if we need it when debugging
    # or if a unit test might require it.
    /bin/mv ${_testlog} ${_testactlog}
    rc=${?}
    if [[ ${rc} -ne 0 ]]; then
        echo "ERROR: failed to rename ${_testlog} to ${_testactlog}: code ${rc}" >&2
        exit ${rc}
    fi

    # Add files for a given test
    local _state_tb=${_tdir}/state/${1}.tar.xz
    if [[ -e ${_state_tb} ]]; then
        (cd ${_testroot}; tar xpf ${_state_tb})
        if [[ ${?} -gt 0 ]]; then
            echo "ERROR: unable to create pbench hierarchy for state ${1}" >&2
            exit 1
        fi
    fi

    # Run per-test state setup
    local _state_setup=${_tdir}/state/${1}.setup
    if [[ -f ${_state_setup} ]]; then
        (cd ${_testroot}; ${_state_setup})
        if [[ ${?} -gt 0 ]]; then
            echo "ERROR: unable to run per-test state setup for ${1}" >&2
            exit 1
        fi
    fi
}

function _run_test {
    # What it takes to run one test
    local testname=${1}
    local cmd=${2}
    shift 2

    export _testroot="${_testbase}/${testname}"
    mkdir -p ${_testroot}
    if [[ ! -d ${_testroot} ]]; then
        echo "ERROR: failed to create test root directory, \"${_testroot}\"" >&2
        exit 1
    fi
    rm -rf ${_testroot}/*
    if [[ ${?} -gt 0 ]]; then
        echo "ERROR: failed to empty test root directory, \"${_testroot}\"" >&2
        exit 1
    fi

    export _testdur=${_testroot}/result.duration
    export _testres=${_testroot}/result.txt
    export _testout=${_testroot}/output.txt
    export _testdiff=${_testroot}/output.diff
    export _testactout=${_testroot}/actoutput.txt
    export _testactlog=${_testroot}/test-activation-execution.log
    export _testlog=${_testroot}/test-execution.log
    export _testdir=${_testroot}/pbench
    export _testdir_local=${_testroot}/pbench-local
    export _testtmp=${_testroot}/tmp
    export TMPDIR=${_testtmp}
    export _testhtml=${_testroot}/var-www-html
    export _testopt=${_testroot}/opt/pbench-server

    _setup_state ${testname}
    if [[ ${?} -gt 0 ]]; then
        echo "ERROR: failed to setup test state for \"${testname}\"" >&2
        exit 1
    fi

    # echo ${testname}: ${cmd}
    SECONDS=0
    ${cmd} ${*}
    echo "${SECONDS} secs" > ${_testdur}

    _audit_server
    rmdir ${_testtmp} > /dev/null 2>&1
    _save_tree
    _dump_logs
    _verify_output ${testname}
    _reset_state ${testname}
    return 0
}

function _reset_state {
    export PATH=${_orig_PATH}

    # Run per-test state reset
    local _state_reset=${_tdir}/state/${1}.reset
    if [[ -f ${_state_reset} ]]; then
        (cd ${_testroot}; ${_state_reset})
        if [[ ${?} -gt 0 ]]; then
            echo "ERROR: unable to run per-test state reset for ${1}" >&2
            exit 1
        fi
    fi

    unset _PBENCH_SERVER_CONFIG
    unset LOGSDIR

    rm -f ${_testroot}/agent-profile
    rm -f ${_testopt}/lib/config/state-pbench-server.cfg
    rm -f ${_testactout}
    rm -f ${_testlog}
    rm -f ${_testactlog}
    rm -rf ${_testdir}
    if [[ -d ${_testdir} ]]; then
        echo "ERROR: unable to remove pbench hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testdir_local}
    if [[ -d ${_testdir_local} ]]; then
        echo "ERROR: unable to remove pbench-local hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testtmp}
    if [[ -d ${_testtmp} ]]; then
        echo "ERROR: unable to remove tmp hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testhtml}
    if [[ -d ${_testhtml} ]]; then
        echo "ERROR: unable to remove var-www-html hierarchy" >&2
        exit 1
    fi
    rm -rf ${_testroot}/opt
    if [[ -d ${_testroot}/opt ]]; then
        echo "ERROR: unable to remove opt hierarchy" >&2
        exit 1
    fi
}

declare -A cmds=(
    # check for no TOP directory
    [test-0]="_run_allscripts"
    # check for no LOGSDIR directory
    [test-0.1]="_run_allscripts"
    # check for no TMP directory
    [test-0.2]="_run_allscripts"
    # check for no backup directory
    [test-0.3]="_run_allscripts"
    # check for no receive directory and no backup directory
    [test-0.4]="_run_allscripts"
    # check for no ARCHIVE directory
    [test-1]="_run_allscripts"
    # check for no INCOMING directory
    [test-2]="_run_allscripts"
    # check for no RESULTS directory
    [test-3]="_run_allscripts"
    # check for no USERS directory
    [test-4]="_run_allscripts"
    # check that all scripts run normally if everything is there
    [test-5]="_run_allscripts"
    # check that all scripts run normally if everything is there, using a
    # separate unpack directory from the incoming directory
    [test-5.1]="_run_allscripts"
    # check that all scripts process tar balls, using a separate unpack
    # directory from the incoming directory
    [test-5.2]="_run_allscripts"

    # activation test
    [test-8]="_run_activate"

    # Missing argument
    [test-9]="_run pbench-sync-satellite"
    # trivial results: no mail
    [test-10]="_run pbench-sync-satellite satellite-one"
    # non-trivial results: mail
    [test-11]="_run pbench-sync-satellite satellite-one"

    # dispatch - verify invocation of pbench-results-push
    [test-12]="_run pbench-dispatch"
    # dispatch - failure, no quarantine directory
    [test-13]="_run pbench-dispatch"
    # dispatch - failure, no reception area
    [test-15]="_run pbench-dispatch"

    # unpacking to the incoming directory
    [test-16]="_run pbench-unpack-tarballs small"
    [test-17]="_run pbench-unpack-tarballs small"

    # re-unpacking tar balls to the incoming directory
    [test-18]="_run_re_unpack --dry-run"
    [test-19]="_run_re_unpack"

    # Simple test to verify an "empty" archive hierarchy
    [test-20]="_run echo audit archive hierarchy"

    # Verify behavior of dispatch when it doesn't do anything
    [test-21]="_run pbench-dispatch"

    # Tests that the behavior of the find command works as we expect to
    # ensure that buckets for pbench-unpack-tarballs work properly.
    [test-25]="_run test-find-behavior"

    # Test to Log messages on File
    [test-26.1]="_run test_logger_type.py"

    # Test to Log messages on devlog
    [test-26.2]="_run test_logger_type.py"

    # Test to check error when logger_port and logger_host are not provided with hostport
    [test-26.3]="_run test_logger_type.py"

    # Test to Log messages on hostport with logger-host and logger_port
    [test-26.4]="_run test_logger_type.py"

    # Test culling unpacked tar balls
    [test-27]="_run pbench-cull-unpacked-tarballs"

    # Test ssh-error
    [test-28]="_run pbench-sync-satellite satellite-one"
    # Test missing satellite prefix
    [test-29]="_run pbench-sync-satellite satellite-noprefix"
    # Test missing satellite host
    [test-30]="_run pbench-sync-satellite satellite-nohost"
    # Test missing satellite opt
    [test-31]="_run pbench-sync-satellite satellite-noopt"
    # Test missing satellite archive
    [test-32]="_run pbench-sync-satellite satellite-noarchive"
)
all_tests_sorted=$(for x in ${!cmds[@]}; do echo ${x}; done | sed 's/\./-/' | sort -n -t '-' -k 3 | sort -n -t '-' -k 2 --stable | sed 's/\(.*-[0-9]\+\)-\([0-9]\+\)/\1.\2/')

# Verify each test has a gold file and each gold file has a test by creating
# two files containing the lists and "diff"ing them.
ls -1 ${_tdir}/gold | sort > ${_testbase}/gold_directory.lis
for testname in ${!cmds[@]}; do
    echo ${testname}.txt
done | sort > ${_testbase}/unittests.lis
diff -cw ${_testbase}/gold_directory.lis ${_testbase}/unittests.lis || exit ${?}
rm -f ${_testbase}/gold_directory.lis ${_testbase}/unittests.lis

# Sanity check the state directory.
${_tdir}/state/sanity-check || exit ${?}

mode="serial"
if [[ -n "${PBENCH_UNITTEST_SERVER_MODE}" ]]; then
    mode="${PBENCH_UNITTEST_SERVER_MODE}"
fi
case $1 in
    --serial)
        shift
        mode="serial"
        ;;
    --parallel)
        shift
        mode="parallel"
        ;;
    --*)
        printf "Bad argument ${1}\n" >&2
        exit 1
        ;;
esac
if [[ "${mode}" != "serial" && "${mode}" != "parallel" ]]; then
    printf "Bad server unit test mode \"${mode}\", choose either 'serial' or 'parallel'\n" >&2
    exit 1
fi

test_args=${*}
if [[ -z "${test_args}" ]]; then
    # No tests given, run them all in sorted order
    tests=${all_tests_sorted}
else
    tests=""
    for test_arg in ${test_args}; do
        let found=0
        for test_name in ${all_tests_sorted}; do
            case ${test_arg} in
            test-*)
                ;;
            *)
                test_arg="test-${test_arg}"
                ;;
            esac
            case ${test_arg} in
            *\.)
                # if ${test_arg} ends in "." use as a prefix
                if [[ "${test_name}" =~ "${test_arg}" ]]; then
                    tests="${tests} ${test_name}"
                    let found=found+1
                fi
                ;;
            *)
                # if ${test_arg} does not end in "." use as a full test name
                if [[ "${test_name}" = "${test_arg}" ]]; then
                    tests="${tests} ${test_name}"
                    let found=found+1
                    break
                fi
                ;;
            esac
        done
        if [[ ${found} -eq 0 ]]; then
            printf "Unknown test ${test_arg}, skipping ...\n" >&2
        fi
    done
fi

function report_test_result {
    local testname=${1}
    local res
    local pass_or_fail=$(cat ${_testbase}/${testname}/result.txt)
    if [[ ${pass_or_fail} == "PASS" ]]; then
        echo "${pass_or_fail} - ${testname} ($(cat ${_testbase}/${testname}/result.duration))"
        rm ${_testbase}/${testname}/result.txt
        rm -f ${_testbase}/${testname}/result.duration
        rmdir ${_testbase}/${testname}
        res=0
    else
        if [[ -e ${_testbase}/${testname}/output.diff ]]; then
            cat ${_testbase}/${testname}/output.diff
        else
            echo "+++"
            echo "*** ${testname} failed with no output"
            echo "---"
        fi
        echo "FAIL - ${testname} ($(cat ${_testbase}/${testname}/result.duration))"
        res=1
        rm -f ${_testbase}/${testname}/result.duration
    fi
    return ${res}
}

declare -A pids

trap "kill -KILL -${$}" INT TERM QUIT

let count=0
let failures=0
for testname in ${tests}; do
    cmd=${cmds[$testname]}
    if [[ -z "${cmd}" ]]; then
        printf "Unknown test - Logic bomb!: \"${testname}\"\n" >&2
        continue
    fi
    let count=count+1
    if [[ ${mode} == "parallel" ]]; then
        _run_test ${testname} ${cmd} &
        pids[${testname}]=${!}
    else
        _run_test ${testname} ${cmd}
        report_test_result ${testname}
        if [[ ${?} -ne 0 ]]; then
            let failures=${failures}+1
        fi
    fi
done
if [[ ${count} -eq 0 ]]; then
    printf "No tests run!\n" >&2
    let failures=1
elif [[ ${mode} == "parallel" ]]; then
    for testname in ${tests}; do
        wait ${pids[${testname}]} > /dev/null 2>&1
        report_test_result ${testname}
        if [[ ${?} -ne 0 ]]; then
            let failures=${failures}+1
        fi
    done
fi

# Attempt to remove test directory entirely; if we fail, ignore it as it
# just means there are output files present for failed tests.
rmdir ${_testbase} > /dev/null 2>&1

exit ${failures}
