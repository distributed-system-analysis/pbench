#!/usr/bin/python3

# Copyright 2020 Robert Krawitz/Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# generate-pbench-timeseries-graphs -- generate graphs from .csv files from Pbench raw output
#
# Usage (for now): process-timeseries [files...] | bash
#
# Lots of TODOs:
#
# 1) Command line arguments for image size, max # of data sets, what else?
# 2) Actually feed the output into gnuplot rather than just sending it to stdout
# 3) Optionally sort data sets by max value or max average
# 4) Combine multiple samples into 1.
#    This will require that generate-pbench-timeseries-graphs enumerate the files
#    (or accept the file list on stdin rather than command line) because the command
#    line won't be able to accept all of the files in a data set.
# 5) Handle .csv files from perf and any other tools I've missed.
# 6) Allow scaling of Y and possibly X axis.
# 7) Allow command line override of per-file settings.
# 8) Allow additional gnuplot options
# 9) Error checking
# 10) Make data max/minvals available to graph generator so that it can set
#    the axis bounds in gnuplot correctly.
# 11->Graham's number ...

from __future__ import print_function
from pathlib import Path
import argparse
import math
import re
import subprocess
import sys
import tarfile

parser = argparse.ArgumentParser(description='Generate graphs from raw Pbench tarball')
parser.add_argument('-s', "--size", help='Graph size (x,y)', metavar='Graph size (x,y)', default='1280,1024')
parser.add_argument('-w', '--linewidth', help='Line width', metavar='Line Width', default=2, type=int)
parser.add_argument('--max-elements', help='Maximum elements to plot', metavar='Maximum elements to plot', default=0, type=int)
parser.add_argument('-t', "--tarfile-name", help='tarfile name', metavar='tarfile name', action='append')
parser.add_argument('-m', "--match", help='process filenames matching all --match patterns', action='append')
parser.add_argument('-M', "--dont-match", "--no-match", help="don't process filenames matching any --dont-match patterns", action='append')
parser.add_argument('-O', "--out-dir", help="Output root directory", metavar="Output directory", default=".")
parser.add_argument('-L', "--max-label-size", type=int, help="Maximum label size", metavar="max label size")
parser.add_argument('-n', "--dont-generate-graphs", action='store_true', help="Don't actually generate graph")
parser.add_argument('files', metavar='file', type=str, nargs="*", help="files")
args = parser.parse_args()

pattern_map = {
    re.compile(r'disk_IOPS\.csv$') : {
        'type' : 'line',
        'title' : r'IOPS',
    },
    re.compile(r'disk_Queue_Size\.csv$') : {
        'type' : 'line',
        'title' : r'Queue Size',
    },
    re.compile(r'disk_Request_Merges_per_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Request Merges/sec',
    },
    re.compile(r'disk_Request_Size_in_512_byte_sectors\.csv$') : {
        'type' : 'line',
        'title' : r'Request Size (512B sectors)',
    },
    re.compile(r'disk_Request_Size_in_kB\.csv$') : {
        'type' : 'line',
        'title' : r'Request Size (KB)',
    },
    re.compile(r'disk_Throughput_MB_per_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Throughput (MB/sec)',
    },
    re.compile(r'disk_Utilization_percent\.csv$') : {
        'type' : 'line',
        'title' : r'Utilization (percent)',
    },
    re.compile(r'disk_Wait_Time_msec\.csv$') : {
        'type' : 'line',
        'title' : r'Wait Time (msec)',
    },
    re.compile(r'cpuall_cpuall\.csv$') : {
        'type' : 'stack',
        'title' : r'CPU All Utilization',
        'y_max' : 100,
    },
    re.compile(r'cpu([0-9]+)_cpu([0-9]+)\.csv$') : {
        'type' : 'stack',
        'title' : r'CPU \1 Utilization',
        'y_max' : 100,
    },
    re.compile(r'memory_usage_virtual_size\.csv$') : {
        'type' : 'line',
        'title' : r'Virtual Size',
    },
    re.compile(r'memory_usage_resident_set_size\.csv$') : {
        'type' : 'line',
        'title' : r'Resident Set',
    },
    re.compile(r'memory_faults_major_faults_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Major Faults/Sec',
    },
    re.compile(r'memory_faults_minor_faults_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Minor Faults/Sec',
    },
    re.compile(r'file_io_io_reads_KB_sec\.csv$') : {
        'type' : 'line',
        'title' : r'I/O Reads (KB/sec)',
    },
    re.compile(r'file_io_io_writes_KB_sec\.csv$') : {
        'type' : 'line',
        'title' : r'I/O Writes (KB/sec)',
    },
    re.compile(r'cpu_usage_percent_cpu\.csv$') : {
        'type' : 'stack',
        'title' : r'CPU Usage (percent)',
    },
    re.compile(r'context_switches_voluntary_switches_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Voluntary Context Switches/sec',
    },
    re.compile(r'context_switches_nonvoluntary_switches_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Involuntary Context Switches/sec',
    },
    re.compile(r'proc-interrupts-by-irq_IRQ_(.*)\.csv$') : {
        'type' : 'stack',
        'title' : r'Interrupts/sec (IRQ \1)',
        'x_axis' : 'Interrupts/sec by CPU',
    },
    re.compile(r'proc-interrupts-by-cpu_CPU_(.*)\.csv$') : {
        'type' : 'line',
        'title' : r'Interrupts/sec (CPU \1)',
        'x_axis' : 'Interrupts/sec by IRQ',
    },
    re.compile(r'proc-vmstat_allocstall_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Alloc stall delta/sec',
    },
    re.compile(r'proc-vmstat_balloon_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Balloon delta/sec',
    },
    re.compile(r'proc-vmstat_compact_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Compact delta/sec',
    },
    re.compile(r'proc-vmstat_drop_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Drop delta/sec',
    },
    re.compile(r'proc-vmstat_htlb_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'HTLB delta/sec',
    },
    re.compile(r'proc-vmstat_kswapd_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Kswapd delta/sec',
    },
    re.compile(r'proc-vmstat_nr_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Nr delta/sec',
    },
    re.compile(r'proc-vmstat_numa_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'NUMA delta/sec',
    },
    re.compile(r'proc-vmstat_oom_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Oom delta/sec',
    },
    re.compile(r'proc-vmstat_pgalloc_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Pgalloc delta/sec',
    },
    re.compile(r'proc-vmstat_pgmigrate_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Pgmigrate delta/sec',
    },
    re.compile(r'proc-vmstat_pgscan_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Pgscan delta/sec',
    },
    re.compile(r'proc-vmstat_pgskip_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Pgskip delta/sec',
    },
    re.compile(r'proc-vmstat_pgsteal_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Pgsteal delta/sec',
    },
    re.compile(r'proc-vmstat_slabs_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Slabs delta/sec',
    },
    re.compile(r'proc-vmstat_swap_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Swap delta/sec',
    },
    re.compile(r'proc-vmstat_thp_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Thp delta/sec',
    },
    re.compile(r'proc-vmstat_unevictable_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Unevictable delta/sec',
    },
    re.compile(r'proc-vmstat_workingset_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Workingset delta/sec',
    },
    re.compile(r'proc-vmstat_zone_delta_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Zone delta/sec',
    },
    re.compile(r'cpu_all_cpu_busy\.csv$') : {
        'type' : 'stack',
        'title' : r'%Busy (All CPU)',
    },
    re.compile(r'cpu_frequency_MHz\.csv$') : {
        'type' : 'line',
        'title' : r'CPU Frequency (MHz)',
    },
    re.compile(r'per_cpu_cpu_(.*)\.csv$') : {
        'type' : 'stack',
        'title' : r'CPU \1 States (percent)',
        'y_max' : 100,
    },
    re.compile(r'memory_memory_activity\.csv$') : {
        'type' : 'line',
        'title' : r'Memory Activity',
        'y_axis' : 'Activity (counts/sec)',
    },
    re.compile(r'memory_memory_usage\.csv$') : {
        'type' : 'line',
        'title' : r'Memory Usage',
    },
    re.compile(r'network_l2_carrier_errors\.csv$') : {
        'type' : 'line',
        'title' : r'Carrier Errors',
    },
    re.compile(r'network_l2_drops_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Carrier Errors',
    },
    re.compile(r'network_l2_errors_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Errors',
    },
    re.compile(r'network_l2_fifo_overrun_errors\.csv$') : {
        'type' : 'line',
        'title' : r'FIFO Overrun Errors',
    },
    re.compile(r'network_l2_frame_alignment_errors\.csv$') : {
        'type' : 'line',
        'title' : r'Frame Alignment Errors',
    },
    re.compile(r'network_l2_network_compressed_packets_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Compressed Packets/sec',
    },
    re.compile(r'network_l2_network_Mbits_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Network traffic (Mbits/sec)',
    },
    re.compile(r'network_l2_network_multicast_packets_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Multicast Packets/sec',
    },
    re.compile(r'network_l2_network_packets_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Packets/sec',
    },
    re.compile(r'network_l345_ip\.csv$') : {
        'type' : 'line',
        'title' : r'IP Activity (counts/sec)',
    },
    re.compile(r'network_l345_nfs_client\.csv$') : {
        'type' : 'line',
        'title' : r'NFS Activity (counts/sec)',
    },
    re.compile(r'network_l345_sockets\.csv$') : {
        'type' : 'line',
        'title' : r'Sockets (count)',
    },
    re.compile(r'network_l345_tcp_sockets\.csv$') : {
        'type' : 'line',
        'title' : r'TCP Activity (counts/sec)',
    },
    re.compile(r'network_l345_udp\.csv$') : {
        'type' : 'line',
        'title' : r'UDP Activity (counts/sec)',
    },
    re.compile(r'system_interrupts_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Interrupts/sec',
    },
    re.compile(r'system_proc_cswch_sec\.csv$') : {
        'type' : 'line',
        'title' : r'Context Switches/sec',
    },
    re.compile(r'/[0-9]+-[^/]*/sample[0-9]+/csv/[^/]+\.csv$'): {
        'type': 'Empty',
    },
    re.compile(r'/result\.csv$'): {
        'type': 'Empty',
    },
}


def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def generate_spectrum(steps):
    """Generate a spectrum that provides good local contrast.
    This varies saturation and lightness to produce good contrast between
    near-adjacent values.

    As for why I use HSL (hue, saturation, lightness) rather than HSV (hue, saturation,
    value), its big advantage is that it's symmetric from dark to light, which better
    matches what I'm trying to achieve (distribution of light and dark colors to
    maximize contrast between nearby colors).  HSL's weak point is with highly
    saturated light or dark colors, but we're not using such colors, which are difficult
    to distinguish.

    More discussion (if you care) at https://psychology.wikia.org/wiki/HSL_and_HSV

    """

    def normalize_hue(h):
        while h < 0:
            h = h + 6.0
        return h % 6.0

    def hsl_value(n1, n2, h):
        """Generate an RGB coordinate from HSL space.
        """
        h = normalize_hue(h)
        if h < 1.0:
            return n1 + ((n2 - n1) * h)
        elif h < 3.0:
            return n2
        elif h < 4.0:
            return n1 + ((n2 - n1) * (4 - h))
        else:
            return n1

    def color_from_hsl(h, s, l):
        gamma = 1.05
        m2 = 0
        if (l < .5):
            m2 = l * (1 + s)
        else:
            m2 = l + s - (l * s)
        m1 = (l * 2) - m2
        r = hsl_value(m1, m2, h + 2)
        g = hsl_value(m1, m2, h)
        b = hsl_value(m1, m2, h - 2)
        r = int(255 * (r ** (1.0 / gamma)))
        g = int(255 * (g ** (1.0 / gamma)))
        b = int(255 * (b ** (1.0 / gamma)))
        return {'r': r, 'g': g, 'b': b}

    # Perceptual-modified base brightness.  Yellow and cyan, and to a lesser
    # extent green, are perceptually brighter than red and particularly
    # blue.
    lgt_base = [.6, .5, .55, .5, .9, .55, .6]

    def interpolate_base_lgt(h):
        h = normalize_hue(h)
        lower = math.floor(h)
        return (lgt_base[lower] * (1 - (h - lower))) + (lgt_base[lower + 1] * (h - lower))

    direction = 1
    if steps < 0:
        direction = -1
        steps = -steps
    if steps < 2:
        return [{'r': 255, 'g': 0, 'b': 0}]
    else:
        # I've determined all of these constants empirically as
        # "yields a reasonably easy to read graph".  This prefers
        # fairly dark (lightness between .15 and .5) and fairly
        # saturated (saturation between .58 and 1) colors, with
        # steps between colors that are out of sync so we don't
        # wind up with nearby (and hence similar hue) colors with
        # similar saturation and lightness.
        # 0.58 < = sat < = 1
        sat_base = 1.0
        sat_step = 5
        sat_mod = 13
        sat_incr = (.2 / sat_mod)

        lgt_step = 7
        lgt_mod = 10
        lgt_incr = (.4 / lgt_mod)
        # Colors between blue and red (purple/magenta)
        # are a bit confusing.
        min_hue = 0.0           # red
        max_hue = 4.0           # blue
        increment = (max_hue - min_hue) / (steps - 1)

        answer = []
        if direction > 0:
            hue_steps = range(steps - 1, -1, -1)
        else:
            hue_steps = range(0, steps, 1)

        for n in hue_steps:
            h = min_hue + (n * increment)
            answer.append(color_from_hsl(h,
                                         sat_base - (sat_incr * ((n * sat_step) % sat_mod)),
                                         interpolate_base_lgt(h) - (lgt_incr * ((n * lgt_step) % lgt_mod))))
        return answer


# Entirely too much intermingling between generic and Gnuplot code here,
# although the data processing will be different when we use matplotlib.
def generate_graph_gnuplot(filename, graphtypename, title, x_axis, y_axis, y_max, blob):
    def generate_linestyles(count):
        gnuplot_maximum_colors = 255
        if count > gnuplot_maximum_colors:
            count = gnuplot_maximum_colors

        colors = generate_spectrum(count)
        color_list = []
        for n in range(count):
            color_list.append("set style line %d linecolor rgb '#%2.2x%2.2x%2.2x'" % (n + 1, colors[n]['r'], colors[n]['g'], colors[n]['b']))
        answer = "\n"
        return answer.join(color_list)

    def genrow(graphtype, row, overflow_elements=0, ordering=None):
        if ordering is None:
            ordering = []
            for i in range(len(row)):
                ordering.append(i)
        answer = [str(row[0])]
        total_elements = len(row)
        base_elements = total_elements - overflow_elements
        if graphtype == 'line':
            for valno in range(1, base_elements):
                answer.append(str(row[ordering[valno]]))
        else:
            stack_row = []
            acc = 0
            for valno in range(1, len(row)):
                acc += row[ordering[len(row) - valno]]
                if (valno < base_elements):
                    stack_row.append(acc)
            if (overflow_elements > 0):
                stack_row.append(acc)
            for valno in range(len(stack_row), 0, -1):
                answer.append(str(stack_row[valno - 1]))
        return answer

    def sort_columns(criterion, reverse=False):
        """Sort data by column value.
        Ignore the first column, which is the time series"""

        def column_val(col):
            return col[0]

        if (len(criterion) == 1):
            return [0]
        if (len(criterion) == 2):
            return [0, 1]
        ordering = []
        for column in range(1, len(criterion)):
            ordering.append([criterion[column], column])

        ordering.sort(key=lambda item: item[0], reverse=reverse)

        answer = [0]
        for column in range(0, len(ordering)):
            answer.append(ordering[column][1])
        return answer

    def trim_name(name):
        if args.max_label_size is not None:
            return name[:args.max_label_size]
        return name

    def do_generate_graph(fd):

        print("""set yrange [0:%s]
set xrange [0:%d]
set key outside
set terminal pngcairo dashed size %s linewidth %d
set title '%s'
%s
set style fill solid
set output '%s'
set xlabel '%s'
set ylabel '%s'
set grid""" % (y_max, data[len(data)-1][0], args.size, args.linewidth, title, colors, imgfile, x_axis, y_axis), file=fd)

        for nameidx in range(1, len(names)):
            comma = ""
            if len(names) > 1 and nameidx != len(names) - 1:
                comma = ', \\'
            plot = "    "
            if nameidx == 1:
                plot = "plot"
            print("%s '-' using 1:2 with %s ls %s title '%s'%s" % (plot, graphtype, nameidx, trim_name(names[nameidx]), comma), file=fd)

        for col in range(1, len(names)):
            for row in range(1, len(data)):
                data_elts = genrow(graphtypename, data[row], overflow_elements=overflow_elements, ordering=cmap)
                print("%s, %s," % (data_elts[0], data_elts[col]), file=fd)
            print('e', file=fd)

    title = title.replace("'", '"')  # So we don't choke gnuplot
    x_axis = x_axis.replace("'", '"')
    y_axis = y_axis.replace("'", '"')
    data = blob['data']
    if y_max is None:
        y_max = ''

    if graphtypename == "line":
        graphtype = "lines"
    else:
        graphtype = "filledcurves x1"
    cmap = sort_columns(blob['maxvals'], reverse=(graphtypename == 'line'))
    names_cmap = sort_columns(blob['maxvals'], reverse=True)

    eprint("Plotting %s" % (filename))

    total_elements = len(cmap)
    base_elements = total_elements
    overflow_elements = 0
    # The first element is the X axis; that needs to be ignored when
    # computing how many elements we want.
    if args.max_elements > 0 and total_elements > args.max_elements + 1:
        if (graphtypename == 'line'):
            base_elements = args.max_elements + 1
        else:
            base_elements = args.max_elements
        overflow_elements = total_elements - base_elements

    names = []
    tnames = []
    for col in range(1, len(data[0])):
        if col >= base_elements:
            if graphtypename != 'line':
                tnames.append('Other')
            break
        name = data[0][names_cmap[col]]
        name = name.replace("'", '"')
        name = name.replace("_", " ")
        tnames.append(name)
    if graphtypename != 'line':
        tnames.reverse()
    names = [data[0][0]]
    names.extend(tnames)

    imgfile = "%s/%s" % (args.out_dir, filename.replace(".csv", ".png").replace("/csv/", "/"))
    if not args.dont_generate_graphs:
        m = re.compile(r'^(.*/)([^/]*)$').search(imgfile)
        if m:
            imgdir = m.expand(r'\1')
            try:
                Path(imgdir).mkdir(parents=True, exist_ok=True)
            except PermissionError:
                eprint("Cannot create directory %s" % (imgdir))
                return
    colors = generate_linestyles(len(names) - 1)

    program = 'gnuplot'
    if args.dont_generate_graphs:
        program = 'cat'
    with subprocess.Popen([program], stdin=subprocess.PIPE, encoding='ascii') as proc:
        do_generate_graph(proc.stdin)


def read_file(f, filename, fixlinefn=None):
    coldata = []
    maxval = []
    minval = []
    abssum = []
    may_maxval = 0
    min_minval = 0
    timestamp_divisor = 1
    base_ts = 0
    rows_read = 0
    col_count = 0
    for line in f:
        if fixlinefn is not None:
            line = fixlinefn(line)
        line = line.rstrip()
        vals = line.split(',')
        if rows_read == 0:
            if vals[0] == 'timestamp_ms':
                timestamp_divisor = 1000
                vals[0] = 'timestamp_sec'
            for i in range(0, len(vals)):
                coldata.append([])
                coldata[i].append(vals[i])
                maxval.append(0)
                minval.append(0)
                abssum.append(0)
                col_count = col_count + 1
            rows_read = 1
        else:
            if rows_read == 1:
                base_ts = float(vals[0])
            coldata[0].append((float(vals[0]) - base_ts) / timestamp_divisor)
            for i in range(1, len(vals)):
                # Not ideal, but if there's a missing value,
                # probably the best we can do
                try:
                    val = float(vals[i])
                except ValueError:
                    if vals[i] is not None and vals[i] != '':
                        eprint("Bad value '%s' in %s at row %d, col %d" % (vals[i], filename, rows_read, i))
                    val = 0.0
                if i >= col_count:
                    eprint("Unexpected overflow value '%s' in %s at row %d, col %d" % (vals[i], filename, rows_read, i))
                else:
                    coldata[i].append(val)
                abssum.append(abs(val))
                if val > maxval[i]:
                    maxval[i] = val
                    if val > may_maxval:
                        may_maxval = val
                if val < minval[i]:
                    minval[i] = val
                    if val < min_minval:
                        min_minval = val
            rows_read = rows_read + 1
    if maxval == minval:
        return None
    colmap = []
    for i in range(len(coldata)):
        if i == 0 or maxval[i] > minval[i]:
            colmap.append(i)
    data = []
    maxval_n = []
    minval_n = []
    abssum_n = []
    for i in range(len(colmap)):
        mcol = colmap[i]
        maxval_n.append(maxval[mcol])
        minval_n.append(minval[mcol])
        abssum_n.append(abssum[mcol])
    for j in range(rows_read):
        row = []
        for i in range(len(colmap)):
            row.append(coldata[colmap[i]][j])
        data.append(row)
    answer = dict()
    answer['data'] = data
    answer['maxvals'] = maxval_n
    answer['minvals'] = minval_n
    answer['abssum'] = abssum_n
    answer['may_maxval'] = may_maxval
    answer['min_minval'] = min_minval
    return answer


match_res = []
nomatch_re = None


def process_file(filename, openfn=open, fixlinefn=None):
    def find_pattern_for(filename):
        def matches(filename):
            if nomatch_re is not None and nomatch_re.search(filename):
                return False
            for regexp in match_res:
                if not regexp.search(filename):
                    return False
            return True
        doesmatch = matches(filename)
        if not doesmatch:
            return (False, None, None)
        for pattern in pattern_map:
            m = pattern.search(filename)
            if m:
                return (True, m, pattern_map[pattern])
        return (True, None, None)

    initialized_matches = False
    if not initialized_matches:
        if args.match:
            for pattern in args.match:
                match_res.append(re.compile(pattern))
        nomatch_re = None
        if args.dont_match:
            nomatch = "|"
            nomatch = "(%s)" % nomatch.join(args.dont_match)
            nomatch_re = re.compile(nomatch)
        initialized_matches = True

    filename = filename.rstrip()
    if not re.compile(r'\.csv$').search(filename):
        return
    matches, m, p = find_pattern_for(filename)
    if not matches:
        return
    if p is None:
        eprint("Skipping file %s (unknown type)" % filename)
        return
    elif p['type'] == 'Empty':
        eprint("Skipping file %s (not tool output)" % (filename))
        return
    try:
        with openfn(filename, 'r') as f:
            data = read_file(f, filename, fixlinefn=fixlinefn)
        if data is not None and data['data'] is not None and len(data['data']) > 0:
            title = None
            graph_type = None
            x_axis = 'Elapsed Time (sec)'
            y_axis = ''
            y_max = None
            if 'y_max' in p:
                y_max = p['y_max']
            graph_type = p["type"]
            title = m.expand(p["title"])
            if 'x_axis' in p:
                x_axis = m.expand(p["x_axis"])
            if 'y_axis' in p:
                y_axis = m.expand(p["y_axis"])
            generate_graph_gnuplot(filename, graph_type, title, x_axis, y_axis, y_max, data)
        else:
            eprint("Skipping file %s (no data)" % (filename))
    except IOError:
        eprint("Unable to open '%s'" % (filename))


if args.files:
    for filename in args.files:
        process_file(filename)

if args.tarfile_name:
    for filename in args.tarfile_name:
        with open(filename, 'rb') as f:
            with tarfile.open(fileobj=f) as t:
                for tarobj in t:
                    process_file(tarobj.name, openfn=lambda fn, mode: t.extractfile(tarobj), fixlinefn=lambda line: line.decode())

if not args.files and not args.tarfile_name:
    for line in sys.stdin:
        process_file(line.rstrip())
